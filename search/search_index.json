{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WEBSITE UNDER CONSTRUCTION TheHive Project # This is the official documentation website of TheHive Project. TheHive # TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. Sources: https://github.com/TheHive-Project/TheHive Documentation: https://docs.thehive-project.org/docs/thehive/ TheHive4py # TheHive4py is a Python API client for TheHive. Sources: https://github.com/TheHive-Project/TheHive4py Documentation: https://thehive-project.github.io/TheHive4py/ Cortex # Cortex is a powerful observable analysis and active response engine. Sources: https://github.com/TheHive-Project/Cortex Documentation: https://github.com/TheHive-Project/CortexDocs Cortex Neurons # Cortex neurons is the repository of the reviewed Analyzers and Responders, contributed by the community. Sources: https://github.com/TheHive-Project/Cortex-Analyzers/ Documentation: https://thehive-project.github.io/Cortex-Analyzers/ Cortex4py # Cortex4py is a Python API client for Cortex. Sources: https://github.com/TheHive-Project/Cortex4py Documentation: https://github.com/TheHive-Project/Cortex4py Cortexutils # Cortexutils is a Python library containing a set of classes that aims to make users write Cortex analyzers and responders easier. Sources: https://github.com/TheHive-Project/Cortexutils Documentation: https://github.com/TheHive-Project/Cortexutils Docker-templates # This repository is hosting docker configurations for TheHive, Cortex and 3rd party tools integrations. Sources: https://github.com/TheHive-Project/Docker-Templates Awesome # This repository aims at reference and centralise a curated list of awesome things related to TheHive & Cortex. Website: https://github.com/TheHive-Project/awesome TODO # How to update to TheHive 4.1.0 User Guides","title":"Home"},{"location":"#thehive-project","text":"This is the official documentation website of TheHive Project.","title":"TheHive Project"},{"location":"#thehive","text":"TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. Sources: https://github.com/TheHive-Project/TheHive Documentation: https://docs.thehive-project.org/docs/thehive/","title":"TheHive"},{"location":"#thehive4py","text":"TheHive4py is a Python API client for TheHive. Sources: https://github.com/TheHive-Project/TheHive4py Documentation: https://thehive-project.github.io/TheHive4py/","title":"TheHive4py"},{"location":"#cortex","text":"Cortex is a powerful observable analysis and active response engine. Sources: https://github.com/TheHive-Project/Cortex Documentation: https://github.com/TheHive-Project/CortexDocs","title":"Cortex"},{"location":"#cortex-neurons","text":"Cortex neurons is the repository of the reviewed Analyzers and Responders, contributed by the community. Sources: https://github.com/TheHive-Project/Cortex-Analyzers/ Documentation: https://thehive-project.github.io/Cortex-Analyzers/","title":"Cortex Neurons"},{"location":"#cortex4py","text":"Cortex4py is a Python API client for Cortex. Sources: https://github.com/TheHive-Project/Cortex4py Documentation: https://github.com/TheHive-Project/Cortex4py","title":"Cortex4py"},{"location":"#cortexutils","text":"Cortexutils is a Python library containing a set of classes that aims to make users write Cortex analyzers and responders easier. Sources: https://github.com/TheHive-Project/Cortexutils Documentation: https://github.com/TheHive-Project/Cortexutils","title":"Cortexutils"},{"location":"#docker-templates","text":"This repository is hosting docker configurations for TheHive, Cortex and 3rd party tools integrations. Sources: https://github.com/TheHive-Project/Docker-Templates","title":"Docker-templates"},{"location":"#awesome","text":"This repository aims at reference and centralise a curated list of awesome things related to TheHive & Cortex. Website: https://github.com/TheHive-Project/awesome","title":"Awesome"},{"location":"#todo","text":"How to update to TheHive 4.1.0 User Guides","title":"TODO"},{"location":"download/","text":"","title":"Index"},{"location":"resources/Keynotes/list/","text":"Additional Resources # The following page lists additional resources that should help you get more acquainted with TheHive, Cortex & other tools. Table of Contents # Presentations Workshops and Trainings Hack.lu 2019 Botconf 2018 User Contributions Presentations # We make several presentations throughout the year during conferences and various events. Please find below some of the latest presentation material we produced: Cruising Ocean Threat without Sinking Using TheHive, Cortex & MISP. BSidesLisbon 2018. November 29, 2018. ( PDF ) TheHive & Cortex UYBHYS 2018. November 17, 2018. ( PDF ) MISP, TheHive & Cortex: better, faster, happier. MISP Summit 04. October 16, 2018. ( PDF ) Workshops and Trainings # We frequently organize workshops and trainings, often with our friends from the MISP Project . We do not publish all the materials because we often leverage MISP instances containing training-specific events and Cortex servers configured with commercial analyzers that supporting partners such as DomainTools and Onyphe kindly give us access to for the duration of the workshops and trainings. If you'd like to attend a future workshop or training, please follow us on https://twitter.com/thehive_project or regularly visit our blog . However, if you'd like to do the training at your own pace, you can find below the materials used for some of the workshops and trainings we gave in the past. Please note that you might have some difficulties completing the case studies without access to the commercial analyzers highlighted above. Hack.lu 2019 # We gave a workshop during Hack.lu on Thu Oct 24, 2019. We prepared a MISP and Cortex instance on the cloud as well as a custom built training VM containing TheHive 3.4.0 which took advantage of those cloud instances.The VM was shared with the attendees during the workshop but will not be posted online. Indeed, the above-mentioned cloud instances were turned off after the workshop. That being said, you can still get a look at the slides we used to set the stage for the workshop. They contain some valuable information if you are considering installing TheHive, Cortex & MISP or just beginning with the trio. Botconf 2018 # We gave a workshop during Botconf on Tue Dec 4, 2018. If you'd like to give it a try on your own, you will need: - familiarity with TCP/IP, Linux (including editing configuration files), SSH & incident response - the joint MISP, TheHive & Cortex training VM ( SHA256 checksum ) - a powerful laptop with virtualization software (either VMware Workstation, VMware Fusion or VirtualBox) - the ability to give the training VM 6GB of RAM and 2 processor cores. If that's not possible, we consider 4GB and 1 processor core the bare minimum - the training instructions and cheatsheet - Case Study 1 - Case Study 2 Before undertaking the workshop, we highly recommend reading the following slides in the specified order: - Threat Intelligence and Information Sharing with MISP - Detect, Investigate & Respond with MISP, TheHive & Cortex Important Note : you won't be able to do case study 3 as it requires access to the instructors' MISP instance which is only available during the workshops and trainings. You must also skip the steps which ask you to synchronize your MISP instance with the instructors' (unless you have access to an instance pre-populated with events) or configure TheHive to leverage the instructors' Cortex instance. User Contributions # The resources below have been contributed by our user community. Please note that the fact that they are listed here does not mean that they have been checked, validated or endorsed in any way by TheHive Project. Use your own judgment if you decide to read them. TheHive Scripting: Task Imports , Matt B. Last accessed on March 26, 2019.","title":"Additional Resources"},{"location":"resources/Keynotes/list/#additional-resources","text":"The following page lists additional resources that should help you get more acquainted with TheHive, Cortex & other tools.","title":"Additional Resources"},{"location":"resources/Keynotes/list/#table-of-contents","text":"Presentations Workshops and Trainings Hack.lu 2019 Botconf 2018 User Contributions","title":"Table of Contents"},{"location":"resources/Keynotes/list/#presentations","text":"We make several presentations throughout the year during conferences and various events. Please find below some of the latest presentation material we produced: Cruising Ocean Threat without Sinking Using TheHive, Cortex & MISP. BSidesLisbon 2018. November 29, 2018. ( PDF ) TheHive & Cortex UYBHYS 2018. November 17, 2018. ( PDF ) MISP, TheHive & Cortex: better, faster, happier. MISP Summit 04. October 16, 2018. ( PDF )","title":"Presentations"},{"location":"resources/Keynotes/list/#workshops-and-trainings","text":"We frequently organize workshops and trainings, often with our friends from the MISP Project . We do not publish all the materials because we often leverage MISP instances containing training-specific events and Cortex servers configured with commercial analyzers that supporting partners such as DomainTools and Onyphe kindly give us access to for the duration of the workshops and trainings. If you'd like to attend a future workshop or training, please follow us on https://twitter.com/thehive_project or regularly visit our blog . However, if you'd like to do the training at your own pace, you can find below the materials used for some of the workshops and trainings we gave in the past. Please note that you might have some difficulties completing the case studies without access to the commercial analyzers highlighted above.","title":"Workshops and Trainings"},{"location":"resources/Keynotes/list/#hacklu-2019","text":"We gave a workshop during Hack.lu on Thu Oct 24, 2019. We prepared a MISP and Cortex instance on the cloud as well as a custom built training VM containing TheHive 3.4.0 which took advantage of those cloud instances.The VM was shared with the attendees during the workshop but will not be posted online. Indeed, the above-mentioned cloud instances were turned off after the workshop. That being said, you can still get a look at the slides we used to set the stage for the workshop. They contain some valuable information if you are considering installing TheHive, Cortex & MISP or just beginning with the trio.","title":"Hack.lu 2019"},{"location":"resources/Keynotes/list/#botconf-2018","text":"We gave a workshop during Botconf on Tue Dec 4, 2018. If you'd like to give it a try on your own, you will need: - familiarity with TCP/IP, Linux (including editing configuration files), SSH & incident response - the joint MISP, TheHive & Cortex training VM ( SHA256 checksum ) - a powerful laptop with virtualization software (either VMware Workstation, VMware Fusion or VirtualBox) - the ability to give the training VM 6GB of RAM and 2 processor cores. If that's not possible, we consider 4GB and 1 processor core the bare minimum - the training instructions and cheatsheet - Case Study 1 - Case Study 2 Before undertaking the workshop, we highly recommend reading the following slides in the specified order: - Threat Intelligence and Information Sharing with MISP - Detect, Investigate & Respond with MISP, TheHive & Cortex Important Note : you won't be able to do case study 3 as it requires access to the instructors' MISP instance which is only available during the workshops and trainings. You must also skip the steps which ask you to synchronize your MISP instance with the instructors' (unless you have access to an instance pre-populated with events) or configure TheHive to leverage the instructors' Cortex instance.","title":"Botconf 2018"},{"location":"resources/Keynotes/list/#user-contributions","text":"The resources below have been contributed by our user community. Please note that the fact that they are listed here does not mean that they have been checked, validated or endorsed in any way by TheHive Project. Use your own judgment if you decide to read them. TheHive Scripting: Task Imports , Matt B. Last accessed on March 26, 2019.","title":"User Contributions"},{"location":"resources/Virtual%20Machine/demo/","text":"Demo VM # A ready-to-use virtual machine can be downloaded at https://www.strangebee.com/tryit . This VM is prepared and updated by StrangeBee and is powered by the latest versions of: TheHive: Security Incident Response and Case management platform Cortex: Extendable Analysis, Enrichment and Response automation framework Warning The VM is built for testing purposes and is NOT RECOMMENDED for production .","title":"Demo VM"},{"location":"resources/Virtual%20Machine/demo/#demo-vm","text":"A ready-to-use virtual machine can be downloaded at https://www.strangebee.com/tryit . This VM is prepared and updated by StrangeBee and is powered by the latest versions of: TheHive: Security Incident Response and Case management platform Cortex: Extendable Analysis, Enrichment and Response automation framework Warning The VM is built for testing purposes and is NOT RECOMMENDED for production .","title":"Demo VM"},{"location":"thehive/","text":"TheHive : Installation, operation and user guides Source Code : https://github.com/thehive-project/TheHive/ Website : https://www.thehive-project.org TheHive # TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. TheHive supports different methods to store data, files, and indexes according to your needs. However, even for a standalone, production server, we strongly recommend using Apache Cassandra as a scalable and fault-tolerant database. Files and indexes storage can vary, depending on your target setup ; for standalone server, the local filesystem is suitable, while sereval options are possible in the case of a cluster configuration. Installation and configuration guides # This documentation contains step-by-step installation instructions for TheHive for different operating systems as well as corresponding binary archives. All aspects of the configuration are aslo detailled in a dedicated section. User guides # TheHive supports differents roles for users. Depending on if you are an administrator of the plateform, an administrator of an organisation or an analyst you can have access and run differents actions in the plateform. The user guides aims at describing all major howtos for users according to their roles and permissions. Operations # Discover how to migration from TheHive 3.x to TheHive 4.x with our migration guide . Several other operational guides are provided to the community. Setup HTTPS with nginx or haproxy Backup and restore : example on how to backup and restore data stored in Apache Cassandra Adding security in Apache Cassandra Using Fail2Ban and block unwanted connections to the plateform TheHive 3 # If you are still using TheHive 3.x, the associated documentation is available here End of Life TheHive 3 is coming End of Life. This version no longer benefits from new features. We recommend migrating as soon as possible to TheHive 4.x. License # TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run. Updates and community discussions # Information, news and updates are regularly posted on several communication channels: TheHive Project Twitter account TheHive Project blog TheHive Project Discord Users forum on Google Groups . Request an access: using a Gmail address or without it . Contributing # We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing. Community support # Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Note If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository . Professional support # TheHive is fully developped and maintained by StrangeBee . Should you need specific assistance, be aware that StrangeBee also provides professional services and support.","title":"Home"},{"location":"thehive/#thehive","text":"TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. TheHive supports different methods to store data, files, and indexes according to your needs. However, even for a standalone, production server, we strongly recommend using Apache Cassandra as a scalable and fault-tolerant database. Files and indexes storage can vary, depending on your target setup ; for standalone server, the local filesystem is suitable, while sereval options are possible in the case of a cluster configuration.","title":"TheHive"},{"location":"thehive/#installation-and-configuration-guides","text":"This documentation contains step-by-step installation instructions for TheHive for different operating systems as well as corresponding binary archives. All aspects of the configuration are aslo detailled in a dedicated section.","title":"Installation and configuration guides"},{"location":"thehive/#user-guides","text":"TheHive supports differents roles for users. Depending on if you are an administrator of the plateform, an administrator of an organisation or an analyst you can have access and run differents actions in the plateform. The user guides aims at describing all major howtos for users according to their roles and permissions.","title":"User guides"},{"location":"thehive/#operations","text":"Discover how to migration from TheHive 3.x to TheHive 4.x with our migration guide . Several other operational guides are provided to the community. Setup HTTPS with nginx or haproxy Backup and restore : example on how to backup and restore data stored in Apache Cassandra Adding security in Apache Cassandra Using Fail2Ban and block unwanted connections to the plateform","title":"Operations"},{"location":"thehive/#thehive-3","text":"If you are still using TheHive 3.x, the associated documentation is available here End of Life TheHive 3 is coming End of Life. This version no longer benefits from new features. We recommend migrating as soon as possible to TheHive 4.x.","title":"TheHive 3"},{"location":"thehive/#license","text":"TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run.","title":"License"},{"location":"thehive/#updates-and-community-discussions","text":"Information, news and updates are regularly posted on several communication channels: TheHive Project Twitter account TheHive Project blog TheHive Project Discord Users forum on Google Groups . Request an access: using a Gmail address or without it .","title":"Updates and community discussions"},{"location":"thehive/#contributing","text":"We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing.","title":"Contributing"},{"location":"thehive/#community-support","text":"Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Note If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository .","title":"Community support"},{"location":"thehive/#professional-support","text":"TheHive is fully developped and maintained by StrangeBee . Should you need specific assistance, be aware that StrangeBee also provides professional services and support.","title":"Professional support"},{"location":"thehive/code-of-conduct/","text":"Contributor Covenant Code of Conduct # Our Pledge # In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards # Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities # Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior, in compliance with the licensing terms applying to the Project developments. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. However, these actions shall respect the licensing terms of the Project Developments that will always supersede such Code of Conduct. Scope # This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement # Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at support@thehive-project.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution # This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4 This version includes a clarification to ensure that the code of conduct is in compliance with the free software licensing terms of the project.","title":"Contributor Covenant Code of Conduct"},{"location":"thehive/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"thehive/code-of-conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"thehive/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"thehive/code-of-conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior, in compliance with the licensing terms applying to the Project developments. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. However, these actions shall respect the licensing terms of the Project Developments that will always supersede such Code of Conduct.","title":"Our Responsibilities"},{"location":"thehive/code-of-conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"thehive/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at support@thehive-project.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"thehive/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4 This version includes a clarification to ensure that the code of conduct is in compliance with the free software licensing terms of the project.","title":"Attribution"},{"location":"thehive/api/","text":"Introduction # Authorization # Library # StrangeBee provides an official library for integrating with the remote API of TheHive: TheHive4py","title":"Introduction"},{"location":"thehive/api/#introduction","text":"","title":"Introduction"},{"location":"thehive/api/#authorization","text":"","title":"Authorization"},{"location":"thehive/api/#library","text":"StrangeBee provides an official library for integrating with the remote API of TheHive: TheHive4py","title":"Library"},{"location":"thehive/api/alert/create/","text":"Create # Create an Alert . Query # POST /api/alert Example Request Body # { \"artifacts\" : [], \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"source\" : \"misp server\" , \"sourceRef\" : \"1311\" , \"tags\" : [ \"tlp:white\" , \"type:OSINT\" ], \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"tlp\" : 0 , \"type\" : \"MISP Event\" } Example Response Body # { \"_id\" : \"~987889880\" , \"id\" : \"~987889880\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630323713949 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"misp event\" , \"source\" : \"misp server\" , \"sourceRef\" : \"1311-2\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"date\" : 1630323713937 , \"tags\" : [ \"tlp:pwhite\" , \"type:OSINT\" , ], \"tlp\" : 0 , \"pap\" : 2 , \"status\" : \"New\" , \"follow\" : true , \"customFields\" : {}, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] }","title":"Create"},{"location":"thehive/api/alert/create/#create","text":"Create an Alert .","title":"Create"},{"location":"thehive/api/alert/create/#query","text":"POST /api/alert","title":"Query"},{"location":"thehive/api/alert/create/#example-request-body","text":"{ \"artifacts\" : [], \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"source\" : \"misp server\" , \"sourceRef\" : \"1311\" , \"tags\" : [ \"tlp:white\" , \"type:OSINT\" ], \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"tlp\" : 0 , \"type\" : \"MISP Event\" }","title":"Example Request Body"},{"location":"thehive/api/alert/create/#example-response-body","text":"{ \"_id\" : \"~987889880\" , \"id\" : \"~987889880\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630323713949 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"misp event\" , \"source\" : \"misp server\" , \"sourceRef\" : \"1311-2\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"date\" : 1630323713937 , \"tags\" : [ \"tlp:pwhite\" , \"type:OSINT\" , ], \"tlp\" : 0 , \"pap\" : 2 , \"status\" : \"New\" , \"follow\" : true , \"customFields\" : {}, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] }","title":"Example Response Body"},{"location":"thehive/api/alert/delete/","text":"Delete # Delete an Alert . Query # DELETE /api/alert/{id}?force=1 Example Request Body # Example Response Body #","title":"Delete"},{"location":"thehive/api/alert/delete/#delete","text":"Delete an Alert .","title":"Delete"},{"location":"thehive/api/alert/delete/#query","text":"DELETE /api/alert/{id}?force=1","title":"Query"},{"location":"thehive/api/alert/delete/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/alert/delete/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/alert/merge/","text":"Merge # Merge an Alert into an existing Case . Query # Example Request Body # Example Response Body #","title":"Merge"},{"location":"thehive/api/alert/merge/#merge","text":"Merge an Alert into an existing Case .","title":"Merge"},{"location":"thehive/api/alert/merge/#query","text":"","title":"Query"},{"location":"thehive/api/alert/merge/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/alert/merge/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/alert/promote-as-case/","text":"Promote # Promote an Alert as a new Case . Query # Example Request Body # Example Response Body #","title":"Promote"},{"location":"thehive/api/alert/promote-as-case/#promote","text":"Promote an Alert as a new Case .","title":"Promote"},{"location":"thehive/api/alert/promote-as-case/#query","text":"","title":"Query"},{"location":"thehive/api/alert/promote-as-case/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/alert/promote-as-case/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/alert/update/","text":"Update # Update an existing Alert, like add observables, mark as read, update status ... Query # Example Request Body # Example Response Body #","title":"Update"},{"location":"thehive/api/alert/update/#update","text":"Update an existing Alert, like add observables, mark as read, update status ...","title":"Update"},{"location":"thehive/api/alert/update/#query","text":"","title":"Query"},{"location":"thehive/api/alert/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/alert/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/case/create/","text":"Create # Query # Example Request Body # Example Response Body #","title":"Create"},{"location":"thehive/api/case/create/#create","text":"","title":"Create"},{"location":"thehive/api/case/create/#query","text":"","title":"Query"},{"location":"thehive/api/case/create/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/case/create/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/case/delete/","text":"Delete # Query # Example Request Body # Example Response Body #","title":"Delete"},{"location":"thehive/api/case/delete/#delete","text":"","title":"Delete"},{"location":"thehive/api/case/delete/#query","text":"","title":"Query"},{"location":"thehive/api/case/delete/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/case/delete/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/case/merge/","text":"Merge # Query # Example Request Body # Example Response Body #","title":"Merge"},{"location":"thehive/api/case/merge/#merge","text":"","title":"Merge"},{"location":"thehive/api/case/merge/#query","text":"","title":"Query"},{"location":"thehive/api/case/merge/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/case/merge/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/case/update/","text":"Update # Query # Example Request Body # Example Response Body #","title":"Update"},{"location":"thehive/api/case/update/#update","text":"","title":"Update"},{"location":"thehive/api/case/update/#query","text":"","title":"Query"},{"location":"thehive/api/case/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/case/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/case-template/create/","text":"Create # Query # Example Request Body # Example Response Body #","title":"Create"},{"location":"thehive/api/case-template/create/#create","text":"","title":"Create"},{"location":"thehive/api/case-template/create/#query","text":"","title":"Query"},{"location":"thehive/api/case-template/create/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/case-template/create/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/case-template/update/","text":"Update # Query # Example Request Body # Example Response Body #","title":"Update"},{"location":"thehive/api/case-template/update/#update","text":"","title":"Update"},{"location":"thehive/api/case-template/update/#query","text":"","title":"Query"},{"location":"thehive/api/case-template/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/case-template/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/custom-field/create/","text":"Create # Query # Example Request Body # Example Response Body #","title":"Create"},{"location":"thehive/api/custom-field/create/#create","text":"","title":"Create"},{"location":"thehive/api/custom-field/create/#query","text":"","title":"Query"},{"location":"thehive/api/custom-field/create/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/custom-field/create/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/custom-field/update/","text":"Update # Query # Example Request Body # Example Response Body #","title":"Update"},{"location":"thehive/api/custom-field/update/#update","text":"","title":"Update"},{"location":"thehive/api/custom-field/update/#query","text":"","title":"Query"},{"location":"thehive/api/custom-field/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/custom-field/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/dashboard/create/","text":"","title":"Create"},{"location":"thehive/api/dashboard/update/","text":"Update # Query # Example Request Body # Example Response Body #","title":"Update"},{"location":"thehive/api/dashboard/update/#update","text":"","title":"Update"},{"location":"thehive/api/dashboard/update/#query","text":"","title":"Query"},{"location":"thehive/api/dashboard/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/dashboard/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/observable/create/","text":"Create # Query # Example Request Body # Example Response Body #","title":"Create"},{"location":"thehive/api/observable/create/#create","text":"","title":"Create"},{"location":"thehive/api/observable/create/#query","text":"","title":"Query"},{"location":"thehive/api/observable/create/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/observable/create/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/observable/update/","text":"Update # Query # Example Request Body # Example Response Body #","title":"Update"},{"location":"thehive/api/observable/update/#update","text":"","title":"Update"},{"location":"thehive/api/observable/update/#query","text":"","title":"Query"},{"location":"thehive/api/observable/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/observable/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/organisation/create/","text":"Create # Query # Example Request Body # Example Response Body #","title":"Create"},{"location":"thehive/api/organisation/create/#create","text":"","title":"Create"},{"location":"thehive/api/organisation/create/#query","text":"","title":"Query"},{"location":"thehive/api/organisation/create/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/organisation/create/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/organisation/update/","text":"Update# Update # Query # Example Request Body # Example Response Body #","title":"Update# Update"},{"location":"thehive/api/organisation/update/#update-update","text":"","title":"Update# Update"},{"location":"thehive/api/organisation/update/#query","text":"","title":"Query"},{"location":"thehive/api/organisation/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/organisation/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/task/create/","text":"","title":"Create"},{"location":"thehive/api/task/update/","text":"Update # Query # Example Request Body # Example Response Body #","title":"Update"},{"location":"thehive/api/task/update/#update","text":"","title":"Update"},{"location":"thehive/api/task/update/#query","text":"","title":"Query"},{"location":"thehive/api/task/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/task/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/ttp/create/","text":"Create # Query # Example Request Body # Example Response Body #","title":"Create"},{"location":"thehive/api/ttp/create/#create","text":"","title":"Create"},{"location":"thehive/api/ttp/create/#query","text":"","title":"Query"},{"location":"thehive/api/ttp/create/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/ttp/create/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/ttp/update/","text":"Update # Query # Example Request Body # Example Response Body #","title":"Update"},{"location":"thehive/api/ttp/update/#update","text":"","title":"Update"},{"location":"thehive/api/ttp/update/#query","text":"","title":"Query"},{"location":"thehive/api/ttp/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/ttp/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/user/create/","text":"Create # Query # Example Request Body # Example Response Body #","title":"Create"},{"location":"thehive/api/user/create/#create","text":"","title":"Create"},{"location":"thehive/api/user/create/#query","text":"","title":"Query"},{"location":"thehive/api/user/create/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/user/create/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/api/user/update/","text":"Update # Query # Example Request Body # Example Response Body #","title":"Update"},{"location":"thehive/api/user/update/#update","text":"","title":"Update"},{"location":"thehive/api/user/update/#query","text":"","title":"Query"},{"location":"thehive/api/user/update/#example-request-body","text":"","title":"Example Request Body"},{"location":"thehive/api/user/update/#example-response-body","text":"","title":"Example Response Body"},{"location":"thehive/installation-and-configuration/","text":"Installation & configuration guides # Overview # The scalability of TheHive allows it to be set up as a standalone server or as nodes inside a cluster. Any number of nodes can rely on a database and a file system also setup as standalone servers or a cluster. Before starting installing and configuring, you need to identify and define the targetted architecture. Choose a setup # The modular architecture makes it support several types of database, file storage system and indexing system. The initial choices made with the target architecture and the setup are crucial, especially for the database. If high availability and fault tolerance are necessary, implementing a cluster might be the choice, and this choice determines the database, the file storage and indexing system to install. Hardware Pre-requisites Hardware requirements depends on the number of concurrent users and how they use the system. The following table give some information to choose the hardware. Number of users CPU RAM < 3 2 4-8 < 10 4 8-16 < 20 8 16-32 Choose a database # Once the target setup is identified, the first choice to make is the database. Even of local Berkeley DB and Cassandra database are supported, we recommend using Apache Cassandra , which is a scalable and high available Database, even for standalone servers. Berkeley DB can be enough for testing purposes. Upgradability This choice is decisive as migration from Berkeley DB to Cassandra is not possible . Choose a file storage system # Like for databases, several options exist regarding file system. Basically, for standalone setups, using the local filesystem is the easiest solution. If installing a cluster, there are several options: Using a share NFS folder Using Apache Hadoop , a distributed file system Using a S3-compatible storage service ; for example with Min.IO Upgradability Starting with a standalone server and a local file storage and upgrading to a cluster with S3 of Hadoop is possible. Existing files can be moved to the targetted solutions. Choose an index system # Introduced with TheHive 4.1 to increase performances, TheHive relies on a dedicated indexing process. With a standalone setup, using a local index with Lucene is sufficient. In the case of a cluster, all nodes have to connect to the same index: an instance of Elasticsearch is then required. Upgradability Starting with a standalone server and Lucene and upgrading to a cluster with Elasticsearch is possible. Indices can be rebuilt. However, it can takes some time. Installation Guide # The following Guide let you prepare , install and configure TheHive and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages. If you want to build TheHive from sources, you can follow this guide . Configuration Guides # The configuration of TheHive is in files stored in the /etc/thehive folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/thehive \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by Debian or RPM packages. This file should contain a secret that should be used by one instance. Various aspects can configured in the application.conf file: database and indexing File storage Akka Authentication Connectors Cortex: connecting to one or more organisation MISP: connecting to one or more organisation Webhooks Streams Uses Cases # Basic stand alone server # Follow the installation guides for you prefered operating system. Cluster with 3 TheHive nodes # The folling guide details all the installation and configuration steps to get a cluster with 3 nodes working. The cluster is composed of: 3 TheHive servers 3 Cassandra servers 3 Min.IO servers","title":"Overview"},{"location":"thehive/installation-and-configuration/#installation-configuration-guides","text":"","title":"Installation &amp; configuration guides"},{"location":"thehive/installation-and-configuration/#overview","text":"The scalability of TheHive allows it to be set up as a standalone server or as nodes inside a cluster. Any number of nodes can rely on a database and a file system also setup as standalone servers or a cluster. Before starting installing and configuring, you need to identify and define the targetted architecture.","title":"Overview"},{"location":"thehive/installation-and-configuration/#choose-a-setup","text":"The modular architecture makes it support several types of database, file storage system and indexing system. The initial choices made with the target architecture and the setup are crucial, especially for the database. If high availability and fault tolerance are necessary, implementing a cluster might be the choice, and this choice determines the database, the file storage and indexing system to install. Hardware Pre-requisites Hardware requirements depends on the number of concurrent users and how they use the system. The following table give some information to choose the hardware. Number of users CPU RAM < 3 2 4-8 < 10 4 8-16 < 20 8 16-32","title":"Choose a setup"},{"location":"thehive/installation-and-configuration/#choose-a-database","text":"Once the target setup is identified, the first choice to make is the database. Even of local Berkeley DB and Cassandra database are supported, we recommend using Apache Cassandra , which is a scalable and high available Database, even for standalone servers. Berkeley DB can be enough for testing purposes. Upgradability This choice is decisive as migration from Berkeley DB to Cassandra is not possible .","title":"Choose a database"},{"location":"thehive/installation-and-configuration/#choose-a-file-storage-system","text":"Like for databases, several options exist regarding file system. Basically, for standalone setups, using the local filesystem is the easiest solution. If installing a cluster, there are several options: Using a share NFS folder Using Apache Hadoop , a distributed file system Using a S3-compatible storage service ; for example with Min.IO Upgradability Starting with a standalone server and a local file storage and upgrading to a cluster with S3 of Hadoop is possible. Existing files can be moved to the targetted solutions.","title":"Choose a file storage system"},{"location":"thehive/installation-and-configuration/#choose-an-index-system","text":"Introduced with TheHive 4.1 to increase performances, TheHive relies on a dedicated indexing process. With a standalone setup, using a local index with Lucene is sufficient. In the case of a cluster, all nodes have to connect to the same index: an instance of Elasticsearch is then required. Upgradability Starting with a standalone server and Lucene and upgrading to a cluster with Elasticsearch is possible. Indices can be rebuilt. However, it can takes some time.","title":"Choose an index system"},{"location":"thehive/installation-and-configuration/#installation-guide","text":"The following Guide let you prepare , install and configure TheHive and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages. If you want to build TheHive from sources, you can follow this guide .","title":"Installation Guide"},{"location":"thehive/installation-and-configuration/#configuration-guides","text":"The configuration of TheHive is in files stored in the /etc/thehive folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/thehive \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by Debian or RPM packages. This file should contain a secret that should be used by one instance. Various aspects can configured in the application.conf file: database and indexing File storage Akka Authentication Connectors Cortex: connecting to one or more organisation MISP: connecting to one or more organisation Webhooks Streams","title":"Configuration Guides"},{"location":"thehive/installation-and-configuration/#uses-cases","text":"","title":"Uses Cases"},{"location":"thehive/installation-and-configuration/#basic-stand-alone-server","text":"Follow the installation guides for you prefered operating system.","title":"Basic stand alone server"},{"location":"thehive/installation-and-configuration/#cluster-with-3-thehive-nodes","text":"The folling guide details all the installation and configuration steps to get a cluster with 3 nodes working. The cluster is composed of: 3 TheHive servers 3 Cassandra servers 3 Min.IO servers","title":"Cluster with 3 TheHive nodes"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/","text":"Use TheHive as a cluster # This guide provides configuration examples for TheHive, Cassandra and MinIO to build a fault-tolerant cluster of 3 active nodes. Prerequisite # 3 servers with TheHive and Cassandra installed. TheHive # In this guide, we are considering the node 1 to be the master node. Start by configuring akka component by editing the /etc/thehive/application.conf file of each node like this: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] } Cassandra # We are considering setting up a cluster of 3 active nodes of Cassandra with a replication factor of 3. That means that all nodes are active and the data is present on each node. This setup is tolerant to a 1 node failure. For the rest of this part, we are considering that all nodes sit on the same network. Configuration # Nodes configuration # For each node, update configuration files with the following parameters: /etc/cassandra/cassandra.yml cluster_name: 'thp' num_tokens: 256 authenticator: PasswordAuthenticator authorizer: CassandraAuthorizer role_manager: CassandraRoleManager data_file_directories: - /var/lib/cassandra/data commitlog_directory: /var/lib/cassandra/commitlog saved_caches_directory: /var/lib/cassandra/saved_caches seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"<ip node 1>, <ip node 2>, <ip node 3>\" listen_interface : eth0 rpc_interface: eth0 endpoint_snitch: SimpleSnitch Ensure to setup the right interface name. delete file /etc/cassandra/cassandra-topology.properties rm /etc/cassandra/cassandra-topology.properties Start nodes # On each node, start the service: service cassandra start Ensure that all nodes are up and running: root@cassandra:/# nodetool status Datacenter: dc1 =============== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN <ip node 1> 776.53 KiB 256 100.0% a79c9a8c-c99b-4d74-8e78-6b0c252abd86 rack1 UN <ip node 2> 671.72 KiB 256 100.0% 8fda2906-2097-4d62-91f8-005e33d3e839 rack1 UN <ip node 3> 611.54 KiB 256 100.0% 201ab99c-8e16-49b1-9b66-5444044fb1cd rack1 Initialise the database # On one node run (default password for cassandra account is cassandra ): cqlsh <ip node X> -u cassandra Start by changing the password of superadmin named cassandra : ALTER USER cassandra WITH PASSWORD 'NEWPASSWORD' ; exit and reconnect. Ensure user accounts are duplicated on all nodes ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Create keyspace named thehive CREATE KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : '3' } AND durable_writes = 'true' ; Create role thehive and grant permissions on thehive keyspace (choose a password) CREATE ROLE thehive WITH LOGIN = true AND PASSWORD = 'PASSWORD' ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO 'thehive' ; TheHive associated configuration # Update the configuration of thehive accordingly in /etc/thehive/application.conf : ## Database configuration db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"<ip node 1>\", \"<ip node 2>\", \"<ip node 3>\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"PASSWORD\" cql { cluster-name: thp keyspace: thehive } } Troubleshooting # InvalidRequest: code=2200 [Invalid query] message=\u201dorg.apache.cassandra.auth.CassandraRoleManager doesn\u2019t support PASSWORD\u201d. set the value authenticator: PasswordAuthenticator in cassandra.yaml Caused by: java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.UnauthorizedException: Unable to perform authorization of permissions: Unable to perform authorization of super-user permission: Cannot achieve consistency level LOCAL_ONE ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; MinIO # MinIO distributed mode requires fresh directories. Here is an example of implementation of MinIO with TheHive. The following procedure should be performed on all servers belonging the the cluster. We are considering the setup where the cluster is composed of 3 servers named minio1, minio2 & minio3. Create a dedicated system account # Create a dedicated user with /opt/minio as homedir. adduser minio Create at least 2 data volumes on each server # Create 2 folders on each server: mkdir -p /srv/minio/{1,2} chown -R minio:minio /srv/minio Setup hosts files # Edit /etc/hosts of all servers ip-minio-1 minio1 ip-minio-2 minio2 ip-minio-3 minio3 installation # cd /opt/minio mkdir /opt/minio/{bin,etc} wget -O /opt/minio/bin https://dl.minio.io/server/minio/release/linux-amd64/minio chown -R minio:minio /opt/minio Configuration # Create or edit file `/opt/minio/etc/minio.conf MINIO_OPTS=\"server --address :9100 http://minio{1...3}/srv/minio/{1...2}\" MINIO_ACCESS_KEY=\"<ACCESS_KEY>\" MINIO_SECRET_KEY=\"<SECRET_KEY>\" Create a service file named /usr/lib/systemd/system/minio.service [Unit] Description=minio Documentation=https://docs.min.io Wants=network-online.target After=network-online.target AssertFileIsExecutable=/opt/minio/bin/minio [Service] WorkingDirectory=/opt/minio User=minio Group=minio EnvironmentFile=/opt/minio/etc/minio.conf ExecStart=/opt/minio/bin/minio $MINIO_OPTS Restart=always LimitNOFILE=65536 TimeoutStopSec=0 SendSIGKILL=no [Install] WantedBy=multi-user.target Enable and start the service # systemctl daemon-reload systemctl enable minio systemctl start minio.service Prepare the service for TheHive # Following operations should be performed once all servers are up and running. A new server CAN NOT be added afterward. Connect using the access key and secret key to one server with your browser on port 9100: http://minio:9100 Create a bucket named thehive The bucket should be created and available on all your servers. TheHive associated configuration # For each TheHive node of the cluster, add the relevant storage configuration. Example for the first node thehive1: storage { provider: s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Each TheHive server can connect to one MinIO server.","title":"Use TheHive as a cluster"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#use-thehive-as-a-cluster","text":"This guide provides configuration examples for TheHive, Cassandra and MinIO to build a fault-tolerant cluster of 3 active nodes.","title":"Use TheHive as a cluster"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#prerequisite","text":"3 servers with TheHive and Cassandra installed.","title":"Prerequisite"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#thehive","text":"In this guide, we are considering the node 1 to be the master node. Start by configuring akka component by editing the /etc/thehive/application.conf file of each node like this: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] }","title":"TheHive"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#cassandra","text":"We are considering setting up a cluster of 3 active nodes of Cassandra with a replication factor of 3. That means that all nodes are active and the data is present on each node. This setup is tolerant to a 1 node failure. For the rest of this part, we are considering that all nodes sit on the same network.","title":"Cassandra"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#configuration","text":"","title":"Configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#nodes-configuration","text":"For each node, update configuration files with the following parameters: /etc/cassandra/cassandra.yml cluster_name: 'thp' num_tokens: 256 authenticator: PasswordAuthenticator authorizer: CassandraAuthorizer role_manager: CassandraRoleManager data_file_directories: - /var/lib/cassandra/data commitlog_directory: /var/lib/cassandra/commitlog saved_caches_directory: /var/lib/cassandra/saved_caches seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"<ip node 1>, <ip node 2>, <ip node 3>\" listen_interface : eth0 rpc_interface: eth0 endpoint_snitch: SimpleSnitch Ensure to setup the right interface name. delete file /etc/cassandra/cassandra-topology.properties rm /etc/cassandra/cassandra-topology.properties","title":"Nodes configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#start-nodes","text":"On each node, start the service: service cassandra start Ensure that all nodes are up and running: root@cassandra:/# nodetool status Datacenter: dc1 =============== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN <ip node 1> 776.53 KiB 256 100.0% a79c9a8c-c99b-4d74-8e78-6b0c252abd86 rack1 UN <ip node 2> 671.72 KiB 256 100.0% 8fda2906-2097-4d62-91f8-005e33d3e839 rack1 UN <ip node 3> 611.54 KiB 256 100.0% 201ab99c-8e16-49b1-9b66-5444044fb1cd rack1","title":"Start nodes"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#initialise-the-database","text":"On one node run (default password for cassandra account is cassandra ): cqlsh <ip node X> -u cassandra Start by changing the password of superadmin named cassandra : ALTER USER cassandra WITH PASSWORD 'NEWPASSWORD' ; exit and reconnect. Ensure user accounts are duplicated on all nodes ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Create keyspace named thehive CREATE KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : '3' } AND durable_writes = 'true' ; Create role thehive and grant permissions on thehive keyspace (choose a password) CREATE ROLE thehive WITH LOGIN = true AND PASSWORD = 'PASSWORD' ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO 'thehive' ;","title":"Initialise the database"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#thehive-associated-configuration","text":"Update the configuration of thehive accordingly in /etc/thehive/application.conf : ## Database configuration db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"<ip node 1>\", \"<ip node 2>\", \"<ip node 3>\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"PASSWORD\" cql { cluster-name: thp keyspace: thehive } }","title":"TheHive associated configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#troubleshooting","text":"InvalidRequest: code=2200 [Invalid query] message=\u201dorg.apache.cassandra.auth.CassandraRoleManager doesn\u2019t support PASSWORD\u201d. set the value authenticator: PasswordAuthenticator in cassandra.yaml Caused by: java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.UnauthorizedException: Unable to perform authorization of permissions: Unable to perform authorization of super-user permission: Cannot achieve consistency level LOCAL_ONE ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ;","title":"Troubleshooting"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#minio","text":"MinIO distributed mode requires fresh directories. Here is an example of implementation of MinIO with TheHive. The following procedure should be performed on all servers belonging the the cluster. We are considering the setup where the cluster is composed of 3 servers named minio1, minio2 & minio3.","title":"MinIO"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#create-a-dedicated-system-account","text":"Create a dedicated user with /opt/minio as homedir. adduser minio","title":"Create a dedicated system account"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#create-at-least-2-data-volumes-on-each-server","text":"Create 2 folders on each server: mkdir -p /srv/minio/{1,2} chown -R minio:minio /srv/minio","title":"Create at least 2 data volumes on each server"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#setup-hosts-files","text":"Edit /etc/hosts of all servers ip-minio-1 minio1 ip-minio-2 minio2 ip-minio-3 minio3","title":"Setup hosts files"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#installation","text":"cd /opt/minio mkdir /opt/minio/{bin,etc} wget -O /opt/minio/bin https://dl.minio.io/server/minio/release/linux-amd64/minio chown -R minio:minio /opt/minio","title":"installation"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#configuration_1","text":"Create or edit file `/opt/minio/etc/minio.conf MINIO_OPTS=\"server --address :9100 http://minio{1...3}/srv/minio/{1...2}\" MINIO_ACCESS_KEY=\"<ACCESS_KEY>\" MINIO_SECRET_KEY=\"<SECRET_KEY>\" Create a service file named /usr/lib/systemd/system/minio.service [Unit] Description=minio Documentation=https://docs.min.io Wants=network-online.target After=network-online.target AssertFileIsExecutable=/opt/minio/bin/minio [Service] WorkingDirectory=/opt/minio User=minio Group=minio EnvironmentFile=/opt/minio/etc/minio.conf ExecStart=/opt/minio/bin/minio $MINIO_OPTS Restart=always LimitNOFILE=65536 TimeoutStopSec=0 SendSIGKILL=no [Install] WantedBy=multi-user.target","title":"Configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#enable-and-start-the-service","text":"systemctl daemon-reload systemctl enable minio systemctl start minio.service","title":"Enable and start the service"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#prepare-the-service-for-thehive","text":"Following operations should be performed once all servers are up and running. A new server CAN NOT be added afterward. Connect using the access key and secret key to one server with your browser on port 9100: http://minio:9100 Create a bucket named thehive The bucket should be created and available on all your servers.","title":"Prepare the service for TheHive"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#thehive-associated-configuration_1","text":"For each TheHive node of the cluster, add the relevant storage configuration. Example for the first node thehive1: storage { provider: s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Each TheHive server can connect to one MinIO server.","title":"TheHive associated configuration"},{"location":"thehive/installation-and-configuration/configuration/akka/","text":"Cluster # Quote Akka is a toolkit for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala -- https://akka.io/ Akka is used to make several nodes of TheHive work together and offer a smooth user experience. A good cluster setup requires at least 3 nodes of THeHive applications. For each node, Akka must be configured like this: ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } with: remote.artery.hostname containing the hostname or IP address of the node, cluster.seed-nodes containing the list of akka nodes and beeing the same on all nodes Configuration of a Cluster with 3 nodes Node 1 Akka configuration for Node 1: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.1\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Node 2 Akka configuration for Node 2: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.2\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Node 3 Akka configuration for Node 3: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.3\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } SSL/TLS # Akka supports SSL/TLS to encrypt communications between nodes. A typical configuration with SSL support : ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } ssl.config-ssl-engine { key-store = \"<PATH TO KEYSTORE>\" trust-store = \"<PATH TO TRUSTSTORE>\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } Note Note that akka.remote.artery.transport has changed and akka.ssl.config-ssl-engine needs to be configured. Reference : https://doc.akka.io/docs/akka/current/remoting-artery.html#remote-security About Certificates Use your own internal PKI, or keytool commands to generate your certificates. Reference : https://lightbend.github.io/ssl-config/CertificateGeneration.html#using-keytool Your server certificates should contain various KeyUsage and ExtendedkeyUsage extensions to make everything work properly: KeyUsage extensions nonRepudiation dataEncipherment digitalSignature keyEncipherment ExtendedkeyUsage extensions serverAuth clientAuth Akka configuration with SSL for Node 1 ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"10.1.2.1\" port = 2551 } ssl.config-ssl-engine { key-store = \"/etc/thehive/application.conf.d/certs/10.1.2.1.jks\" trust-store = \"/etc/thehive/application.conf.d/certs/internal_ca.jks\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Apply the same principle for the other nodes, and restart all services.","title":"Cluster"},{"location":"thehive/installation-and-configuration/configuration/akka/#cluster","text":"Quote Akka is a toolkit for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala -- https://akka.io/ Akka is used to make several nodes of TheHive work together and offer a smooth user experience. A good cluster setup requires at least 3 nodes of THeHive applications. For each node, Akka must be configured like this: ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } with: remote.artery.hostname containing the hostname or IP address of the node, cluster.seed-nodes containing the list of akka nodes and beeing the same on all nodes Configuration of a Cluster with 3 nodes Node 1 Akka configuration for Node 1: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.1\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Node 2 Akka configuration for Node 2: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.2\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Node 3 Akka configuration for Node 3: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.3\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] }","title":"Cluster"},{"location":"thehive/installation-and-configuration/configuration/akka/#ssltls","text":"Akka supports SSL/TLS to encrypt communications between nodes. A typical configuration with SSL support : ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } ssl.config-ssl-engine { key-store = \"<PATH TO KEYSTORE>\" trust-store = \"<PATH TO TRUSTSTORE>\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } Note Note that akka.remote.artery.transport has changed and akka.ssl.config-ssl-engine needs to be configured. Reference : https://doc.akka.io/docs/akka/current/remoting-artery.html#remote-security About Certificates Use your own internal PKI, or keytool commands to generate your certificates. Reference : https://lightbend.github.io/ssl-config/CertificateGeneration.html#using-keytool Your server certificates should contain various KeyUsage and ExtendedkeyUsage extensions to make everything work properly: KeyUsage extensions nonRepudiation dataEncipherment digitalSignature keyEncipherment ExtendedkeyUsage extensions serverAuth clientAuth Akka configuration with SSL for Node 1 ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"10.1.2.1\" port = 2551 } ssl.config-ssl-engine { key-store = \"/etc/thehive/application.conf.d/certs/10.1.2.1.jks\" trust-store = \"/etc/thehive/application.conf.d/certs/internal_ca.jks\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Apply the same principle for the other nodes, and restart all services.","title":"SSL/TLS"},{"location":"thehive/installation-and-configuration/configuration/authentication/","text":"Authentication # Authentication consists of a set of module. Each one tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. The default configuration for authentication is: auth { providers = [ {name : session} {name : basic, realm : thehive} {name : local} {name : key} ] } Below are the available authentication modules: session # Authenticates HTTP requests using a cookie. This module manage the cookie creation and expiration. It accepts the following configuration parameters: Parameter Type Description inactivity duration the maximum time of user inactivity before the session is closed warning duration the time before the expiration TheHive returns a warning message Example auth { providers = [ { name : session inactivity : 600 minutes warning : 10 minutes } ] } local # Create a session if the provided login and password, or API key is correct according to the local user database. key # Authenticates HTTP requests using API key provided in the authorization header. The format is Authorization: Bearer xxx (xxx is replaced by the API key). The key is searched using other authentication modules (currently, only local authentication module can validate the key). basic # Authenticates HTTP requests using the login and password provided in authorization header using basic authentication format (Base64). Password is checked from the local user database. Parameter Type Description realm string name of the realm. Without this parameter, the browser doesn't ask to authenticate header # Authenticates HTTP requests using a HTTP header containing the user login. This is used to delegate authentication in a reverse proxy. This module accepts the configuration: Parameter Type Description userHeader string the name of the header that contain the user login ad # Use Microsoft ActiveDirectory to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the domain controllers. If missing, the dnsDomain is used winDomain string the Windows domain name ( MYDOMAIN ) dnsDomain string the Windows domain name in DNS format ( mydomain.local ) useSSL boolean indicate if SSL must be used to connect to domain controller. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ad , hosts : [ \"dc.mydomain.local\" ], dnsDomain : \"mydomain.local\" , winDomain : \"MYDOMAIN\" , } ] } ldap # Use LDAP directory server to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the LDAP servers bindDN string DN of the service account in LDAP. This account is used to search the user bindPW string password of the service account baseDN string DN where the users are located in filter string filter used to search the user. \"{0}\" is replaced by the user login. A valid filter is: (&(uid={0})(objectClass=posixAccount)) useSSL boolean indicate if SSL must be used to connect to LDAP server. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ldap hosts : [ ldap1.mydomain.local , ldap2.mydomain.local ] bindDN : \"cn=thehive,ou=services,dc=mydomain,dc=local\" bindPW : \"SuperSecretPassword\" baseDN : \"ou=users,dc=mydomain,dc=local\" filter : \"(cn={0})\" useSSL : true } ] } oauth2 # Authenticate the user using an external OAuth2 authenticator server. It accepts the following configuration parameters: Parameter Type Description clientId string client ID in the OAuth2 server clientSecret string client secret in the OAuth2 server redirectUri string the url of TheHive AOuth2 page ( xxx/api/ssoLogin ) responseType string type of the response. Currently only \"code\" is accepted grantType string type of the grant. Currently only \"authorization_code\" is accepted authorizationUrl string the url of the OAuth2 server tokenUrl string the token url of the OAuth2 server userUrl string the url to get user information in OAuth2 server scope list of string list of scope userIdField string the field that contains the id of the user in user info organisationField string (optional) the field that contains the organisation name in user info defaultOrganisation string (optional) the default organisation used to login if not present on user info authorizationHeader string prefix of the authorization header to get user info: Bearer, token, ... Example Keycloak ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/auth\" authorizationHeader : \"Bearer\" tokenUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/token\" userUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } Okta ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"https://OKTA/oauth2/v1/authorize\" authorizationHeader : \"Bearer\" tokenUrl : \"http://OKTA/oauth2/v1/token\" userUrl : \"http://OKTA/oauth2/v1/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } Github ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://github.com/login/oauth/authorize\" authorizationHeader : \"token\" tokenUrl : \"https://github.com/login/oauth/access_token\" userUrl : \"https://api.github.com/user\" scope : [ \"user\" ] userIdField : \"email\" #userOrganisation : \"\" } ] } Note CLIENT_ID and CLIENT_SECRET are created in the OAuth Apps section at https://github.com/settings/developers . this configuration requires that users set the Public email in their Public Profile on https://github.com/settings/profile . Microsoft 365 ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/authorize\" authorizationHeader : \"Bearer \" tokenUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/token\" userUrl : \"https://graph.microsoft.com/v1.0/me\" scope : [ \"User.Read\" ] userIdField : \"mail\" #userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note To create CLIENT_ID , CLIENT_SECRET and TENANT , register a new app at https://aad.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps . Google Note CLIENT_ID and CLIENT_SECRET are created in the _APIs & Services_ > _Credentials_ section of the GCP Console Instructions on how to create Oauth2 credentials at https://support.google.com/cloud/answer/6158849 For the latest reference for Google auth URLs please check Google's .well-known/openid-configuration User autocreation # To allow users to login without previously creating them, you can enable autocreation by adding user.autoCreateOnSso=true to the top level of your configuration. Example user.autoCreateOnSso : true user.profileFieldName : profile user.organisationFieldName : organisation user.defaults.profile : analyst user.defaults.organisation : cert Multi-Factor Authentication # Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page (top-Right corner button > Settings). User administrators can: See which users have activated MFA Reset MFA settings of any user This feature can be **disabled** by setting a config property to false : Example auth.multifactor.enabled = false","title":"Authentication"},{"location":"thehive/installation-and-configuration/configuration/authentication/#authentication","text":"Authentication consists of a set of module. Each one tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. The default configuration for authentication is: auth { providers = [ {name : session} {name : basic, realm : thehive} {name : local} {name : key} ] } Below are the available authentication modules:","title":"Authentication"},{"location":"thehive/installation-and-configuration/configuration/authentication/#session","text":"Authenticates HTTP requests using a cookie. This module manage the cookie creation and expiration. It accepts the following configuration parameters: Parameter Type Description inactivity duration the maximum time of user inactivity before the session is closed warning duration the time before the expiration TheHive returns a warning message Example auth { providers = [ { name : session inactivity : 600 minutes warning : 10 minutes } ] }","title":"session"},{"location":"thehive/installation-and-configuration/configuration/authentication/#local","text":"Create a session if the provided login and password, or API key is correct according to the local user database.","title":"local"},{"location":"thehive/installation-and-configuration/configuration/authentication/#key","text":"Authenticates HTTP requests using API key provided in the authorization header. The format is Authorization: Bearer xxx (xxx is replaced by the API key). The key is searched using other authentication modules (currently, only local authentication module can validate the key).","title":"key"},{"location":"thehive/installation-and-configuration/configuration/authentication/#basic","text":"Authenticates HTTP requests using the login and password provided in authorization header using basic authentication format (Base64). Password is checked from the local user database. Parameter Type Description realm string name of the realm. Without this parameter, the browser doesn't ask to authenticate","title":"basic"},{"location":"thehive/installation-and-configuration/configuration/authentication/#header","text":"Authenticates HTTP requests using a HTTP header containing the user login. This is used to delegate authentication in a reverse proxy. This module accepts the configuration: Parameter Type Description userHeader string the name of the header that contain the user login","title":"header"},{"location":"thehive/installation-and-configuration/configuration/authentication/#ad","text":"Use Microsoft ActiveDirectory to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the domain controllers. If missing, the dnsDomain is used winDomain string the Windows domain name ( MYDOMAIN ) dnsDomain string the Windows domain name in DNS format ( mydomain.local ) useSSL boolean indicate if SSL must be used to connect to domain controller. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ad , hosts : [ \"dc.mydomain.local\" ], dnsDomain : \"mydomain.local\" , winDomain : \"MYDOMAIN\" , } ] }","title":"ad"},{"location":"thehive/installation-and-configuration/configuration/authentication/#ldap","text":"Use LDAP directory server to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the LDAP servers bindDN string DN of the service account in LDAP. This account is used to search the user bindPW string password of the service account baseDN string DN where the users are located in filter string filter used to search the user. \"{0}\" is replaced by the user login. A valid filter is: (&(uid={0})(objectClass=posixAccount)) useSSL boolean indicate if SSL must be used to connect to LDAP server. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ldap hosts : [ ldap1.mydomain.local , ldap2.mydomain.local ] bindDN : \"cn=thehive,ou=services,dc=mydomain,dc=local\" bindPW : \"SuperSecretPassword\" baseDN : \"ou=users,dc=mydomain,dc=local\" filter : \"(cn={0})\" useSSL : true } ] }","title":"ldap"},{"location":"thehive/installation-and-configuration/configuration/authentication/#oauth2","text":"Authenticate the user using an external OAuth2 authenticator server. It accepts the following configuration parameters: Parameter Type Description clientId string client ID in the OAuth2 server clientSecret string client secret in the OAuth2 server redirectUri string the url of TheHive AOuth2 page ( xxx/api/ssoLogin ) responseType string type of the response. Currently only \"code\" is accepted grantType string type of the grant. Currently only \"authorization_code\" is accepted authorizationUrl string the url of the OAuth2 server tokenUrl string the token url of the OAuth2 server userUrl string the url to get user information in OAuth2 server scope list of string list of scope userIdField string the field that contains the id of the user in user info organisationField string (optional) the field that contains the organisation name in user info defaultOrganisation string (optional) the default organisation used to login if not present on user info authorizationHeader string prefix of the authorization header to get user info: Bearer, token, ... Example Keycloak ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/auth\" authorizationHeader : \"Bearer\" tokenUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/token\" userUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } Okta ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"https://OKTA/oauth2/v1/authorize\" authorizationHeader : \"Bearer\" tokenUrl : \"http://OKTA/oauth2/v1/token\" userUrl : \"http://OKTA/oauth2/v1/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } Github ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://github.com/login/oauth/authorize\" authorizationHeader : \"token\" tokenUrl : \"https://github.com/login/oauth/access_token\" userUrl : \"https://api.github.com/user\" scope : [ \"user\" ] userIdField : \"email\" #userOrganisation : \"\" } ] } Note CLIENT_ID and CLIENT_SECRET are created in the OAuth Apps section at https://github.com/settings/developers . this configuration requires that users set the Public email in their Public Profile on https://github.com/settings/profile . Microsoft 365 ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/authorize\" authorizationHeader : \"Bearer \" tokenUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/token\" userUrl : \"https://graph.microsoft.com/v1.0/me\" scope : [ \"User.Read\" ] userIdField : \"mail\" #userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note To create CLIENT_ID , CLIENT_SECRET and TENANT , register a new app at https://aad.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps . Google Note CLIENT_ID and CLIENT_SECRET are created in the _APIs & Services_ > _Credentials_ section of the GCP Console Instructions on how to create Oauth2 credentials at https://support.google.com/cloud/answer/6158849 For the latest reference for Google auth URLs please check Google's .well-known/openid-configuration","title":"oauth2"},{"location":"thehive/installation-and-configuration/configuration/authentication/#user-autocreation","text":"To allow users to login without previously creating them, you can enable autocreation by adding user.autoCreateOnSso=true to the top level of your configuration. Example user.autoCreateOnSso : true user.profileFieldName : profile user.organisationFieldName : organisation user.defaults.profile : analyst user.defaults.organisation : cert","title":"User autocreation"},{"location":"thehive/installation-and-configuration/configuration/authentication/#multi-factor-authentication","text":"Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page (top-Right corner button > Settings). User administrators can: See which users have activated MFA Reset MFA settings of any user This feature can be **disabled** by setting a config property to false : Example auth.multifactor.enabled = false","title":"Multi-Factor Authentication"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/","text":"TheHive connector: Cortex # Enable the connector # The Cortex connector module needs to be enabled to allow TheHive work with Cortex. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule Configure one connection # TheHive is able to connect more than one Cortex organisation. Several parameters can be configured for one server : Parameter Type Description name string name given to the Cortex instance (eg: Cortex-Internal ) url string url to connect to the Cortex instance auth dict method used to authenticate on the server ( bearer if using API keys) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy refreshDelay duration frequency of job updates checks (default: 5 seconds ) maxRetryOnError integer maximum number of successive errors before give up (default: 3 ) statusCheckInterval duration check remote Cortex status time interval (default: 1 minute ) includedTheHiveOrganisations list of string list of TheHive organisations which can use this Cortex server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this Cortex server (default: None ( [] ) ) This configuration has to be added to TheHive conf/application.conf file. ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = local url = \"http://localhost:9001\" auth { type = \"bearer\" key = \"[REDACTED]\" } wsConfig {} includedTheHiveOrganisations = [\"*\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } Note By default, adding a Cortex server in TheHive configuration make it available for all organisations added on the instance. Example 1 server Configuration with one Cortex connection: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } more servers Configuration with 2 Cortex connections: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex1 url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig {} includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } { name = Cortex2 url = \"http://cortex2:9001\" auth { type = \"bearer\" key = \"lSDkjDGGGHtipueroBHOroNJKLbpi\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG2\", \"ORG3\"] excludedTheHiveOrganisations = [\"ORG1\"] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 5 minutes }","title":"Cortex connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/#thehive-connector-cortex","text":"","title":"TheHive connector: Cortex"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/#enable-the-connector","text":"The Cortex connector module needs to be enabled to allow TheHive work with Cortex. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule","title":"Enable the connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/#configure-one-connection","text":"TheHive is able to connect more than one Cortex organisation. Several parameters can be configured for one server : Parameter Type Description name string name given to the Cortex instance (eg: Cortex-Internal ) url string url to connect to the Cortex instance auth dict method used to authenticate on the server ( bearer if using API keys) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy refreshDelay duration frequency of job updates checks (default: 5 seconds ) maxRetryOnError integer maximum number of successive errors before give up (default: 3 ) statusCheckInterval duration check remote Cortex status time interval (default: 1 minute ) includedTheHiveOrganisations list of string list of TheHive organisations which can use this Cortex server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this Cortex server (default: None ( [] ) ) This configuration has to be added to TheHive conf/application.conf file. ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = local url = \"http://localhost:9001\" auth { type = \"bearer\" key = \"[REDACTED]\" } wsConfig {} includedTheHiveOrganisations = [\"*\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } Note By default, adding a Cortex server in TheHive configuration make it available for all organisations added on the instance. Example 1 server Configuration with one Cortex connection: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } more servers Configuration with 2 Cortex connections: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex1 url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig {} includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } { name = Cortex2 url = \"http://cortex2:9001\" auth { type = \"bearer\" key = \"lSDkjDGGGHtipueroBHOroNJKLbpi\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG2\", \"ORG3\"] excludedTheHiveOrganisations = [\"ORG1\"] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 5 minutes }","title":"Configure one connection"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/","text":"TheHive connector: MISP # Enable MISP connector # The MISP connector module needs to be enabled to allow TheHive to interact with MISP. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.misp.MispModule Configuration # TheHive is able to connect to more than one MISP server for pulling, pushing or both. Several parameters can be configured for one server : Parameter Type Description interval duration delay between to pull/push events to remote MISP servers. This is a common parameter for all configured server name string name given to the MISP instance (eg: MISP-MyOrg ) url string url to connect to the MISP instance auth dict method used to authenticate on the server ( key if using API keys) purpose string define the purpose of the server MISP: ImportOnly , ExportOnly or ImportAndExport (default: ImportAndExport ) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy caseTemplate string case template used by default in TheHive to import events as Alerts tags list of string tags to be added to events imported as Alerts in TheHive exportCaseTags boolean indicate if the tags of the case should be exported to MISP event (default: false) Optional parameters can be added to filter out some events coming into TheHive: Parameter Type Description exclusion.organisations list of string list of MISP organisation from which event will not be imported exclusion.tags list of string don't import MISP events which have one of these tags whitelist.organisations list of string import only events from these MISP organisations whitelist.tags list of string import only MISP events which have one of these tags max-age duration maximum age of the last publish date of event to be imported in TheHive includedTheHiveOrganisations list of string list of TheHive organisations which can use this MISP server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this MISP server (default: None ( [] ) ) Additionally, some organisations or tags from MISP can be defined to exclude events. Note By default, adding a MISP server in TheHive configuration make it available for all organisations added on the instance. This configuration has to be added to TheHive conf/application.conf file: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"local\" url = \"http : //localhost/\" auth { type = key key = \"***\" } wsConfig {} caseTemplate = \"<Template_Name_goes_here>\" tags = [ \"misp-server-id\" ] max-age = 7 days exclusion { organisation = [ \"bad organisation\" , \"other orga\" ] tags = [ \"tag1\" , \"tag2\" ] } whitelist { tags = [ \"tag1\" , \"tag2\" ] } includedTheHiveOrganisations = [ \"*\" ] excludedTheHiveOrganisations = [] } ] } Example Connection with 1 MISP server: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"MISP Server\" url = \"https : //misp.server\" auth { type = key key = \"XhtropikjthiuGIORWUHHlLhlfeerljta\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } tags = [ \"tag1\" , \"tag2\" , \"tag3\" ] caseTemplate = \"misp\" includedTheHiveOrganisations = [ \"ORG1\" , \"ORG2\" ] } ] }","title":"MISP connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/#thehive-connector-misp","text":"","title":"TheHive connector: MISP"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/#enable-misp-connector","text":"The MISP connector module needs to be enabled to allow TheHive to interact with MISP. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.misp.MispModule","title":"Enable MISP connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/#configuration","text":"TheHive is able to connect to more than one MISP server for pulling, pushing or both. Several parameters can be configured for one server : Parameter Type Description interval duration delay between to pull/push events to remote MISP servers. This is a common parameter for all configured server name string name given to the MISP instance (eg: MISP-MyOrg ) url string url to connect to the MISP instance auth dict method used to authenticate on the server ( key if using API keys) purpose string define the purpose of the server MISP: ImportOnly , ExportOnly or ImportAndExport (default: ImportAndExport ) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy caseTemplate string case template used by default in TheHive to import events as Alerts tags list of string tags to be added to events imported as Alerts in TheHive exportCaseTags boolean indicate if the tags of the case should be exported to MISP event (default: false) Optional parameters can be added to filter out some events coming into TheHive: Parameter Type Description exclusion.organisations list of string list of MISP organisation from which event will not be imported exclusion.tags list of string don't import MISP events which have one of these tags whitelist.organisations list of string import only events from these MISP organisations whitelist.tags list of string import only MISP events which have one of these tags max-age duration maximum age of the last publish date of event to be imported in TheHive includedTheHiveOrganisations list of string list of TheHive organisations which can use this MISP server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this MISP server (default: None ( [] ) ) Additionally, some organisations or tags from MISP can be defined to exclude events. Note By default, adding a MISP server in TheHive configuration make it available for all organisations added on the instance. This configuration has to be added to TheHive conf/application.conf file: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"local\" url = \"http : //localhost/\" auth { type = key key = \"***\" } wsConfig {} caseTemplate = \"<Template_Name_goes_here>\" tags = [ \"misp-server-id\" ] max-age = 7 days exclusion { organisation = [ \"bad organisation\" , \"other orga\" ] tags = [ \"tag1\" , \"tag2\" ] } whitelist { tags = [ \"tag1\" , \"tag2\" ] } includedTheHiveOrganisations = [ \"*\" ] excludedTheHiveOrganisations = [] } ] } Example Connection with 1 MISP server: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"MISP Server\" url = \"https : //misp.server\" auth { type = key key = \"XhtropikjthiuGIORWUHHlLhlfeerljta\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } tags = [ \"tag1\" , \"tag2\" , \"tag3\" ] caseTemplate = \"misp\" includedTheHiveOrganisations = [ \"ORG1\" , \"ORG2\" ] } ] }","title":"Configuration"},{"location":"thehive/installation-and-configuration/configuration/database/","text":"Database and index configuration # TheHive can be configured to connect to local Berkeley database or Cassandra database. Tip Using Cassandra is strongly recommended for production use while Berkeley DB can be prefered for testing and training purpose. Starting with TheHive 4.1.0, indexes are managed by a dedicated engine. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch . Configuation # A typical database configuration for TheHive looks like this: ## Database configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"IP_ADDRESS\"] cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : lucene directory: /path/to/index/folder } } } } List of possible parameters # Parameter Type Description provider string provider name. Default: janusgraph storage dict storage configuration storage.backend string storage type. Can be cql or berkeleyje storage.hostname list of string list of IP addresses or hostnames when using cql backend storage.directory string local path for data when using berkeleyje backend storage.username string account username with cql backend if Cassandra auth is configured storage.password string account password with cql backend if Cassandra auth is configured storage.port integer port number with cql backend ( 9042 by default). Change this if using an alternate port or a dedicated port number when using SSL with Cassandra storage.cql dict configuration for cql backend if used storage.cql.cluster-name string name of the cluster name used in the configuration of Apache Cassandra storage.cql.keyspace string Keyspace name used to store TheHive data in Apache Cassandra storage.cql.ssl.enabled boolean false by default. set it to true if SSL is used with Cassandra storage.cql.ssl.truststore.location string path the the truststore. Specify it when using SSL with Cassandra storage.cql.ssl.password string password to access the truststore storage.cql.ssl.client-authentication-enabled boolean Enables use of a client key to authenticate with Cassandra storage.cql.ssl.keystore.location string path the the keystore. Specify it when using SSL and client auth. with Cassandra storage.cql.ssl.keystore.keypassword string password to access the key in the keystore storage.cql.ssl.truststore.storepassword string password the access the keystore index.search dict configuration for indexes index.search.backend string index engine. Default: lucene provided with TheHive. Can also be elasticsearch index.search.directory string path to folder where indexes should be stored, when using lucene engine index.search.hostname list of string list of IP addresses or hostnames when using elasticsearch engine index.search.index-name string name of index, when using elasticseach engine index.search.elasticsearch.http.auth.type: basic string basic is the only possible value index.search.elasticsearch.http.auth.basic.username string Username account on Elasticsearch index.search.elasticsearch.http.auth.basic.password string Password of the account on Elasticsearch index.search.elasticsearch.ssl.enabled boolean Enable SSL true/false index.search.elasticsearch.ssl.truststore.location string Location of the truststore index.search.elasticsearch.ssl.truststore.password string Password of the truststore index.search.elasticsearch.ssl.keystore.location string Location of the keystore for client authentication index.search.elasticsearch.ssl.keystore.storepassword string Password of the keystore index.search.elasticsearch.ssl.keystore.keypassword string Password of the client certificate index.search.elasticsearch.ssl.disable-hostname-verification boolean Disable SSL verification true/false index.search.elasticsearch.ssl.allow-self-signed-certificates boolean Allow self signe certificates true/false Warning Using Elasticsearch to manage indexes is required if you are setting up TheHive as a cluster. The initial start , or first start after configuring indexes might take some time if the database contains a large amount of data. This time is due to the indexes creation More information on configuration for Elasticsearch connection: https://docs.janusgraph.org/index-backend/elasticsearch/ . Use cases # Database and index engine can be different, depending on the use case and target setup: Example Testing server For such use cases, local database and indexes are adequate: Create a dedicated folder for data and for indexes. These folders should belong to the user thehive:thehive . mkdir /opt/thp/thehive/ { data, index } chown -R thehive:thehive /opt/thp/thehive/ { data, index } Configure TheHive accordingly: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: berkeleyje directory: /opt/thp/thehive/database } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Standalone server with Cassandra Install a Cassandra server locally Create a dedicated folder for indexes. This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Cluster with Cassandra & Elasticsearch Install a cluster of Cassandra servers Get access to an Elasticsearch server Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive elasticsearch { http { auth { type: basic basic { username: httpuser password: httppassword } } } ssl { enabled: true truststore { location: /path/to/your/truststore.jks password: truststorepwd } } } } } } } Warning In this configuration, all TheHive nodes should have the same configuration. Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored","title":"Database & indexes"},{"location":"thehive/installation-and-configuration/configuration/database/#database-and-index-configuration","text":"TheHive can be configured to connect to local Berkeley database or Cassandra database. Tip Using Cassandra is strongly recommended for production use while Berkeley DB can be prefered for testing and training purpose. Starting with TheHive 4.1.0, indexes are managed by a dedicated engine. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch .","title":"Database and index configuration"},{"location":"thehive/installation-and-configuration/configuration/database/#configuation","text":"A typical database configuration for TheHive looks like this: ## Database configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"IP_ADDRESS\"] cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : lucene directory: /path/to/index/folder } } } }","title":"Configuation"},{"location":"thehive/installation-and-configuration/configuration/database/#list-of-possible-parameters","text":"Parameter Type Description provider string provider name. Default: janusgraph storage dict storage configuration storage.backend string storage type. Can be cql or berkeleyje storage.hostname list of string list of IP addresses or hostnames when using cql backend storage.directory string local path for data when using berkeleyje backend storage.username string account username with cql backend if Cassandra auth is configured storage.password string account password with cql backend if Cassandra auth is configured storage.port integer port number with cql backend ( 9042 by default). Change this if using an alternate port or a dedicated port number when using SSL with Cassandra storage.cql dict configuration for cql backend if used storage.cql.cluster-name string name of the cluster name used in the configuration of Apache Cassandra storage.cql.keyspace string Keyspace name used to store TheHive data in Apache Cassandra storage.cql.ssl.enabled boolean false by default. set it to true if SSL is used with Cassandra storage.cql.ssl.truststore.location string path the the truststore. Specify it when using SSL with Cassandra storage.cql.ssl.password string password to access the truststore storage.cql.ssl.client-authentication-enabled boolean Enables use of a client key to authenticate with Cassandra storage.cql.ssl.keystore.location string path the the keystore. Specify it when using SSL and client auth. with Cassandra storage.cql.ssl.keystore.keypassword string password to access the key in the keystore storage.cql.ssl.truststore.storepassword string password the access the keystore index.search dict configuration for indexes index.search.backend string index engine. Default: lucene provided with TheHive. Can also be elasticsearch index.search.directory string path to folder where indexes should be stored, when using lucene engine index.search.hostname list of string list of IP addresses or hostnames when using elasticsearch engine index.search.index-name string name of index, when using elasticseach engine index.search.elasticsearch.http.auth.type: basic string basic is the only possible value index.search.elasticsearch.http.auth.basic.username string Username account on Elasticsearch index.search.elasticsearch.http.auth.basic.password string Password of the account on Elasticsearch index.search.elasticsearch.ssl.enabled boolean Enable SSL true/false index.search.elasticsearch.ssl.truststore.location string Location of the truststore index.search.elasticsearch.ssl.truststore.password string Password of the truststore index.search.elasticsearch.ssl.keystore.location string Location of the keystore for client authentication index.search.elasticsearch.ssl.keystore.storepassword string Password of the keystore index.search.elasticsearch.ssl.keystore.keypassword string Password of the client certificate index.search.elasticsearch.ssl.disable-hostname-verification boolean Disable SSL verification true/false index.search.elasticsearch.ssl.allow-self-signed-certificates boolean Allow self signe certificates true/false Warning Using Elasticsearch to manage indexes is required if you are setting up TheHive as a cluster. The initial start , or first start after configuring indexes might take some time if the database contains a large amount of data. This time is due to the indexes creation More information on configuration for Elasticsearch connection: https://docs.janusgraph.org/index-backend/elasticsearch/ .","title":"List of possible parameters"},{"location":"thehive/installation-and-configuration/configuration/database/#use-cases","text":"Database and index engine can be different, depending on the use case and target setup: Example Testing server For such use cases, local database and indexes are adequate: Create a dedicated folder for data and for indexes. These folders should belong to the user thehive:thehive . mkdir /opt/thp/thehive/ { data, index } chown -R thehive:thehive /opt/thp/thehive/ { data, index } Configure TheHive accordingly: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: berkeleyje directory: /opt/thp/thehive/database } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Standalone server with Cassandra Install a Cassandra server locally Create a dedicated folder for indexes. This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Cluster with Cassandra & Elasticsearch Install a cluster of Cassandra servers Get access to an Elasticsearch server Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive elasticsearch { http { auth { type: basic basic { username: httpuser password: httppassword } } } ssl { enabled: true truststore { location: /path/to/your/truststore.jks password: truststorepwd } } } } } } } Warning In this configuration, all TheHive nodes should have the same configuration. Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored","title":"Use cases"},{"location":"thehive/installation-and-configuration/configuration/file-storage/","text":"File storage configuration # TheHive can be configured to use local or distributed filesystems. Example Local or NFS Create dedicated folder ; it should belong to user and group thehive:thehive . mkdir /opt/thp/thehive/files chown thehive:thehive /opt/thp/thehive/files Configure TheHive accordingly: ## Attachment storage configuration storage { ## Local filesystem provider : localfs localfs { location : /opt/thp/thehive/files } } Min.IO Install a Min.IO cluster Configure each node of TheHive accordingly: ## Attachment storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://10.1.2.4:9100\" accessKey = \"thehive\" secretKey = \"minio_password\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Apache Hadoop Install an Apache Hadoop server Configure each node of TheHive accordingly ( /etc/thehive/application.conf ): ## Attachment storage configuration ## Hadoop filesystem (HDFS) provider : hdfs hdfs { root : \"hdfs://10.1.2.4:10000\" # namenode server hostname location : \"/thehive\" # location inside HDFS username : thehive # file owner } }","title":"File Storage"},{"location":"thehive/installation-and-configuration/configuration/file-storage/#file-storage-configuration","text":"TheHive can be configured to use local or distributed filesystems. Example Local or NFS Create dedicated folder ; it should belong to user and group thehive:thehive . mkdir /opt/thp/thehive/files chown thehive:thehive /opt/thp/thehive/files Configure TheHive accordingly: ## Attachment storage configuration storage { ## Local filesystem provider : localfs localfs { location : /opt/thp/thehive/files } } Min.IO Install a Min.IO cluster Configure each node of TheHive accordingly: ## Attachment storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://10.1.2.4:9100\" accessKey = \"thehive\" secretKey = \"minio_password\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Apache Hadoop Install an Apache Hadoop server Configure each node of TheHive accordingly ( /etc/thehive/application.conf ): ## Attachment storage configuration ## Hadoop filesystem (HDFS) provider : hdfs hdfs { root : \"hdfs://10.1.2.4:10000\" # namenode server hostname location : \"/thehive\" # location inside HDFS username : thehive # file owner } }","title":"File storage configuration"},{"location":"thehive/installation-and-configuration/configuration/logs/","text":"","title":"Logs"},{"location":"thehive/installation-and-configuration/configuration/manage-configuration/","text":"Manage configuration files # TheHive uses HOCON as configuration file format. This format gives enough flexibility to structure and organise the configuration of TheHive. TheHive is delivered with following files, in the folder /etc/thehive : logback.xml containing the log policy secret.conf containing a secret key used to create sessions. This key should be unique per instance (in the case of a cluster, this key should be the same for all nodes of this cluster) application.conf HOCON file format let you organise the configuration to have separate files for each purpose. It is the possible to create a /etc/thehive/application.conf.d folder and have several files inside that will be included in the main file /etc/thehive/application.conf . At the end, the following configuration structure is possible: /etc/thehive |-- application.conf |-- application.conf.d | |-- secret.conf | |-- service.conf | |-- database.conf | |-- storage.conf | |-- cluster.conf | |-- authentication.conf | |-- cortex.conf | |-- misp.conf | `-- webhooks.conf `-- logback.xml And the content of /etc/thehive/application.conf : ### ## Documentation is available at https://docs.thehive-project.org ### ## Include Play secret key # More information on secret key at https://www.playframework.com/documentation/2.8.x/ApplicationSecret include \"/etc/thehive/application.conf.d/secret.conf\" ## Service include \"/etc/thehive/application.conf.d/service.conf\" ## Database include \"/etc/thehive/application.conf.d/database.conf\" ## Storage include \"/etc/thehive/application.conf.d/storage.conf\" ## Cluster include \"/etc/thehive/application.conf.d/cluster.conf\" ## Authentication include \"/etc/thehive/application.conf.d/authentication.conf\" ## Cortex include \"/etc/thehive/application.conf.d/cortex.conf\" ## MISP include \"/etc/thehive/application.conf.d/misp.conf\" ## Webhooks include \"/etc/thehive/application.conf.d/webhooks.conf\"","title":"Manage Configuration"},{"location":"thehive/installation-and-configuration/configuration/manage-configuration/#manage-configuration-files","text":"TheHive uses HOCON as configuration file format. This format gives enough flexibility to structure and organise the configuration of TheHive. TheHive is delivered with following files, in the folder /etc/thehive : logback.xml containing the log policy secret.conf containing a secret key used to create sessions. This key should be unique per instance (in the case of a cluster, this key should be the same for all nodes of this cluster) application.conf HOCON file format let you organise the configuration to have separate files for each purpose. It is the possible to create a /etc/thehive/application.conf.d folder and have several files inside that will be included in the main file /etc/thehive/application.conf . At the end, the following configuration structure is possible: /etc/thehive |-- application.conf |-- application.conf.d | |-- secret.conf | |-- service.conf | |-- database.conf | |-- storage.conf | |-- cluster.conf | |-- authentication.conf | |-- cortex.conf | |-- misp.conf | `-- webhooks.conf `-- logback.xml And the content of /etc/thehive/application.conf : ### ## Documentation is available at https://docs.thehive-project.org ### ## Include Play secret key # More information on secret key at https://www.playframework.com/documentation/2.8.x/ApplicationSecret include \"/etc/thehive/application.conf.d/secret.conf\" ## Service include \"/etc/thehive/application.conf.d/service.conf\" ## Database include \"/etc/thehive/application.conf.d/database.conf\" ## Storage include \"/etc/thehive/application.conf.d/storage.conf\" ## Cluster include \"/etc/thehive/application.conf.d/cluster.conf\" ## Authentication include \"/etc/thehive/application.conf.d/authentication.conf\" ## Cortex include \"/etc/thehive/application.conf.d/cortex.conf\" ## MISP include \"/etc/thehive/application.conf.d/misp.conf\" ## Webhooks include \"/etc/thehive/application.conf.d/webhooks.conf\"","title":"Manage configuration files"},{"location":"thehive/installation-and-configuration/configuration/secret/","text":"secret.conf file # This file contains a secret that is used to define cookies used to manage the users session. As a result, one instance of TheHive should use a unique secret key. Example ## Play secret key play.http.secret.key=\"dgngu325mbnbc39cxas4l5kb24503836y2vsvsg465989fbsvop9d09ds6df6\" Warning In the case of a cluster of TheHive nodes, all nodes should have the same secret.conf file with the same secret key. The secret is used to generate user sessions.","title":"Secret key"},{"location":"thehive/installation-and-configuration/configuration/secret/#secretconf-file","text":"This file contains a secret that is used to define cookies used to manage the users session. As a result, one instance of TheHive should use a unique secret key. Example ## Play secret key play.http.secret.key=\"dgngu325mbnbc39cxas4l5kb24503836y2vsvsg465989fbsvop9d09ds6df6\" Warning In the case of a cluster of TheHive nodes, all nodes should have the same secret.conf file with the same secret key. The secret is used to generate user sessions.","title":"secret.conf file"},{"location":"thehive/installation-and-configuration/configuration/service/","text":"Service # Listen address & port # By default the application listens on all interfaces and port 9000 . This is possible to specify listen address and ports with following parameters in the application.conf file: http.address=127.0.0.1 http.port=9000 Context # If you are using a reverse proxy, and you want to specify a location (ex: /thehive ), updating the configuration of TheHive is also required Example play.http.context: \"/thehive\" Specific configuration for streams # If you are using a reverse proxy like Nginx, you might receive error popups with the following message: StreamSrv 504 Gateway Time-Out . You need to change default setting for long polling refresh, Set stream.longPolling.refresh accordingly. Example stream.longPolling.refresh: 45 seconds Manage content lengh # Content length of text and files managed by the application are limited by default. Before TheHive v4.1.1 , the Play framework sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. Since TheHive v4.1.1 , these values are set with default parameters: # Max file size play.http.parser.maxDiskBuffer : 128MB # Max textual content length play.http.parser.maxMemoryBuffer : 256kB If you feel that these should be updated, edit /etc/thehive/application.conf file and update these parameters accordingly. Tip if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"Service"},{"location":"thehive/installation-and-configuration/configuration/service/#service","text":"","title":"Service"},{"location":"thehive/installation-and-configuration/configuration/service/#listen-address-port","text":"By default the application listens on all interfaces and port 9000 . This is possible to specify listen address and ports with following parameters in the application.conf file: http.address=127.0.0.1 http.port=9000","title":"Listen address &amp; port"},{"location":"thehive/installation-and-configuration/configuration/service/#context","text":"If you are using a reverse proxy, and you want to specify a location (ex: /thehive ), updating the configuration of TheHive is also required Example play.http.context: \"/thehive\"","title":"Context"},{"location":"thehive/installation-and-configuration/configuration/service/#specific-configuration-for-streams","text":"If you are using a reverse proxy like Nginx, you might receive error popups with the following message: StreamSrv 504 Gateway Time-Out . You need to change default setting for long polling refresh, Set stream.longPolling.refresh accordingly. Example stream.longPolling.refresh: 45 seconds","title":"Specific configuration for streams"},{"location":"thehive/installation-and-configuration/configuration/service/#manage-content-lengh","text":"Content length of text and files managed by the application are limited by default. Before TheHive v4.1.1 , the Play framework sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. Since TheHive v4.1.1 , these values are set with default parameters: # Max file size play.http.parser.maxDiskBuffer : 128MB # Max textual content length play.http.parser.maxMemoryBuffer : 256kB If you feel that these should be updated, edit /etc/thehive/application.conf file and update these parameters accordingly. Tip if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"Manage content lengh"},{"location":"thehive/installation-and-configuration/configuration/webhooks/","text":"TheHive webhooks # TheHive can notify external system of modification events (case creation, alert update, task assignment, ...). To use webhooks notifications, 2 steps are required: configure a notification, and activate it. 1. Define webhook endpoints # The configuration can accept following parameters: Parameter Type Description name string the identifier of the endpoint. It is used when the webhook is setup for an organisation version integer defines the format of the message. If version is 0 , TheHive will send messages with the same format as TheHive3. Currently TheHive only supports version 0. wsConfig dict the configuration of HTTP client. It contains proxy, SSL and timeout configuration. auth dict the configuration of authenticationI. It contains type, and additional options. includedTheHiveOrganisations list of string list of TheHive organisations which can use this endpoint (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this endpoint (default: None ( [] ) ) The following section should be added in application.conf : ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig : {} auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ] Use a proxy # Wehbook call can go through a proxy, in which case, Webhooks configuration requires a wsConfig config notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ] Use an authentication method # Webhook endpoints can be authenticated, in this case, Webhook configuration requires a auth setting. Supported methods are: No Auth (Default) auth : { type : \"none\" } Basic Auth auth : { type : \"basic\" , username : \"foo\" , password : \"bar\" } Beared Auth auth : { type : \"bearer\" , key : \"foobar\" } Key Auth auth : { type : \"key\" , key : \"foobar\" } Warning In 4.1.0 release, the auth config is REQUIRED . Examples # Example ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"bearer\" , key : \"API_KEY\" } includedTheHiveOrganisations : [ \"ORG1\" , \"ORG2\" ] excludedTheHiveOrganisations : [ \"ORG3\" ] } ] 2. Activate webhooks # This action must be done by an organisation admin (with permission manageConfig ) and requires to run a curl command: read -p 'Enter the URL of TheHive: ' thehive_url read -p 'Enter your login: ' thehive_user read -s -p 'Enter your password: ' thehive_password curl -XPUT -u $thehive_user : $thehive_password -H 'Content-type: application/json' $thehive_url /api/config/organisation/notification -d ' { \"value\": [ { \"delegate\": false, \"trigger\": { \"name\": \"AnyEvent\"}, \"notifier\": { \"name\": \"webhook\", \"endpoint\": \"local\" } } ] }'","title":"Webhooks"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#thehive-webhooks","text":"TheHive can notify external system of modification events (case creation, alert update, task assignment, ...). To use webhooks notifications, 2 steps are required: configure a notification, and activate it.","title":"TheHive webhooks"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#1-define-webhook-endpoints","text":"The configuration can accept following parameters: Parameter Type Description name string the identifier of the endpoint. It is used when the webhook is setup for an organisation version integer defines the format of the message. If version is 0 , TheHive will send messages with the same format as TheHive3. Currently TheHive only supports version 0. wsConfig dict the configuration of HTTP client. It contains proxy, SSL and timeout configuration. auth dict the configuration of authenticationI. It contains type, and additional options. includedTheHiveOrganisations list of string list of TheHive organisations which can use this endpoint (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this endpoint (default: None ( [] ) ) The following section should be added in application.conf : ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig : {} auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ]","title":"1. Define webhook endpoints"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#use-a-proxy","text":"Wehbook call can go through a proxy, in which case, Webhooks configuration requires a wsConfig config notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ]","title":"Use a proxy"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#use-an-authentication-method","text":"Webhook endpoints can be authenticated, in this case, Webhook configuration requires a auth setting. Supported methods are: No Auth (Default) auth : { type : \"none\" } Basic Auth auth : { type : \"basic\" , username : \"foo\" , password : \"bar\" } Beared Auth auth : { type : \"bearer\" , key : \"foobar\" } Key Auth auth : { type : \"key\" , key : \"foobar\" } Warning In 4.1.0 release, the auth config is REQUIRED .","title":"Use an authentication method"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#examples","text":"Example ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"bearer\" , key : \"API_KEY\" } includedTheHiveOrganisations : [ \"ORG1\" , \"ORG2\" ] excludedTheHiveOrganisations : [ \"ORG3\" ] } ]","title":"Examples"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#2-activate-webhooks","text":"This action must be done by an organisation admin (with permission manageConfig ) and requires to run a curl command: read -p 'Enter the URL of TheHive: ' thehive_url read -p 'Enter your login: ' thehive_user read -s -p 'Enter your password: ' thehive_password curl -XPUT -u $thehive_user : $thehive_password -H 'Content-type: application/json' $thehive_url /api/config/organisation/notification -d ' { \"value\": [ { \"delegate\": false, \"trigger\": { \"name\": \"AnyEvent\"}, \"notifier\": { \"name\": \"webhook\", \"endpoint\": \"local\" } } ] }'","title":"2. Activate webhooks"},{"location":"thehive/installation-and-configuration/installation/build-sources/","text":"Installing and running from sources # Dependencies # System packages # apt-get install apt-transport-https NPM # curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.0/install.sh | bash Bower and Grunt # nvm install --lts npm install -g bower grunt Build # The backend cd /opt git clone https://github.com/TheHive-Project/TheHive.git cd TheHive git checkout scalligraph git submodule init git submodule update ./sbt stage The UI cd /opt/TheHive/frontend npm install bower install grunt build","title":"Build sources"},{"location":"thehive/installation-and-configuration/installation/build-sources/#installing-and-running-from-sources","text":"","title":"Installing and running from sources"},{"location":"thehive/installation-and-configuration/installation/build-sources/#dependencies","text":"","title":"Dependencies"},{"location":"thehive/installation-and-configuration/installation/build-sources/#system-packages","text":"apt-get install apt-transport-https","title":"System packages"},{"location":"thehive/installation-and-configuration/installation/build-sources/#npm","text":"curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.0/install.sh | bash","title":"NPM"},{"location":"thehive/installation-and-configuration/installation/build-sources/#bower-and-grunt","text":"nvm install --lts npm install -g bower grunt","title":"Bower and Grunt"},{"location":"thehive/installation-and-configuration/installation/build-sources/#build","text":"The backend cd /opt git clone https://github.com/TheHive-Project/TheHive.git cd TheHive git checkout scalligraph git submodule init git submodule update ./sbt stage The UI cd /opt/TheHive/frontend npm install bower install grunt build","title":"Build"},{"location":"thehive/installation-and-configuration/installation/hadoop/","text":"Hadoop: installation and configuration # This guide proposes an example of installation and configuration of Apache Hadoop . Installation # Download hadoop distribution from https://hadoop.apache.org/releases.html and uncompress. cd /tmp wget https://downloads.apache.org/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz cd /opt tar zxf /tmp/hadoop-3.1.3.tar.gz ln -s hadoop-3.1.3 hadoop Create a user and update permissions useradd hadoop chown hadoop:root -R /opt/hadoop* Create a datastore and set permissions mkdir /opt/thp/thehive/hdfs chown hadoop:root -R /opt/thp/thehive/hdfs Create ssh keys for hadoop user: su - hadoop ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 0600 ~/.ssh/authorized_keys Update .bashrc file for hadoop user. Add following lines: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export HADOOP_HOME=/opt/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME Note : Apache has a well detailed documentation for more advanced configuration with Hadoop. Configuration the Hadoop Master # Configuration files are located in etc/hadoop ( /opt/hadoop/etc/hadoop ). They must be identical in all nodes. Notes : The configuration described there is for a single node server. This node is the master node, namenode and datanode (refer to Hadoop documentation for more information). After validating this node is running successfully, refer to the related guide to add nodes; Ensure you update the port value to something different than 9000 as it is already reserved for TheHive application service; Edit the file core-site.xml : <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://thehive1:10000 </value> </property> <property> <name> hadoop.tmp.dir </name> <value> /opt/thp/thehive/hdfs/temp </value> </property> <property> <name> dfs.client.block.write.replace-datanode-on-failure.best-effort </name> <value> true </value> </property> </configuration> Edit the file hdfs-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> dfs.replication </name> <value> 2 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/thp/thehive/hdfs/namenode/data </value> </property> <property> <name> dfs.datanode.name.dir </name> <value> /opt/thp/thehive/hdfs/datanode/data </value> </property> <property> <name> dfs.namenode.checkpoint.dir </name> <value> /opt/thp/thehive/hdfs/checkpoint </value> </property> <property> <name> dfs.namenode.http-address </name> <value> 0.0.0.0:9870 </value> </property> <!-- <property> <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name> <value>true</value> </property> --> <property> <name> dfs.client.block.write.replace-datanode-on-failure.policy </name> <value> NEVER </value> </property> </configuration> Format the volume and start services # Format the volume su - hadoop cd /opt/hadoop bin/hdfs namenode -format Run it as a service # Create the /etc/systemd/system/hadoop.service file with the following content: [Unit] Description=Hadoop Documentation=https://hadoop.apache.org/docs/current/index.html Wants=network-online.target After=network-online.target [Service] WorkingDirectory=/opt/hadoop Type=forking User=hadoop Group=hadoop Environment=JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 Environment=HADOOP_HOME=/opt/hadoop Environment=YARN_HOME=/opt/hadoop Environment=HADOOP_COMMON_HOME=/opt/hadoop Environment=HADOOP_HDFS_HOME=/opt/hadoop Environment=HADOOP_MAPRED_HOME=/opt/hadoop Restart=on-failure TimeoutStartSec=2min ExecStart=/opt/hadoop/sbin/start-all.sh ExecStop=/opt/hadoop/sbin/stop-all.sh StandardOutput=null StandardError=null # Specifies the maximum file descriptor number that can be opened by this process LimitNOFILE=65536 # Disable timeout logic and wait until process is stopped TimeoutStopSec=0 # SIGTERM signal is used to stop the Java process KillSignal=SIGTERM # Java process is never killed SendSIGKILL=no [Install] WantedBy=multi-user.target Start the service # service hadoop start You can check cluster status in http://thehive1:9870 Add nodes # To add Hadoop nodes, refer the the related guide .","title":"Hadoop: installation and configuration"},{"location":"thehive/installation-and-configuration/installation/hadoop/#hadoop-installation-and-configuration","text":"This guide proposes an example of installation and configuration of Apache Hadoop .","title":"Hadoop: installation and configuration"},{"location":"thehive/installation-and-configuration/installation/hadoop/#installation","text":"Download hadoop distribution from https://hadoop.apache.org/releases.html and uncompress. cd /tmp wget https://downloads.apache.org/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz cd /opt tar zxf /tmp/hadoop-3.1.3.tar.gz ln -s hadoop-3.1.3 hadoop Create a user and update permissions useradd hadoop chown hadoop:root -R /opt/hadoop* Create a datastore and set permissions mkdir /opt/thp/thehive/hdfs chown hadoop:root -R /opt/thp/thehive/hdfs Create ssh keys for hadoop user: su - hadoop ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 0600 ~/.ssh/authorized_keys Update .bashrc file for hadoop user. Add following lines: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export HADOOP_HOME=/opt/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME Note : Apache has a well detailed documentation for more advanced configuration with Hadoop.","title":"Installation"},{"location":"thehive/installation-and-configuration/installation/hadoop/#configuration-the-hadoop-master","text":"Configuration files are located in etc/hadoop ( /opt/hadoop/etc/hadoop ). They must be identical in all nodes. Notes : The configuration described there is for a single node server. This node is the master node, namenode and datanode (refer to Hadoop documentation for more information). After validating this node is running successfully, refer to the related guide to add nodes; Ensure you update the port value to something different than 9000 as it is already reserved for TheHive application service; Edit the file core-site.xml : <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://thehive1:10000 </value> </property> <property> <name> hadoop.tmp.dir </name> <value> /opt/thp/thehive/hdfs/temp </value> </property> <property> <name> dfs.client.block.write.replace-datanode-on-failure.best-effort </name> <value> true </value> </property> </configuration> Edit the file hdfs-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> dfs.replication </name> <value> 2 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/thp/thehive/hdfs/namenode/data </value> </property> <property> <name> dfs.datanode.name.dir </name> <value> /opt/thp/thehive/hdfs/datanode/data </value> </property> <property> <name> dfs.namenode.checkpoint.dir </name> <value> /opt/thp/thehive/hdfs/checkpoint </value> </property> <property> <name> dfs.namenode.http-address </name> <value> 0.0.0.0:9870 </value> </property> <!-- <property> <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name> <value>true</value> </property> --> <property> <name> dfs.client.block.write.replace-datanode-on-failure.policy </name> <value> NEVER </value> </property> </configuration>","title":"Configuration the Hadoop Master"},{"location":"thehive/installation-and-configuration/installation/hadoop/#format-the-volume-and-start-services","text":"Format the volume su - hadoop cd /opt/hadoop bin/hdfs namenode -format","title":"Format the volume and start services"},{"location":"thehive/installation-and-configuration/installation/hadoop/#run-it-as-a-service","text":"Create the /etc/systemd/system/hadoop.service file with the following content: [Unit] Description=Hadoop Documentation=https://hadoop.apache.org/docs/current/index.html Wants=network-online.target After=network-online.target [Service] WorkingDirectory=/opt/hadoop Type=forking User=hadoop Group=hadoop Environment=JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 Environment=HADOOP_HOME=/opt/hadoop Environment=YARN_HOME=/opt/hadoop Environment=HADOOP_COMMON_HOME=/opt/hadoop Environment=HADOOP_HDFS_HOME=/opt/hadoop Environment=HADOOP_MAPRED_HOME=/opt/hadoop Restart=on-failure TimeoutStartSec=2min ExecStart=/opt/hadoop/sbin/start-all.sh ExecStop=/opt/hadoop/sbin/stop-all.sh StandardOutput=null StandardError=null # Specifies the maximum file descriptor number that can be opened by this process LimitNOFILE=65536 # Disable timeout logic and wait until process is stopped TimeoutStopSec=0 # SIGTERM signal is used to stop the Java process KillSignal=SIGTERM # Java process is never killed SendSIGKILL=no [Install] WantedBy=multi-user.target","title":"Run it as a service"},{"location":"thehive/installation-and-configuration/installation/hadoop/#start-the-service","text":"service hadoop start You can check cluster status in http://thehive1:9870","title":"Start the service"},{"location":"thehive/installation-and-configuration/installation/hadoop/#add-nodes","text":"To add Hadoop nodes, refer the the related guide .","title":"Add nodes"},{"location":"thehive/installation-and-configuration/installation/minio/","text":"","title":"Minio"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/","text":"Step-by-Step guide # This page is a step by step installation and configuration guide to get an TheHive 4 instance up and running. This guide is illustrated with examples for Debian and RPM packages based systems and for installation from binary packages. Java Virtual Machine # Debian apt-get install -y openjdk-8-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" RPM yum install -y java-1.8.0-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" Other The installation requires Java 8, so refer to your system documentation to install it. Note TheHive can be loaded by Java 11, but not the stable version of Cassandra, which still requires Java 8. If you set up a cluster for the database distinct from TheHive servers: Cassandra nodes can be loaded by Java 8 TheHive nodes can be loaded by Java 11 For standalone servers, with TheHive and Cassandra on the same OS, we recommend having only Java 8 installed for both applications. Cassandra database # Apache Cassandra is a scalable and high available database. TheHive supports the latest stable version 3.11.x of Cassandra. Install from repository # Debian Add Apache repository references curl -fsSL https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list Install the package sudo apt update sudo apt install cassandra RPM Add the Apache repository of Cassandra to /etc/yum.repos.d/cassandra.repo [ cassandra ] name = Apache Cassandra baseurl = https://downloads.apache.org/cassandra/redhat/311x/ gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://downloads.apache.org/cassandra/KEYS Install the package yum install -y cassandra Other Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice. By default, data is stored in /var/lib/cassandra . Configuration # Start by changing the cluster_name with thp . Run the command cqlsh : cqlsh localhost 9042 cqlsh > UPDATE system . local SET cluster_name = 'thp' where key = 'local' ; Exit and then run: nodetool flush Configure Cassandra by editing /etc/cassandra/cassandra.yaml file. # content from /etc/cassandra/cassandra.yaml cluster_name: 'thp' listen_address: 'xx.xx.xx.xx' # address for nodes rpc_address: 'xx.xx.xx.xx' # address for clients seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: # Ex: \"<ip1>,<ip2>,<ip3>\" - seeds: 'xx.xx.xx.xx' # self for the first node data_file_directories: - '/var/lib/cassandra/data' commitlog_directory: '/var/lib/cassandra/commitlog' saved_caches_directory: '/var/lib/cassandra/saved_caches' hints_directory: - '/var/lib/cassandra/hints' Then restart the service: Debian service cassandra restart RPM Run the service and ensure it restart after a reboot: systemctl daemon-reload service cassandra start chkconfig cassandra on Warning Cassandra service does not start well with the new systemd version. There is an existing issue and a fix on Apache website: https://issues.apache.org/jira/browse/CASSANDRA-15273 By default Cassandra listens on 7000/tcp (inter-node), 9042/tcp (client). Additional configuration # For additional configuration options, refer to: Cassandra documentation page Datastax documentation page Security # To add security measures in Cassandra , refer the the related administration guide . Add nodes # To add Cassandra nodes, refer the the related administration guide . Indexing engine # Starting from TheHive 4.1.0, a solution to store data indexes is required. These indexes should be unique and the same for all nodes of TheHive cluster. TheHive embed a Lucene engine you can use for standalone server For clusters setups, an instance of Elasticsearch is required Local lucene engine Create a folder dedicated to host indexes for TheHive: mkdir /opt/thp/thehive/index chown thehive:thehive -R /opt/thp/thehive/index Elasticsearch Use an existing Elasticsearch instance or install a new one. This instance should be reachable by all nodes of a cluster. Warning Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored Note Indexes will be created at the first start of TheHive. It can take a certain amount of time, depending the size of the database Like data and files, indexes should be part of the backup policy Indexes can removed and created again File storage # Files uploaded in TheHive (in task logs or in observables ) can be stores in localsystem, in a Hadoop filesystem (recommended) or in the graph database. For standalone production and test servers , we recommends using local filesystem. If you think about building a cluster with TheHive, you have several possible solutions: using Hadoop or S3 services ; see the related guide for more details and an example with MinIO servers. Local Filesystem Warning This option is perfect for standalone servers . If you intend to build a cluster for your instance of TheHive 4 we recommend: using a NFS share, common to all nodes having a look at storage solutions implementing S3 or HDFS. To store files on the local filesystem, start by choosing the dedicated folder: mkdir -p /opt/thp/thehive/files This path will be used in the configuration of TheHive. Later, after having installed TheHive, ensure the user thehive owns the path chosen for storing files: chown -R thehive:thehive /opt/thp/thehive/files S3 with Min.io An example of installing, configuring and use Min.IO is detailed in this documentation . HDFS with Hadoop An example of installing, configuring and use Apache Hadoop is detailed in this documentation . TheHive # This part contains instructions to install TheHive and then configure it. Warning TheHive4 can't be installed on the same server than older versions. We recommend installing it on a new server, especially if a migration is foreseen. Installation # All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . Debian curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - RPM sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY We also release stable and beta version of the applications. Stable versions # Install TheHive 4.x package of the stable version by using the following commands: Debian echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 RPM Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Other Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-latest.zip unzip thehive4-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service Beta versions # To install beta versions of TheHive4, use the following setup: Debian echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 RPM setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Other Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-beta-latest.zip unzip thehive4-beta-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service Warning We recommend using or playing with Beta version for testing purpose only . Configuration # Following configurations are required to start TheHive successfully: Secret key configuration Database configuration File storage configuration Secret key configuration # Debian The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. RPM The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. Other Setup a secret key in the /etc/thehive/secret.conf file by running the following command: cat > /etc/thehive/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ Database # To use Cassandra database, TheHive configuration file ( /etc/thehive/application.conf ) has to be edited and updated with following lines: db { provider : janusgraph janusgraph { storage { backend : cql hostname : [ \"127.0.0.1\" ] # seed node ip addresses #username: \"<cassandra_username>\" # login to connect to database (if configured in Cassandra) #password: \"<cassandra_passowrd\" cql { cluster-name : thp # cluster name keyspace : thehive # name of the keyspace local-datacenter : datacenter1 # name of the datacenter where TheHive runs (relevant only on multi datacenter setup) # replication-factor: 2 # number of replica read-consistency-level : ONE write-consistency-level : ONE } } } } Indexes # Update db.storage configuration part in /etc/thehive/application.conf accordingly to your setup. Lucene If your setup is a standalone server or you are using a common NFS share, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : lucene directory : /opt/thp/thehive/index } } } Elasticsearch If you decided to have access to a centralised index with Elasticsearch, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : elasticsearch hostname : [ \"10.1.2.20\" ] index-name : thehive } } } Filesystem # Local filesystem If you chose to store files on the local filesystem: Ensure permission of the folder chown -R thehive:thehive /opt/thp/thehive/files add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider = localfs localfs.location = /opt/thp/thehive/files } S3 If you chose MinIO and a S3 object storage system to store files in a filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_ADDRESS>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" } } HDFS If you chose Apache Hadoop and a HDFS filesystem to store files in a distrubuted filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : hdfs hdfs { root : \"hdfs://thehive1:10000\" # namenode server location : \"/thehive\" username : thehive } } Run # Save configuration file and run the service: service thehive start Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . The default admin user is admin@thehive.local with password secret . It is recommended to change the default password. Advanced configuration # For additional configuration options, please refer to the Configuration Guides .","title":"Step by step guide"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#step-by-step-guide","text":"This page is a step by step installation and configuration guide to get an TheHive 4 instance up and running. This guide is illustrated with examples for Debian and RPM packages based systems and for installation from binary packages.","title":"Step-by-Step guide"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#java-virtual-machine","text":"Debian apt-get install -y openjdk-8-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" RPM yum install -y java-1.8.0-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" Other The installation requires Java 8, so refer to your system documentation to install it. Note TheHive can be loaded by Java 11, but not the stable version of Cassandra, which still requires Java 8. If you set up a cluster for the database distinct from TheHive servers: Cassandra nodes can be loaded by Java 8 TheHive nodes can be loaded by Java 11 For standalone servers, with TheHive and Cassandra on the same OS, we recommend having only Java 8 installed for both applications.","title":"Java Virtual Machine"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#cassandra-database","text":"Apache Cassandra is a scalable and high available database. TheHive supports the latest stable version 3.11.x of Cassandra.","title":"Cassandra database"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#install-from-repository","text":"Debian Add Apache repository references curl -fsSL https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list Install the package sudo apt update sudo apt install cassandra RPM Add the Apache repository of Cassandra to /etc/yum.repos.d/cassandra.repo [ cassandra ] name = Apache Cassandra baseurl = https://downloads.apache.org/cassandra/redhat/311x/ gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://downloads.apache.org/cassandra/KEYS Install the package yum install -y cassandra Other Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice. By default, data is stored in /var/lib/cassandra .","title":"Install from repository"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#configuration","text":"Start by changing the cluster_name with thp . Run the command cqlsh : cqlsh localhost 9042 cqlsh > UPDATE system . local SET cluster_name = 'thp' where key = 'local' ; Exit and then run: nodetool flush Configure Cassandra by editing /etc/cassandra/cassandra.yaml file. # content from /etc/cassandra/cassandra.yaml cluster_name: 'thp' listen_address: 'xx.xx.xx.xx' # address for nodes rpc_address: 'xx.xx.xx.xx' # address for clients seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: # Ex: \"<ip1>,<ip2>,<ip3>\" - seeds: 'xx.xx.xx.xx' # self for the first node data_file_directories: - '/var/lib/cassandra/data' commitlog_directory: '/var/lib/cassandra/commitlog' saved_caches_directory: '/var/lib/cassandra/saved_caches' hints_directory: - '/var/lib/cassandra/hints' Then restart the service: Debian service cassandra restart RPM Run the service and ensure it restart after a reboot: systemctl daemon-reload service cassandra start chkconfig cassandra on Warning Cassandra service does not start well with the new systemd version. There is an existing issue and a fix on Apache website: https://issues.apache.org/jira/browse/CASSANDRA-15273 By default Cassandra listens on 7000/tcp (inter-node), 9042/tcp (client).","title":"Configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#additional-configuration","text":"For additional configuration options, refer to: Cassandra documentation page Datastax documentation page","title":"Additional configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#security","text":"To add security measures in Cassandra , refer the the related administration guide .","title":"Security"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#add-nodes","text":"To add Cassandra nodes, refer the the related administration guide .","title":"Add nodes"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#indexing-engine","text":"Starting from TheHive 4.1.0, a solution to store data indexes is required. These indexes should be unique and the same for all nodes of TheHive cluster. TheHive embed a Lucene engine you can use for standalone server For clusters setups, an instance of Elasticsearch is required Local lucene engine Create a folder dedicated to host indexes for TheHive: mkdir /opt/thp/thehive/index chown thehive:thehive -R /opt/thp/thehive/index Elasticsearch Use an existing Elasticsearch instance or install a new one. This instance should be reachable by all nodes of a cluster. Warning Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored Note Indexes will be created at the first start of TheHive. It can take a certain amount of time, depending the size of the database Like data and files, indexes should be part of the backup policy Indexes can removed and created again","title":"Indexing engine"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#file-storage","text":"Files uploaded in TheHive (in task logs or in observables ) can be stores in localsystem, in a Hadoop filesystem (recommended) or in the graph database. For standalone production and test servers , we recommends using local filesystem. If you think about building a cluster with TheHive, you have several possible solutions: using Hadoop or S3 services ; see the related guide for more details and an example with MinIO servers. Local Filesystem Warning This option is perfect for standalone servers . If you intend to build a cluster for your instance of TheHive 4 we recommend: using a NFS share, common to all nodes having a look at storage solutions implementing S3 or HDFS. To store files on the local filesystem, start by choosing the dedicated folder: mkdir -p /opt/thp/thehive/files This path will be used in the configuration of TheHive. Later, after having installed TheHive, ensure the user thehive owns the path chosen for storing files: chown -R thehive:thehive /opt/thp/thehive/files S3 with Min.io An example of installing, configuring and use Min.IO is detailed in this documentation . HDFS with Hadoop An example of installing, configuring and use Apache Hadoop is detailed in this documentation .","title":"File storage"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#thehive","text":"This part contains instructions to install TheHive and then configure it. Warning TheHive4 can't be installed on the same server than older versions. We recommend installing it on a new server, especially if a migration is foreseen.","title":"TheHive"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#installation","text":"All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . Debian curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - RPM sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY We also release stable and beta version of the applications.","title":"Installation"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#stable-versions","text":"Install TheHive 4.x package of the stable version by using the following commands: Debian echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 RPM Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Other Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-latest.zip unzip thehive4-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service","title":"Stable versions"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#beta-versions","text":"To install beta versions of TheHive4, use the following setup: Debian echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 RPM setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Other Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-beta-latest.zip unzip thehive4-beta-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service Warning We recommend using or playing with Beta version for testing purpose only .","title":"Beta versions"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#configuration_1","text":"Following configurations are required to start TheHive successfully: Secret key configuration Database configuration File storage configuration","title":"Configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#secret-key-configuration","text":"Debian The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. RPM The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. Other Setup a secret key in the /etc/thehive/secret.conf file by running the following command: cat > /etc/thehive/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_","title":"Secret key configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#database","text":"To use Cassandra database, TheHive configuration file ( /etc/thehive/application.conf ) has to be edited and updated with following lines: db { provider : janusgraph janusgraph { storage { backend : cql hostname : [ \"127.0.0.1\" ] # seed node ip addresses #username: \"<cassandra_username>\" # login to connect to database (if configured in Cassandra) #password: \"<cassandra_passowrd\" cql { cluster-name : thp # cluster name keyspace : thehive # name of the keyspace local-datacenter : datacenter1 # name of the datacenter where TheHive runs (relevant only on multi datacenter setup) # replication-factor: 2 # number of replica read-consistency-level : ONE write-consistency-level : ONE } } } }","title":"Database"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#indexes","text":"Update db.storage configuration part in /etc/thehive/application.conf accordingly to your setup. Lucene If your setup is a standalone server or you are using a common NFS share, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : lucene directory : /opt/thp/thehive/index } } } Elasticsearch If you decided to have access to a centralised index with Elasticsearch, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : elasticsearch hostname : [ \"10.1.2.20\" ] index-name : thehive } } }","title":"Indexes"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#filesystem","text":"Local filesystem If you chose to store files on the local filesystem: Ensure permission of the folder chown -R thehive:thehive /opt/thp/thehive/files add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider = localfs localfs.location = /opt/thp/thehive/files } S3 If you chose MinIO and a S3 object storage system to store files in a filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_ADDRESS>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" } } HDFS If you chose Apache Hadoop and a HDFS filesystem to store files in a distrubuted filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : hdfs hdfs { root : \"hdfs://thehive1:10000\" # namenode server location : \"/thehive\" username : thehive } }","title":"Filesystem"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#run","text":"Save configuration file and run the service: service thehive start Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . The default admin user is admin@thehive.local with password secret . It is recommended to change the default password.","title":"Run"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#advanced-configuration","text":"For additional configuration options, please refer to the Configuration Guides .","title":"Advanced configuration"},{"location":"thehive/legacy/thehive3/","text":"TheHive is a scalable 4-in-1 open source and free security incident response platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. Thanks to Cortex , our powerful free and open source analysis engine, you can analyze (and triage) observables at scale using more than 100 analyzers. Additionally and starting from TheHive 3.1.0, you can actively respond to threats and interact with your constituency and other parties thanks to Cortex responders. Last but not least, TheHive is highly integrated with MISP , the de facto standard of threat sharing, as it can pull events from several MISP instances and export investigation cases back to one or several ones. It also has additional features such as MISP extended events and health checking. This is TheHive's documentation repository. If you are looking for its source code, please visit https://github.com/TheHive-Project/TheHive/ . Hardware Pre-requisites # TheHive uses ElasticSearch to store data. Both software use a Java VM. We recommend using a virtual machine with 8vCPU, 8 GB of RAM and 60 GB of disk. You can also use a physical machine with similar specifications. Guides # Installation Guide Administration Guide Configuration Guide Webhooks Cluster Configuration Updating Backup & Restore Migration Guide API Documentation (incomplete) Miscellaneous Information # Feature Set (In Progress) Changelog FAQ Training Material Additional Resources Single Sign-On on TheHive with X.509 Certificates (Experimental Feature) License # TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run. Updates # Information, news and updates are regularly posted on TheHive Project Twitter account and on the blog . Contributing # We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing. Support # Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Important Note : If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository . Community Discussions # We have set up a Google forum at https://groups.google.com/a/thehive-project.org/d/forum/users . To request access, you need a Google account. You may create one using a Gmail address or without it . Website # https://thehive-project.org/","title":"Index"},{"location":"thehive/legacy/thehive3/#hardware-pre-requisites","text":"TheHive uses ElasticSearch to store data. Both software use a Java VM. We recommend using a virtual machine with 8vCPU, 8 GB of RAM and 60 GB of disk. You can also use a physical machine with similar specifications.","title":"Hardware Pre-requisites"},{"location":"thehive/legacy/thehive3/#guides","text":"Installation Guide Administration Guide Configuration Guide Webhooks Cluster Configuration Updating Backup & Restore Migration Guide API Documentation (incomplete)","title":"Guides"},{"location":"thehive/legacy/thehive3/#miscellaneous-information","text":"Feature Set (In Progress) Changelog FAQ Training Material Additional Resources Single Sign-On on TheHive with X.509 Certificates (Experimental Feature)","title":"Miscellaneous Information"},{"location":"thehive/legacy/thehive3/#license","text":"TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run.","title":"License"},{"location":"thehive/legacy/thehive3/#updates","text":"Information, news and updates are regularly posted on TheHive Project Twitter account and on the blog .","title":"Updates"},{"location":"thehive/legacy/thehive3/#contributing","text":"We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing.","title":"Contributing"},{"location":"thehive/legacy/thehive3/#support","text":"Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Important Note : If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository .","title":"Support"},{"location":"thehive/legacy/thehive3/#community-discussions","text":"We have set up a Google forum at https://groups.google.com/a/thehive-project.org/d/forum/users . To request access, you need a Google account. You may create one using a Gmail address or without it .","title":"Community Discussions"},{"location":"thehive/legacy/thehive3/#website","text":"https://thehive-project.org/","title":"Website"},{"location":"thehive/legacy/thehive3/feature-set/","text":"Feature set # This document lists the features provided by TheHive in either the UI or APIs and Webhooks. TheHive comes with the native support of integrating: one or more Cortex instances one or more MISP instances Authentication # TheHive supports multiple authentication methods: Local authentication using a local user collection AD authentication LDAP authentication SSO authentication X.509 certificates authentication Case Management # List and filter cases Create new cases from scratch or using case templates Add custom fields to cases Add metrics to cases Find linked cases to a given case based shared observables Add tasks and task groups to cases Assign tasks to a given user Add logs to tasks, including attachment to task logs Add observables to a case Execute Cortex responders against cases tasks task logs Delete cases by administrators only Alert Management # Alerts are a sort of incident not yet qualified as a Case. The Alerts sections allows: Listing and searching for alerts Marking alerts as read Ignoring alert updates Previewing alert details Display alert details and editable custom fields Display alerts observables Display similar cases Importing an alert as an emtpty case or using a case template Merging an alert into an existing case MISP Integration # MISP is natively integrated to TheHive allowing: The declaration of one or more MISP instances Each instance can be used to Import and/or Export events from MISP or cases to MISP Imported MISP events are made available as Alerts Imporing is configurable using filters (configuration files) Feeders # Feeders are external tools designed to send alerts to TheHive leveraging the REST APIs Thehive offers Feeders can be written and any programming language as long as it is compatible with TheHive APIs Feeders can be written in Python and use TheHive4Py Search capabilities # The search section provided by TheHive allows searching for the following objects using dynamic forms: cases tasks observables logs alerts Dashboarding # The dashboards section allows: creating private dashboards per user creating shared dashboads visible by all users adding widgets to dashboards using a drag & drop capabilities creating widgets that target cases, tasks, observables, alerts, jobs configuring widgets in a granular manner Administration # Case templates # Create case templates Add tasks to templates Add metrics to templates Add custom fields to templates Define default values for custom fields, metrics and tasks Export case template definitions Import case template definitions Metrics # List and Create metrics Custom fields # Create custom fields Update custom fields Users # List users Create/Edit users Set a user password Set a user API key Revoke a user's API key Lock a user Analyzer report templates # Report templates are used to display the raw reports from Cortex in a text format. This section allows: Importing short and long reports Customize short and long reports for each analyzer Cortex integration # TheHive uses Cortex to have access to analyzers and responsders Analyzers can be launched against observables to get more details about a given observable Responders can be launched against case, tasks, observables, logs, and alerts to execute an action One or more Cortex instances can be connected to TheHive Database migration # TheHive provides a mechanism to upgrade the Elasticsearch database by copying the index and making transformations on it.","title":"Feature set"},{"location":"thehive/legacy/thehive3/feature-set/#feature-set","text":"This document lists the features provided by TheHive in either the UI or APIs and Webhooks. TheHive comes with the native support of integrating: one or more Cortex instances one or more MISP instances","title":"Feature set"},{"location":"thehive/legacy/thehive3/feature-set/#authentication","text":"TheHive supports multiple authentication methods: Local authentication using a local user collection AD authentication LDAP authentication SSO authentication X.509 certificates authentication","title":"Authentication"},{"location":"thehive/legacy/thehive3/feature-set/#case-management","text":"List and filter cases Create new cases from scratch or using case templates Add custom fields to cases Add metrics to cases Find linked cases to a given case based shared observables Add tasks and task groups to cases Assign tasks to a given user Add logs to tasks, including attachment to task logs Add observables to a case Execute Cortex responders against cases tasks task logs Delete cases by administrators only","title":"Case Management"},{"location":"thehive/legacy/thehive3/feature-set/#alert-management","text":"Alerts are a sort of incident not yet qualified as a Case. The Alerts sections allows: Listing and searching for alerts Marking alerts as read Ignoring alert updates Previewing alert details Display alert details and editable custom fields Display alerts observables Display similar cases Importing an alert as an emtpty case or using a case template Merging an alert into an existing case","title":"Alert Management"},{"location":"thehive/legacy/thehive3/feature-set/#misp-integration","text":"MISP is natively integrated to TheHive allowing: The declaration of one or more MISP instances Each instance can be used to Import and/or Export events from MISP or cases to MISP Imported MISP events are made available as Alerts Imporing is configurable using filters (configuration files)","title":"MISP Integration"},{"location":"thehive/legacy/thehive3/feature-set/#feeders","text":"Feeders are external tools designed to send alerts to TheHive leveraging the REST APIs Thehive offers Feeders can be written and any programming language as long as it is compatible with TheHive APIs Feeders can be written in Python and use TheHive4Py","title":"Feeders"},{"location":"thehive/legacy/thehive3/feature-set/#search-capabilities","text":"The search section provided by TheHive allows searching for the following objects using dynamic forms: cases tasks observables logs alerts","title":"Search capabilities"},{"location":"thehive/legacy/thehive3/feature-set/#dashboarding","text":"The dashboards section allows: creating private dashboards per user creating shared dashboads visible by all users adding widgets to dashboards using a drag & drop capabilities creating widgets that target cases, tasks, observables, alerts, jobs configuring widgets in a granular manner","title":"Dashboarding"},{"location":"thehive/legacy/thehive3/feature-set/#administration","text":"","title":"Administration"},{"location":"thehive/legacy/thehive3/feature-set/#case-templates","text":"Create case templates Add tasks to templates Add metrics to templates Add custom fields to templates Define default values for custom fields, metrics and tasks Export case template definitions Import case template definitions","title":"Case templates"},{"location":"thehive/legacy/thehive3/feature-set/#metrics","text":"List and Create metrics","title":"Metrics"},{"location":"thehive/legacy/thehive3/feature-set/#custom-fields","text":"Create custom fields Update custom fields","title":"Custom fields"},{"location":"thehive/legacy/thehive3/feature-set/#users","text":"List users Create/Edit users Set a user password Set a user API key Revoke a user's API key Lock a user","title":"Users"},{"location":"thehive/legacy/thehive3/feature-set/#analyzer-report-templates","text":"Report templates are used to display the raw reports from Cortex in a text format. This section allows: Importing short and long reports Customize short and long reports for each analyzer","title":"Analyzer report templates"},{"location":"thehive/legacy/thehive3/feature-set/#cortex-integration","text":"TheHive uses Cortex to have access to analyzers and responsders Analyzers can be launched against observables to get more details about a given observable Responders can be launched against case, tasks, observables, logs, and alerts to execute an action One or more Cortex instances can be connected to TheHive","title":"Cortex integration"},{"location":"thehive/legacy/thehive3/feature-set/#database-migration","text":"TheHive provides a mechanism to upgrade the Elasticsearch database by copying the index and making transformations on it.","title":"Database migration"},{"location":"thehive/legacy/thehive3/migration-guide/","text":"Migration guide # From 3.4.x to 3.5.0 # Taking into account the EoL of version 6.x. of Elasticsearch, TheHive 3.5.0 is the first version to support Elasticsearch 7.x. This version introduce breaking changes. This time, we had no choice, we were not able to make TheHive support smoothly the ES upgrade. TheHive 3.5.0 supports Elasticsearch 7.x ONLY . This first steps before starting the upgrade process are: Identify the version of Elasticsearch which created your index Stop TheHive service Stop Elasticsearch service How to identify the version of Elasticsearch which created your database index ? # The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Run the following command : curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created_string' if the output is similar to \"5.x\" then your database index has been created with Elasticsearch 5.x reindexing is required, you should follow a dedicated process to upgrade . If it is \"6.x\" then your database has been created with Elasticsearch 6. Your database was created with Elasticsearch 5.x or earlier # This is where things might be complicated. This upgrade progress requires handling the database index by updating parameters, and reindex before updating Elasticsearch, and updating TheHive. Read carefully the dedicated documentation . It should help you run this specific actions on your Elasticsearch database, and also install or update application whether you are using DEB, RPM or binary packages, and even docker images. Your database was created with Elasticsearch 6.x # If you started using TheHive with Elasticsearch 6.x, then you just need to update the configuration of Elasticsearch to reflect this one: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Following parameters * are not accepted anymore by Elasticsearch 7: thread_pool.index.queue_size thread_pool.bulk.queue_size With TheHive service stopped, ensure the new version of Elasticsearch starts. If everything is ok, then TheHive 3.5.0 can be installed. To run this operation successfully, you need to update your repository configuration if you are using DEB and RPM packages, or specify the right version to install if using docker. Read carefully the installation guide . From 3.3.x to 3.4.0 # Starting from version 3.4.0-RC1, TheHive supports Elasticsearch 6 and will continue to work with Elasticsearch 5.x. TheHive 3.4.0-RC1 and later versions communicate with Elasticsearch using its HTTP service (9200/tcp by default) instead of its legacy binary protocol (9300/tcp by default). If you have a firewall between TheHive and Elasticsearch, you probably need to update its rules to change to the new port number. The configuration file ( application.conf ) needs some modifications to reflect the protocol change: The setting search.host is replaced by search.uri The general format of the URI is: http(s)://host:port,host:port(/prefix)?querystring . Multiple host:port combinations can be specified, separated by commas. Options can be specified using a standard URI query string syntax, eg. cluster.name=hive . The search.cluster setting is no longer used. Authentication can be configured with the search.user and search.password settings. When SSL/TLS is enabled, you can set a truststore and a keystore. The truststore contains the certificate authorities used to validate remote certificates. The keystore contains the certificate and the private key used to connect to the Elasticsearch cluster. The configuration is: search { keyStore { path: \"/path/to/keystore/file\" type: \"JKS\" # or PKCS12 password: \"secret.password.of.keystore\" } trustStore { path: \"/path/to/truststore/file\" type: \"JKS\" password: \"secret.password.of.truststore\" } } The Elasticsearch client also accepts the following settings: - circularRedirectsAllowed ( true / false ) - connectionRequestTimeout (number of seconds) - connectTimeout - contentCompressionEnabled.foreach(requestConfigBuilder.setContentCompressionEnabled) - search.cookieSpec (??) - expectContinueEnabled ( true / false ) - maxRedirects (number) - proxy -- not yet supported - proxyPreferredAuthSchemes -- not yet supported - redirectsEnabled ( true / false ) - relativeRedirectsAllowed ( true / false ) - socketTimeout (number of seconds) - targetPreferredAuthSchemes (??) The configuration items keepalive , pageSize , nbshards and nbreplicas are still valid. For practical details, you can have a look here for an example of migration of TheHive and Elasticsearch. From 3.0.x to 3.0.4 # TheHive 3.0.4 (Cerana 0.4) comes with new MISP settings to filter events that will be imported as alerts. Please refer to MISP event filters configuration section. The maximum number of custom fields and metrics in a case is 50 by default. If you try to put more, ElasticSearch will raise an error. You can now increase the limit by adding in your application.conf: index { settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } } The data schema has been changed in Cerana to support some dashboard features. At the first connection, TheHive will ask you to migrate the data. A new index, called the_hive_13 by default, will be created then. See Updating . From 2.13.x to 3.0.0 # The schema of data has been changed in Cerana to integrate dashboard. At the first request, TheHive will ask you to migrate the data. A new index, called the_hive_12 by default, will be created then. See Updating . From 2.13.0 or 2.13.1 to 2.13.2 # At the first connection to TheHive 2.13.2, a migration of the database will be asked. This will create a new ElasticSearch index ( the_hive_11 by default). See Updating . From 2.12.x to 2.13.x # Configuration updates # play.crypto.secret is deprecated, use play.http.secret.key instead. auth.type is deprecated, use auth.provider instead. Basic authentication is disabled by default. We strongly recommand to update the clients that rely on the API to interact with TheHive to use the new API key authentication method . This feature has been added in this release. If you need to enable basic authentication, use auth.method.basic=true in application.conf Note that the TheHive4Py 1.3.0 Python library also adds API key authentication support. Alert role # A new role \"alert\" has been added. Only users with this role can create an alert. If you have tool that uses TheHive API to create alerts, you must give the ability to do it in user administration. ElasticSearch # TheHive 2.13 uses ElasticSearch 5.x. Our tests have been done on ElasticSearch 5.5. So we recommend to use this specific version, even if TheHive should work perfectly with ElasticSearch 5.6 that doesn't introduce breaking changes. Data structure migration # Before upgrading ElasticSearch, backup all your indices . Then remove all indices except the last index of TheHive (most probably the_hive_10). You can list all indices with the following command: curl http://127.0.0.1:9200/_cat/indices ElasticSearch has changed the structure of its data directory (please refer to Path to data on disk ). The node name in the path where data are stored (DATA_DIR) must be removed. Stop ElasticSearch and execute the following lines to change the directory structure: echo -n 'Enter the path of ElasticSearch data: ' read DATA_DIR echo -n 'Enter the name of your cluster [hive]: ' read CLUSTER_NAME mv ${DATA_DIR}/${CLUSTER_NAME:=hive}/* ${DATA_DIR} rmdir ${DATA_DIR}/${CLUSTER_NAME} System requirements # ElasticSearch 5.x requires at least 262144 memory map areas (vm.max_map_count). Run sysctl -w vm.max_map_count=262144. To make this setting persistent after a server restart, add vm.max_map_count = 262144 in /etc/sysctl.conf (or to /etc/sysctl.d/80-elasticsearch.conf ) Configuration # The configuration of ElasticSearch should contain the following settings: http.host: 127.0.0.1 transport.host: 127.0.0.1 cluster.name: hive script.inline: true thread_pool.index.queue_size: 100000 thread_pool.search.queue_size: 100000 thread_pool.bulk.queue_size: 100000 Adapt http.host and transport.host to your environment. Docker # The default ElasticSearch image has been deprecated. It is recommended to use the docker image from Elastic.co . The new image doesn't use the same user ID so you need to change the owner of the data files. You can simply run chown -R 1000.1000 $DATA_DIR (DATA_DIR is the folder which contains ElasticSearch data). Then you can use the following script: docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --publish 127.0.0.1:9300:9300 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"transport.host=0.0.0.0\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:5.5.2 Note : TheHive doesn't support X-Pack. Don't enable it . Warnings You Can Safely Ignore with ES 5.5 # ElasticSearch 5.5 will output the following warnings: - unexpected docvalues type NONE for field '_parent' (expected one of [SORTED, SORTED_SET]). Re-index with correct docvalues type. You can safely ignore this message. For more information see issues #25849 and #26341 - License [will expire] on [***]. If you have a new license, please update it. Ignore this warning as TheHive doesn't use Elasticsearch's commercial features. Note : ElasticSearch 5.6 fixes those warnings. From 2.11.x to 2.12.x # Database migration # At the first connection to TheHive 2.12, a migration of the database will be asked. This will create a new ElasticSearch index (the_hive_10). See Updating . From 2.10.x to 2.11.x # Database migration # At the first connection to TheHive 2.11, a migration of the database will be asked. This will create a new ElastciSearch index (the_hive_9). See Updating . MISP to alert # MISP synchronization is now done using alerting framework. MISP events are seen like other alert. You can use TheHive4py to create your own alert. Configuration changes # MISP certificate authority deprecated # Specifying certificate authority in MISP configuration using \"cert\" key is now deprecated. You must replace it by - before: misp { [...] cert = \"/path/to/truststore.jks\" } - after: misp { [...] ws.ssl.trustManager.stores = [ { type: \"JKS\" path: \"/path/to/truststore.jks\" } ] } ws key can be placed in MISP server section or in global MISP section. In the latter, ws configuration will be applied on all MISP instances. Cortex and MISP HTTP client options # HTTP client used by Cortex and MISP is more configurable. Proxy can be configured, with or without authentication. Refer to configuration for all possible options. Packages # New RPM and DEB packages # RPM and DEB packages are now available. This makes the installation easier than using a binary package (ZIP). See the Installation Guide for reference. Docker # All-in-One docker (containing TheHive and Cortex) is not provided any longer. New TheHive docker image doesn't contain ElasticSearch. We recommend to use docker-compose to link TheHive, ElasticSearch and Cortex dockers. For more information, see the Installation Guide for reference. TheHive configuration is located in /etc/thehive/application.conf for all packages. If you use docker package you must update its location (previously was /opt/docker/conf/application.conf ).","title":"Migration guide"},{"location":"thehive/legacy/thehive3/migration-guide/#migration-guide","text":"","title":"Migration guide"},{"location":"thehive/legacy/thehive3/migration-guide/#from-34x-to-350","text":"Taking into account the EoL of version 6.x. of Elasticsearch, TheHive 3.5.0 is the first version to support Elasticsearch 7.x. This version introduce breaking changes. This time, we had no choice, we were not able to make TheHive support smoothly the ES upgrade. TheHive 3.5.0 supports Elasticsearch 7.x ONLY . This first steps before starting the upgrade process are: Identify the version of Elasticsearch which created your index Stop TheHive service Stop Elasticsearch service","title":"From 3.4.x to 3.5.0"},{"location":"thehive/legacy/thehive3/migration-guide/#how-to-identify-the-version-of-elasticsearch-which-created-your-database-index","text":"The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Run the following command : curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created_string' if the output is similar to \"5.x\" then your database index has been created with Elasticsearch 5.x reindexing is required, you should follow a dedicated process to upgrade . If it is \"6.x\" then your database has been created with Elasticsearch 6.","title":"How to identify the version of Elasticsearch which created your database index ?"},{"location":"thehive/legacy/thehive3/migration-guide/#your-database-was-created-with-elasticsearch-5x-or-earlier","text":"This is where things might be complicated. This upgrade progress requires handling the database index by updating parameters, and reindex before updating Elasticsearch, and updating TheHive. Read carefully the dedicated documentation . It should help you run this specific actions on your Elasticsearch database, and also install or update application whether you are using DEB, RPM or binary packages, and even docker images.","title":"Your database was created with Elasticsearch 5.x or earlier"},{"location":"thehive/legacy/thehive3/migration-guide/#your-database-was-created-with-elasticsearch-6x","text":"If you started using TheHive with Elasticsearch 6.x, then you just need to update the configuration of Elasticsearch to reflect this one: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Following parameters * are not accepted anymore by Elasticsearch 7: thread_pool.index.queue_size thread_pool.bulk.queue_size With TheHive service stopped, ensure the new version of Elasticsearch starts. If everything is ok, then TheHive 3.5.0 can be installed. To run this operation successfully, you need to update your repository configuration if you are using DEB and RPM packages, or specify the right version to install if using docker. Read carefully the installation guide .","title":"Your database was created with Elasticsearch 6.x"},{"location":"thehive/legacy/thehive3/migration-guide/#from-33x-to-340","text":"Starting from version 3.4.0-RC1, TheHive supports Elasticsearch 6 and will continue to work with Elasticsearch 5.x. TheHive 3.4.0-RC1 and later versions communicate with Elasticsearch using its HTTP service (9200/tcp by default) instead of its legacy binary protocol (9300/tcp by default). If you have a firewall between TheHive and Elasticsearch, you probably need to update its rules to change to the new port number. The configuration file ( application.conf ) needs some modifications to reflect the protocol change: The setting search.host is replaced by search.uri The general format of the URI is: http(s)://host:port,host:port(/prefix)?querystring . Multiple host:port combinations can be specified, separated by commas. Options can be specified using a standard URI query string syntax, eg. cluster.name=hive . The search.cluster setting is no longer used. Authentication can be configured with the search.user and search.password settings. When SSL/TLS is enabled, you can set a truststore and a keystore. The truststore contains the certificate authorities used to validate remote certificates. The keystore contains the certificate and the private key used to connect to the Elasticsearch cluster. The configuration is: search { keyStore { path: \"/path/to/keystore/file\" type: \"JKS\" # or PKCS12 password: \"secret.password.of.keystore\" } trustStore { path: \"/path/to/truststore/file\" type: \"JKS\" password: \"secret.password.of.truststore\" } } The Elasticsearch client also accepts the following settings: - circularRedirectsAllowed ( true / false ) - connectionRequestTimeout (number of seconds) - connectTimeout - contentCompressionEnabled.foreach(requestConfigBuilder.setContentCompressionEnabled) - search.cookieSpec (??) - expectContinueEnabled ( true / false ) - maxRedirects (number) - proxy -- not yet supported - proxyPreferredAuthSchemes -- not yet supported - redirectsEnabled ( true / false ) - relativeRedirectsAllowed ( true / false ) - socketTimeout (number of seconds) - targetPreferredAuthSchemes (??) The configuration items keepalive , pageSize , nbshards and nbreplicas are still valid. For practical details, you can have a look here for an example of migration of TheHive and Elasticsearch.","title":"From 3.3.x to 3.4.0"},{"location":"thehive/legacy/thehive3/migration-guide/#from-30x-to-304","text":"TheHive 3.0.4 (Cerana 0.4) comes with new MISP settings to filter events that will be imported as alerts. Please refer to MISP event filters configuration section. The maximum number of custom fields and metrics in a case is 50 by default. If you try to put more, ElasticSearch will raise an error. You can now increase the limit by adding in your application.conf: index { settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } } The data schema has been changed in Cerana to support some dashboard features. At the first connection, TheHive will ask you to migrate the data. A new index, called the_hive_13 by default, will be created then. See Updating .","title":"From 3.0.x to 3.0.4"},{"location":"thehive/legacy/thehive3/migration-guide/#from-213x-to-300","text":"The schema of data has been changed in Cerana to integrate dashboard. At the first request, TheHive will ask you to migrate the data. A new index, called the_hive_12 by default, will be created then. See Updating .","title":"From 2.13.x to 3.0.0"},{"location":"thehive/legacy/thehive3/migration-guide/#from-2130-or-2131-to-2132","text":"At the first connection to TheHive 2.13.2, a migration of the database will be asked. This will create a new ElasticSearch index ( the_hive_11 by default). See Updating .","title":"From 2.13.0 or 2.13.1 to 2.13.2"},{"location":"thehive/legacy/thehive3/migration-guide/#from-212x-to-213x","text":"","title":"From 2.12.x to 2.13.x"},{"location":"thehive/legacy/thehive3/migration-guide/#configuration-updates","text":"play.crypto.secret is deprecated, use play.http.secret.key instead. auth.type is deprecated, use auth.provider instead. Basic authentication is disabled by default. We strongly recommand to update the clients that rely on the API to interact with TheHive to use the new API key authentication method . This feature has been added in this release. If you need to enable basic authentication, use auth.method.basic=true in application.conf Note that the TheHive4Py 1.3.0 Python library also adds API key authentication support.","title":"Configuration updates"},{"location":"thehive/legacy/thehive3/migration-guide/#alert-role","text":"A new role \"alert\" has been added. Only users with this role can create an alert. If you have tool that uses TheHive API to create alerts, you must give the ability to do it in user administration.","title":"Alert role"},{"location":"thehive/legacy/thehive3/migration-guide/#elasticsearch","text":"TheHive 2.13 uses ElasticSearch 5.x. Our tests have been done on ElasticSearch 5.5. So we recommend to use this specific version, even if TheHive should work perfectly with ElasticSearch 5.6 that doesn't introduce breaking changes.","title":"ElasticSearch"},{"location":"thehive/legacy/thehive3/migration-guide/#data-structure-migration","text":"Before upgrading ElasticSearch, backup all your indices . Then remove all indices except the last index of TheHive (most probably the_hive_10). You can list all indices with the following command: curl http://127.0.0.1:9200/_cat/indices ElasticSearch has changed the structure of its data directory (please refer to Path to data on disk ). The node name in the path where data are stored (DATA_DIR) must be removed. Stop ElasticSearch and execute the following lines to change the directory structure: echo -n 'Enter the path of ElasticSearch data: ' read DATA_DIR echo -n 'Enter the name of your cluster [hive]: ' read CLUSTER_NAME mv ${DATA_DIR}/${CLUSTER_NAME:=hive}/* ${DATA_DIR} rmdir ${DATA_DIR}/${CLUSTER_NAME}","title":"Data structure migration"},{"location":"thehive/legacy/thehive3/migration-guide/#system-requirements","text":"ElasticSearch 5.x requires at least 262144 memory map areas (vm.max_map_count). Run sysctl -w vm.max_map_count=262144. To make this setting persistent after a server restart, add vm.max_map_count = 262144 in /etc/sysctl.conf (or to /etc/sysctl.d/80-elasticsearch.conf )","title":"System requirements"},{"location":"thehive/legacy/thehive3/migration-guide/#configuration","text":"The configuration of ElasticSearch should contain the following settings: http.host: 127.0.0.1 transport.host: 127.0.0.1 cluster.name: hive script.inline: true thread_pool.index.queue_size: 100000 thread_pool.search.queue_size: 100000 thread_pool.bulk.queue_size: 100000 Adapt http.host and transport.host to your environment.","title":"Configuration"},{"location":"thehive/legacy/thehive3/migration-guide/#docker","text":"The default ElasticSearch image has been deprecated. It is recommended to use the docker image from Elastic.co . The new image doesn't use the same user ID so you need to change the owner of the data files. You can simply run chown -R 1000.1000 $DATA_DIR (DATA_DIR is the folder which contains ElasticSearch data). Then you can use the following script: docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --publish 127.0.0.1:9300:9300 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"transport.host=0.0.0.0\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:5.5.2 Note : TheHive doesn't support X-Pack. Don't enable it .","title":"Docker"},{"location":"thehive/legacy/thehive3/migration-guide/#warnings-you-can-safely-ignore-with-es-55","text":"ElasticSearch 5.5 will output the following warnings: - unexpected docvalues type NONE for field '_parent' (expected one of [SORTED, SORTED_SET]). Re-index with correct docvalues type. You can safely ignore this message. For more information see issues #25849 and #26341 - License [will expire] on [***]. If you have a new license, please update it. Ignore this warning as TheHive doesn't use Elasticsearch's commercial features. Note : ElasticSearch 5.6 fixes those warnings.","title":"Warnings You Can Safely Ignore with ES 5.5"},{"location":"thehive/legacy/thehive3/migration-guide/#from-211x-to-212x","text":"","title":"From 2.11.x to 2.12.x"},{"location":"thehive/legacy/thehive3/migration-guide/#database-migration","text":"At the first connection to TheHive 2.12, a migration of the database will be asked. This will create a new ElasticSearch index (the_hive_10). See Updating .","title":"Database migration"},{"location":"thehive/legacy/thehive3/migration-guide/#from-210x-to-211x","text":"","title":"From 2.10.x to 2.11.x"},{"location":"thehive/legacy/thehive3/migration-guide/#database-migration_1","text":"At the first connection to TheHive 2.11, a migration of the database will be asked. This will create a new ElastciSearch index (the_hive_9). See Updating .","title":"Database migration"},{"location":"thehive/legacy/thehive3/migration-guide/#misp-to-alert","text":"MISP synchronization is now done using alerting framework. MISP events are seen like other alert. You can use TheHive4py to create your own alert.","title":"MISP to alert"},{"location":"thehive/legacy/thehive3/migration-guide/#configuration-changes","text":"","title":"Configuration changes"},{"location":"thehive/legacy/thehive3/migration-guide/#misp-certificate-authority-deprecated","text":"Specifying certificate authority in MISP configuration using \"cert\" key is now deprecated. You must replace it by - before: misp { [...] cert = \"/path/to/truststore.jks\" } - after: misp { [...] ws.ssl.trustManager.stores = [ { type: \"JKS\" path: \"/path/to/truststore.jks\" } ] } ws key can be placed in MISP server section or in global MISP section. In the latter, ws configuration will be applied on all MISP instances.","title":"MISP certificate authority deprecated"},{"location":"thehive/legacy/thehive3/migration-guide/#cortex-and-misp-http-client-options","text":"HTTP client used by Cortex and MISP is more configurable. Proxy can be configured, with or without authentication. Refer to configuration for all possible options.","title":"Cortex and MISP HTTP client options"},{"location":"thehive/legacy/thehive3/migration-guide/#packages","text":"","title":"Packages"},{"location":"thehive/legacy/thehive3/migration-guide/#new-rpm-and-deb-packages","text":"RPM and DEB packages are now available. This makes the installation easier than using a binary package (ZIP). See the Installation Guide for reference.","title":"New RPM and DEB packages"},{"location":"thehive/legacy/thehive3/migration-guide/#docker_1","text":"All-in-One docker (containing TheHive and Cortex) is not provided any longer. New TheHive docker image doesn't contain ElasticSearch. We recommend to use docker-compose to link TheHive, ElasticSearch and Cortex dockers. For more information, see the Installation Guide for reference. TheHive configuration is located in /etc/thehive/application.conf for all packages. If you use docker package you must update its location (previously was /opt/docker/conf/application.conf ).","title":"Docker"},{"location":"thehive/legacy/thehive3/admin/admin-guide/","text":"Administrator's guide # 1. User management # Users can be managed through the Administration > Users page. Only administrators may access it. Each user is identified by their login, full name and role. Please note that you still need to create user accounts if you use LDAP or Active Directory authentication. This is necessary for TheHive to retrieve their role and authenticate them against the local database, LDAP and/or AD directories. There are 4 roles currently: - read : all non-sensitive data can be read. With this role, a user can't make any change. They can't add a case, task, log or observable. They also can't run analyzers; - write : create, remove and change data of any type. This role is for standard users. write role inherits read rights; - admin : this role is reserved for TheHive administrators. Users with this role can manage user accounts, metrics, create case templates and observable data types. admin inherits write rights; - alert : users with this role can only create alerts. Warning : Please note that user accounts cannot be removed once they have been created, otherwise audit logs will refer to an unknown user. However, unwanted or unused accounts can be locked. 2. Case template management # Some cases may share the same structure (tags, tasks, description, metrics). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template. To create a template, as admin go into the administration menu, and open the \"Case templates\" item. In this screen, you can add, remove or change template. A template contains: * default severity * default tags * title prefix (can be changed by user at case creation) * default TLP * default default * task list (title and description) * metrics * custom fields Except for title prefix, task list and metrics, the user can change values defined in template. 3. Report template management # When TheHive is connected to a Cortex server, observables can be analyzed to get additional information on them. Cortex outputs reports in JSON format. In order to make reports more readable, you can configure report templates. Report templates convert JSON in to HTML using the AngularJS template engine. For each analyzer available in Cortex you can define two kinds of templates: short and long. A short report exposes synthetic information, shows in top of observable page. With short reports you can see a summary of all run analyzers. Long reports show detailed information only when the user selects the report. Raw data in JSON format is always available. Report templates can be configured in the Admin > Report templates menu. We offer report templates for default Cortex analyzers. A package with all report templates can be downloaded at https://download.thehive-project.org/report-templates.zip and can be injected using the Import templates button. 4. Metrics management # Metrics have been integrated to have relevant indicators about cases. Metrics are numerical values associated to cases (for example, the number of impacted users). Each metric has a name , a title and a description , defined by an administrator. When a metric is added to a case, it can't be removed and must be filled. Metrics are used to monitor business indicators, thanks to graphs. Metrics are defined globally. To create metrics, as admin go into the administration menu, and open the \"Case metrics\" item. Metrics are used to create statistics (\"Statistics\" item in the user profile menu). They can be filtered on time interval, and case with specific tags. For example you can show metrics of case with \"malspam\" tag on January 2016 : For graphs based on time, the user can choose metrics to show. They are aggregated on interval of time (by day, week, month of year) using a function (sum, min or max). Some metrics are predefined (in addition to those defined by administrator) like case handling duration (how much time the case had been open) and number of cases open or closed.","title":"Administrator's guide"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#administrators-guide","text":"","title":"Administrator's guide"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#1-user-management","text":"Users can be managed through the Administration > Users page. Only administrators may access it. Each user is identified by their login, full name and role. Please note that you still need to create user accounts if you use LDAP or Active Directory authentication. This is necessary for TheHive to retrieve their role and authenticate them against the local database, LDAP and/or AD directories. There are 4 roles currently: - read : all non-sensitive data can be read. With this role, a user can't make any change. They can't add a case, task, log or observable. They also can't run analyzers; - write : create, remove and change data of any type. This role is for standard users. write role inherits read rights; - admin : this role is reserved for TheHive administrators. Users with this role can manage user accounts, metrics, create case templates and observable data types. admin inherits write rights; - alert : users with this role can only create alerts. Warning : Please note that user accounts cannot be removed once they have been created, otherwise audit logs will refer to an unknown user. However, unwanted or unused accounts can be locked.","title":"1. User management"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#2-case-template-management","text":"Some cases may share the same structure (tags, tasks, description, metrics). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template. To create a template, as admin go into the administration menu, and open the \"Case templates\" item. In this screen, you can add, remove or change template. A template contains: * default severity * default tags * title prefix (can be changed by user at case creation) * default TLP * default default * task list (title and description) * metrics * custom fields Except for title prefix, task list and metrics, the user can change values defined in template.","title":"2. Case template management"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#3-report-template-management","text":"When TheHive is connected to a Cortex server, observables can be analyzed to get additional information on them. Cortex outputs reports in JSON format. In order to make reports more readable, you can configure report templates. Report templates convert JSON in to HTML using the AngularJS template engine. For each analyzer available in Cortex you can define two kinds of templates: short and long. A short report exposes synthetic information, shows in top of observable page. With short reports you can see a summary of all run analyzers. Long reports show detailed information only when the user selects the report. Raw data in JSON format is always available. Report templates can be configured in the Admin > Report templates menu. We offer report templates for default Cortex analyzers. A package with all report templates can be downloaded at https://download.thehive-project.org/report-templates.zip and can be injected using the Import templates button.","title":"3. Report template management"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#4-metrics-management","text":"Metrics have been integrated to have relevant indicators about cases. Metrics are numerical values associated to cases (for example, the number of impacted users). Each metric has a name , a title and a description , defined by an administrator. When a metric is added to a case, it can't be removed and must be filled. Metrics are used to monitor business indicators, thanks to graphs. Metrics are defined globally. To create metrics, as admin go into the administration menu, and open the \"Case metrics\" item. Metrics are used to create statistics (\"Statistics\" item in the user profile menu). They can be filtered on time interval, and case with specific tags. For example you can show metrics of case with \"malspam\" tag on January 2016 : For graphs based on time, the user can choose metrics to show. They are aggregated on interval of time (by day, week, month of year) using a function (sum, min or max). Some metrics are predefined (in addition to those defined by administrator) like case handling duration (how much time the case had been open) and number of cases open or closed.","title":"4. Metrics management"},{"location":"thehive/legacy/thehive3/admin/backup-restore/","text":"Backup and restore data # All persistent data is stored in an Elasticsearch database. The backup and restore procedures are the ones that are detailed in Elasticsearch documentation . Note : you may have to adapt your indices in the examples below. To find the right index, use the following command : curl 'localhost:9200/_cat/indices?v' You can also refer to the schema version page. To save all your data you only need to backup the last indice. For example, if the previous command gives you the following results, all your data belongs to the_hive_12 . health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open the_hive_11 HVVYDC68SrGAfSbcjVPZWg 5 1 43018 17 24.9mb 24.9mb yellow open the_hive_12 Cq4Gc4qkRPaTCqrorFgDRw 5 1 43226 0 25.3mb 25.3mb In the rest of this document, ensure to change to your own last index in order to backup or restore all your data. 1. Create a backup repository # First you must define a location in local filesystem (where Elasticsearch instance runs) where the backup will be written. This repository must be declared in the Elasticsearch configuration. Edit elasticsearch.yml file by adding: path.repo: [\"/absolute/path/to/backup/directory\"] Then, restart the Elasticsearch service. Note : Be careful if you run Elasticsearch in Docker, the directory must be mapped in host filesystem using --volume parameter (cf. Docker documentation ). 2. Register a snapshot repository # Create an Elasticsearch snapshot point named the_hive_backup with the following command (set the same path in the location setting as the one set in the configuration file): $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -H 'Content-Type: application/json' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' The result of the command should look like this : {\"acknowledged\":true} Since, everything is fine to backup and restore data. 3. Backup your data # Create a backup named snapshot_1 of all your data by executing the following command : $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -H 'Content-Type: application/json' -d '{ \"indices\": \"<INDEX>\" }' This command terminates only when the backup is complete and the result of the command should look like this: { \"snapshots\": [{ \"snapshot\": \"snapshot_1\", \"uuid\": \"ZQ3kv5-FQoeN3NFIhfKgMg\", \"version_id\": 5060099, \"version\": \"5.6.0\", \"indices\": [\"the_hive_12\"], \"state\": \"SUCCESS\", \"start_time\": \"2018-01-29T14:41:51.580Z\", \"start_time_in_millis\": 1517236911580, \"end_time\": \"2018-01-29T14:42:05.216Z\", \"end_time_in_millis\": 1517236925216, \"duration_in_millis\": 13636, \"failures\": [], \"shards\": { \"total\": 41, \"failed\": 0, \"successful\": 41 } }] } Note : You can backup the last index of TheHive (you can list indices in your Elasticsearch cluster with curl -s http://localhost:9200/_cat/indices | cut -d ' ' -f3 ) or all indices with _all value. 4. Restore data # Restore will do the reverse actions : it reads the backup in your snapshot directory and loads indices into the Elasticsearch cluster. This operation is done with the following command : $ curl -XPOST 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1/_restore' -d ' { \"indices\": \"<INDEX>\" }' The result of the command should look like this : {\"accepted\":true} Note : be sure to restore data from the same version of Elasticsearch. 5. Moving data from one server to another # If you want to move your data from one server from another: - Create your backup on the origin server (steps 1 , 2 , 3 ) - copy your backup directory from the origin server to the destination server - On the destination server : - Register your backup repository in the Elasticsearch configuration (step 1 ) - Register your snapshot repository with the same snapshot name (step 2 ) - Restore your data (step 4 )","title":"Backup and restore data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#backup-and-restore-data","text":"All persistent data is stored in an Elasticsearch database. The backup and restore procedures are the ones that are detailed in Elasticsearch documentation . Note : you may have to adapt your indices in the examples below. To find the right index, use the following command : curl 'localhost:9200/_cat/indices?v' You can also refer to the schema version page. To save all your data you only need to backup the last indice. For example, if the previous command gives you the following results, all your data belongs to the_hive_12 . health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open the_hive_11 HVVYDC68SrGAfSbcjVPZWg 5 1 43018 17 24.9mb 24.9mb yellow open the_hive_12 Cq4Gc4qkRPaTCqrorFgDRw 5 1 43226 0 25.3mb 25.3mb In the rest of this document, ensure to change to your own last index in order to backup or restore all your data.","title":"Backup and restore data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#1-create-a-backup-repository","text":"First you must define a location in local filesystem (where Elasticsearch instance runs) where the backup will be written. This repository must be declared in the Elasticsearch configuration. Edit elasticsearch.yml file by adding: path.repo: [\"/absolute/path/to/backup/directory\"] Then, restart the Elasticsearch service. Note : Be careful if you run Elasticsearch in Docker, the directory must be mapped in host filesystem using --volume parameter (cf. Docker documentation ).","title":"1. Create a backup repository"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#2-register-a-snapshot-repository","text":"Create an Elasticsearch snapshot point named the_hive_backup with the following command (set the same path in the location setting as the one set in the configuration file): $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -H 'Content-Type: application/json' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' The result of the command should look like this : {\"acknowledged\":true} Since, everything is fine to backup and restore data.","title":"2. Register a snapshot repository"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#3-backup-your-data","text":"Create a backup named snapshot_1 of all your data by executing the following command : $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -H 'Content-Type: application/json' -d '{ \"indices\": \"<INDEX>\" }' This command terminates only when the backup is complete and the result of the command should look like this: { \"snapshots\": [{ \"snapshot\": \"snapshot_1\", \"uuid\": \"ZQ3kv5-FQoeN3NFIhfKgMg\", \"version_id\": 5060099, \"version\": \"5.6.0\", \"indices\": [\"the_hive_12\"], \"state\": \"SUCCESS\", \"start_time\": \"2018-01-29T14:41:51.580Z\", \"start_time_in_millis\": 1517236911580, \"end_time\": \"2018-01-29T14:42:05.216Z\", \"end_time_in_millis\": 1517236925216, \"duration_in_millis\": 13636, \"failures\": [], \"shards\": { \"total\": 41, \"failed\": 0, \"successful\": 41 } }] } Note : You can backup the last index of TheHive (you can list indices in your Elasticsearch cluster with curl -s http://localhost:9200/_cat/indices | cut -d ' ' -f3 ) or all indices with _all value.","title":"3. Backup your data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#4-restore-data","text":"Restore will do the reverse actions : it reads the backup in your snapshot directory and loads indices into the Elasticsearch cluster. This operation is done with the following command : $ curl -XPOST 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1/_restore' -d ' { \"indices\": \"<INDEX>\" }' The result of the command should look like this : {\"accepted\":true} Note : be sure to restore data from the same version of Elasticsearch.","title":"4. Restore data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#5-moving-data-from-one-server-to-another","text":"If you want to move your data from one server from another: - Create your backup on the origin server (steps 1 , 2 , 3 ) - copy your backup directory from the origin server to the destination server - On the destination server : - Register your backup repository in the Elasticsearch configuration (step 1 ) - Register your snapshot repository with the same snapshot name (step 2 ) - Restore your data (step 4 )","title":"5. Moving data from one server to another"},{"location":"thehive/legacy/thehive3/admin/certauth/","text":"Single Sign-On on TheHive with X.509 Certificates # Abstract # SSL managed by TheHive is known to have some stability problem. It is advise to not enable it in production and configure SSL on a reverse proxy, in front of TheHive. This make X509 certificate authentication non applicable. In order to do x509 authentication it is recommended to do it in the reverse proxy and then forward user identity to TheHive in a HTTP header. This feature has been added in version 3.2. WARNING This setup is valid only if nobody except the reverse proxy can connect to TheHive. Users must have to use the reverse proxy. Otherwise, an user would be able to choose his identity on TheHive. Setup a reverse proxy # If you use nginx, the site configuration file should look like: server { listen 443 ssl; server_name thehive.example.com; ssl on; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; # Force client to have a certificate ssl_verify_client on; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; # Map certificate DN to user login stored in TheHive map $ssl_client_s_dn $thehive_user { default \"\"; /C=FR/O=TheHive-Project/CN=Thomas toom; /C=FR/O=TheHive-Project/CN=Georges bofh; }; # Redirect all request to local TheHive location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; # Send the mapped user login to TheHive, in THEHIVE_USER HTTP header proxy_set_header THEHIVE_USER $thehive_user; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; } } Enable authentication delegation in TheHive # Setup TheHive to identify user by the configured HTTP header (THEHIVE_USER): auth { method.header = true header.name = THEHIVE_USER } # Listen only on localhost to prevent direct access to TheHive http.address=127.0.0.1","title":"Single Sign-On on TheHive with X.509 Certificates"},{"location":"thehive/legacy/thehive3/admin/certauth/#single-sign-on-on-thehive-with-x509-certificates","text":"","title":"Single Sign-On on TheHive with X.509 Certificates"},{"location":"thehive/legacy/thehive3/admin/certauth/#abstract","text":"SSL managed by TheHive is known to have some stability problem. It is advise to not enable it in production and configure SSL on a reverse proxy, in front of TheHive. This make X509 certificate authentication non applicable. In order to do x509 authentication it is recommended to do it in the reverse proxy and then forward user identity to TheHive in a HTTP header. This feature has been added in version 3.2. WARNING This setup is valid only if nobody except the reverse proxy can connect to TheHive. Users must have to use the reverse proxy. Otherwise, an user would be able to choose his identity on TheHive.","title":"Abstract"},{"location":"thehive/legacy/thehive3/admin/certauth/#setup-a-reverse-proxy","text":"If you use nginx, the site configuration file should look like: server { listen 443 ssl; server_name thehive.example.com; ssl on; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; # Force client to have a certificate ssl_verify_client on; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; # Map certificate DN to user login stored in TheHive map $ssl_client_s_dn $thehive_user { default \"\"; /C=FR/O=TheHive-Project/CN=Thomas toom; /C=FR/O=TheHive-Project/CN=Georges bofh; }; # Redirect all request to local TheHive location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; # Send the mapped user login to TheHive, in THEHIVE_USER HTTP header proxy_set_header THEHIVE_USER $thehive_user; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; } }","title":"Setup a reverse proxy"},{"location":"thehive/legacy/thehive3/admin/certauth/#enable-authentication-delegation-in-thehive","text":"Setup TheHive to identify user by the configured HTTP header (THEHIVE_USER): auth { method.header = true header.name = THEHIVE_USER } # Listen only on localhost to prevent direct access to TheHive http.address=127.0.0.1","title":"Enable authentication delegation in TheHive"},{"location":"thehive/legacy/thehive3/admin/cluster/","text":"Cluster Configuration # Starting from version 3.1.0, TheHive can scale horizontally very easily. You can dynamically add nodes to your cluster to increase the performance of the platform. TheHive API is stateless to the exclusion of the stream (or real-time flow). For this reason, the cluster nodes need to communicate with each other. The first node of the cluster has a specific role: it must initiate the cluster creation. Any additional node only needs to contact at least one node of the cluster to join it. This is done by configuring so-called seed nodes . The first node must have itself in the seed node list. The other nodes must have at least one entry corresponding to a node that has already joined the seed node list. Note : all cluster nodes must share the same secret ( play.http.secret.key in application.conf ). Configuration # Define node1 (for example with IP address 10.0.0.1 ) as the first node of the cluster. The configuration section in application.conf should look like the following: akka { remote { netty.tcp { hostname = \"10.0.0.1\" port = 2552 } } # seed node is itself as it is the first node of the cluster cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } Then add another node. Let's call it node2 and assume its IP address is 10.0.0.2 to our one-node cluster. You can see that it is referring to the first node in cluster.seed-nodes : akka { remote { netty.tcp { hostname = \"10.0.0.2\" port = 2552 } } # seed node list contains at least one active node cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } We recommend defining several seed nodes in the respective configuration files, except for the first one. For example: node configured seed nodes node1 node1 node2 node1, node3 node3 node2, node4 node4 node1, node2, node3 Load Balancing # In front of TheHive cluster, you can add a load balancer which distributes HTTP requests to cluster nodes. One client does not need to always use the same node as affinity is not required. Below is an non-optimized example of a haproxy configuration: # Global standard configuration, nothing specific for TheHive global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon ca-base /etc/ssl/certs crt-base /etc/ssl/private ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 500 timeout client 50000 # server timeout must be at least the stream.refresh parameter in application.conf timeout server 2m errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # Listen on all interfaces, on port 9000/tcp frontend http-in bind *:9000 default_backend servers # Configure all cluster node backend servers balance roundrobin server node1 10.0.0.1:9000 check server node2 10.0.0.2:9000 check server node3 10.0.0.3:9000 check server node4 10.0.0.4:9000 check Troubleshooting # Should you encounter troubles with your setup, you can enable debug messages with the following configuration: akka { actor { debug { receive = on autoreceive = on lifecycle = on unhandled = on } } } Additional Information # TheHive Leverages Akka Cluster. You can refer to the Akka documentation for additional information.","title":"Cluster Configuration"},{"location":"thehive/legacy/thehive3/admin/cluster/#cluster-configuration","text":"Starting from version 3.1.0, TheHive can scale horizontally very easily. You can dynamically add nodes to your cluster to increase the performance of the platform. TheHive API is stateless to the exclusion of the stream (or real-time flow). For this reason, the cluster nodes need to communicate with each other. The first node of the cluster has a specific role: it must initiate the cluster creation. Any additional node only needs to contact at least one node of the cluster to join it. This is done by configuring so-called seed nodes . The first node must have itself in the seed node list. The other nodes must have at least one entry corresponding to a node that has already joined the seed node list. Note : all cluster nodes must share the same secret ( play.http.secret.key in application.conf ).","title":"Cluster Configuration"},{"location":"thehive/legacy/thehive3/admin/cluster/#configuration","text":"Define node1 (for example with IP address 10.0.0.1 ) as the first node of the cluster. The configuration section in application.conf should look like the following: akka { remote { netty.tcp { hostname = \"10.0.0.1\" port = 2552 } } # seed node is itself as it is the first node of the cluster cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } Then add another node. Let's call it node2 and assume its IP address is 10.0.0.2 to our one-node cluster. You can see that it is referring to the first node in cluster.seed-nodes : akka { remote { netty.tcp { hostname = \"10.0.0.2\" port = 2552 } } # seed node list contains at least one active node cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } We recommend defining several seed nodes in the respective configuration files, except for the first one. For example: node configured seed nodes node1 node1 node2 node1, node3 node3 node2, node4 node4 node1, node2, node3","title":"Configuration"},{"location":"thehive/legacy/thehive3/admin/cluster/#load-balancing","text":"In front of TheHive cluster, you can add a load balancer which distributes HTTP requests to cluster nodes. One client does not need to always use the same node as affinity is not required. Below is an non-optimized example of a haproxy configuration: # Global standard configuration, nothing specific for TheHive global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon ca-base /etc/ssl/certs crt-base /etc/ssl/private ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 500 timeout client 50000 # server timeout must be at least the stream.refresh parameter in application.conf timeout server 2m errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # Listen on all interfaces, on port 9000/tcp frontend http-in bind *:9000 default_backend servers # Configure all cluster node backend servers balance roundrobin server node1 10.0.0.1:9000 check server node2 10.0.0.2:9000 check server node3 10.0.0.3:9000 check server node4 10.0.0.4:9000 check","title":"Load Balancing"},{"location":"thehive/legacy/thehive3/admin/cluster/#troubleshooting","text":"Should you encounter troubles with your setup, you can enable debug messages with the following configuration: akka { actor { debug { receive = on autoreceive = on lifecycle = on unhandled = on } } }","title":"Troubleshooting"},{"location":"thehive/legacy/thehive3/admin/cluster/#additional-information","text":"TheHive Leverages Akka Cluster. You can refer to the Akka documentation for additional information.","title":"Additional Information"},{"location":"thehive/legacy/thehive3/admin/configuration/","text":"Configuration Guide # The configuration file of TheHive is /etc/thehive/application.conf by default. This file uses the HOCON format . All configuration parameters should go in this file. You can have a look at the default settings . Table of Contents # 1. Database 2. Datastore 3. Authentication 3.1 LDAP/AD 3.2 OAuth2/OpenID Connect 4. Streaming (a.k.a The Flow) 5. Entity size limit 6. Cortex 7. MISP 7.1 Configuration 7.2 Associate a Case Template to Alerts corresponding to MISP events 7.3 Event Filters 7.4 MISP Purpose 8. HTTP Client Configuration 9. Monitoring and Performance Metrics (deprecated) 10. HTTPS 10.1 HTTPS using a reverse proxy 10.2 HTTPS without reverse proxy 10.3 Strengthen security 1. Database # TheHive uses the Elasticsearch search engine to store all persistent data. Elasticsearch is not part of TheHive package. It must be installed and configured as a standalone instance which can be located on the same machine. For more information on how to set up Elasticsearch, please refer to Elasticsearch installation guide . Three settings are required to connect to Elasticsearch: * the base name of the index * the name of the cluster * the address(es) and port(s) of the Elasticsearch instance The Defaults settings are: # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200/\" # Scroll keepalive keepalive = 1m # Size of the page for scroll pagesize = 50 # Number of shards nbshards = 5 # Number of replicas nbreplicas = 1 # Arbitrary settings settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } ## Authentication configuration #search.username = \"\" #search.password = \"\" ## SSL configuration #search.keyStore { # path = \"/path/to/keystore\" # type = \"JKS\" # or PKCS12 # password = \"keystore-password\" #} #search.trustStore { # path = \"/path/to/trustStore\" # type = \"JKS\" # or PKCS12 # password = \"trustStore-password\" #} } If you use a different configuration, modify the parameters accordingly in the application.conf file. If multiple Elasticsearch nodes are used as a cluster, you should add addresses of the master nodes in the url like this: search { uri = http://node1:9200,node2:9200/ ... TheHive uses the http port of Elasticsearch (9200/tcp by default). TheHive versions index schema (mapping) in Elasticsearch. Version numbers are appended to the index base name (the 8th version of the schema uses the index the_hive_8 if search.index = the_hive ). When too many documents are requested to TheHive, it uses the scroll feature: the results are retrieved through pagination. You can specify the size of the page ( search.pagesize ) and how long pages are kept in Elasticsearch (( search.keepalive ) before purging. 2. Datastore # TheHive stores attachments as Elasticsearch documents. They are split in chunks and each chunk sent to Elasticsearch is identified by the hash of the entire attachment and the associated chunk number. The chunk size ( datastore.chunksize ) can be changed but any change will only affect new attachments. Existing ones won't be changed. An attachment is identified by its hash. The algorithm used is configurable ( datastore.hash.main ) but must not be changed after the first attachment insertion. Otherwise, previous files cannot be retrieved. Extra hash algorithms can be configured using datastore.hash.extra . These hashes are not used to identify the attachment but are shown in the user interface (the hash associated to the main algorithm is also shown). If you change extra algorithms, you should inform TheHive and ask it to recompute all hashes. Please note that the associated API call is currently disabled in Buckfast (v 2.10). It will be reinstated in the next release. Observables can contain malicious data. When you try to download an attachment from an observable (typically a file), it is automatically zipped and the resulting ZIP file is password-protected. The default password is malware but it can be changed with the datastore.attachment.password setting. Default values are: # Datastore datastore { name = data # Size of stored data chunks chunksize = 50k hash { # Main hash algorithm /!\\ Don't change this value main = \"SHA-256\" # Additional hash algorithms (used in attachments) extra = [\"SHA-1\", \"MD5\"] } attachment.password = \"malware\" } 3. Authentication # TheHive supports local, LDAP, Active Directory (AD) or OAuth2/OpenID Connect for authentication. By default, it relies on local credentials stored in Elasticsearch. Authentication methods are stored in the auth.provider parameter, which is multi-valued. When a user logs in, each authentication method is tried in order until one succeeds. If no authentication method works, an error is returned and the user cannot log in. The default values within the configuration file are: auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # services.LocalAuthSrv : passwords are stored in user entity (in Elasticsearch). No configuration is required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key # oauth2 : use OAuth/OIDC to authenticate users. Configuration is under \"auth.oauth2\" and \"auth.sso\" keys provider = [local] # By default, basic authentication is disabled. You can enable it by setting \"method.basic\" to true. #method.basic = true ad { # The Windows domain name in DNS format. This parameter is required if you do not use # 'serverNames' below. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers instead of using 'domainFQDN # above. If this parameter is not set, TheHive uses 'domainFQDN'. #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Windows domain name using short format. This parameter is required. #domainName = \"MYDOMAIN\" # If 'true', use SSL to connect to the domain controller. #useSSL = true } ldap { # The LDAP server name or address. The port can be specified using the 'host:port' # syntax. This parameter is required if you don't use 'serverNames' below. #serverName = \"ldap.mydomain.local:389\" # If you have multiple LDAP servers, use the multi-valued setting 'serverNames' instead. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Account to use to bind to the LDAP server. This parameter is required. #bindDN = \"cn=thehive,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user in the directory server. Please note that {0} is replaced # by the actual user name. This parameter is required. #filter = \"(cn={0})\" # If 'true', use SSL to connect to the LDAP directory server. #useSSL = true } oauth2 { # URL of the authorization server #clientId = \"client-id\" #clientSecret = \"client-secret\" #redirectUri = \"https://my-thehive-instance.example/api/ssoLogin\" #responseType = \"code\" #grantType = \"authorization_code\" # URL from where to get the access token #authorizationUrl = \"https://auth-site.com/OAuth/Authorize\" #tokenUrl = \"https://auth-site.com/OAuth/Token\" # The endpoint from which to obtain user details using the OAuth token, after successful login #userUrl = \"https://auth-site.com/api/User\" #scope = [\"openid profile\"] } # Single-Sign On sso { # Autocreate user in database? #autocreate = false # Autoupdate its profile and roles? #autoupdate = false # Autologin user using SSO? #autologin = false # Attributes mappings #attributes { # login = \"sub\" # name = \"name\" # groups = \"groups\" # #roles = \"roles\" #} # Name of mapping class from user resource to backend user ('simple' or 'group') #mapper = group # Default roles for users with no groups mapped (\"read\", \"write\", \"admin\") #defaultRoles = [] #groups { # # URL to retrieve groups (leave empty if you are using OIDC) # #url = \"https://auth-site.com/api/Groups\" # # Group mappings, you can have multiple roles for each group: they are merged # mappings { # admin-profile-name = [\"admin\"] # editor-profile-name = [\"write\"] # reader-profile-name = [\"read\"] # } #} } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h } 3.1. LDAP/AD # To enable authentication using AD or LDAP, edit the application.conf file and supply the values for your environment. Then you need to create an account on TheHive for each AD or LDAP user in Administration > Users page (which can only be accessed by an administrator). This is required as TheHive needs to look up the role associated with the user and that role is stored locally by TheHive. Obviously, you don't need to supply a password as TheHive will check the credentials against the remote directory. In order to use SSL on LDAP or AD, TheHive must be able to validate remote certificates. To that end, the Java truststore must contain certificate authorities used to generate the AD and/or LDAP certificates. The Default JVM truststore contains the main official authorities but LDAP and AD certificates are probably not issued by them. Use keytool to create the truststore: keytool -import -file /path/to/your/ca.cert -alias InternalCA -keystore /path/to/your/truststore.jks Then add -Djavax.net.ssl.trustStore=/path/to/your/truststore.jks parameter when you start TheHive or put it in the JAVA_OPTS environment variable before starting TheHive. 3.2. OAuth2/OpenID Connect # To enable authentication using OAuth2/OpenID Connect, edit the application.conf file and supply the values of auth.oauth2 according to your environment. In addition, you need to supply: auth.sso.attributes.login : name of the attribute containing the OAuth2 user's login in retreived user info (mandatory) auth.sso.attributes.name : name of the attribute containing the OAuth2 user's name in retreived user info (mandatory) auth.sso.attributes.groups : name of the attribute containing the OAuth2 user's groups (mandatory using groups mappings) auth.sso.attributes.roles : name of the attribute containing the OAuth2 user's roles in retreived user info (mandatory using simple mapping) Important notes # Authenticate the user using an external OAuth2 authenticator server. The configuration is: clientId (string) client ID in the OAuth2 server. clientSecret (string) client secret in the OAuth2 server. redirectUri (string) the url of TheHive AOuth2 page (.../api/ssoLogin). responseType (string) type of the response. Currently only \"code\" is accepted. grantType (string) type of the grant. Currently only \"authorization_code\" is accepted. authorizationUrl (string) the url of the OAuth2 server. authorizationHeader (string) prefix of the authorization header to get user info: Bearer, token, ... tokenUrl (string) the token url of the OAuth2 server. userUrl (string) the url to get user information in OAuth2 server. scope (list of string) list of scope. Example # auth { provider = [local, oauth2] [..] sso { autocreate: false autoupdate: false mapper: \"simple\" attributes { login: \"login\" name: \"name\" roles: \"role\" } defaultRoles: [\"read\", \"write\"] defaultOrganization: \"demo\" } oauth2 { name: oauth2 clientId: \"Client_ID\" clientSecret: \"Client_ID\" redirectUri: \"http://localhost:9000/api/ssoLogin\" responseType: code grantType: \"authorization_code\" authorizationUrl: \"https://github.com/login/oauth/authorize\" authorizationHeader: \"token\" tokenUrl: \"https://github.com/login/oauth/access_token\" userUrl: \"https://api.github.com/user\" scope: [\"user\"] } [..] } 3.2.1. Roles mappings # You can choose a roles mapping with the auth.sso.mapper parameter. The available options are simple and group : Using simple mapping, we assume that the user info retrieved from auth.oauth2.userUrl contains the roles associated to the OAuth2 user. They can be: read , write or admin . Using groups mappings, we assume the retrieved user info contains groups that have to be associated to internal roles. In that case, you have to define mapppings in auth.sso.groups.mappings . If a user has multiple groups, mapped roles are merged. If you need to retreive groups from another endpoint that the one used for user info, you can provide it in auth.sso.groups.url . The retrieved groups can be a valid JSON array or a string listing them: { \"sub\": \"userid1\", \"name\": \"User name 1\", \"groups\": [\"admin-profile-name\", \"reader-profile-name\"] } OR { \"sub\": \"userid2\", \"name\": \"User name 2\", \"groups\": \"[admin-profile-name, reader-profile-name, \\\"another profile\\\", 'a last group']\" } OR { \"sub\": \"userid3\", \"name\": \"User name 3\", \"groups\": \"the-only-group-of-the-user\" } Finally, you can setup default roles associated with user with no roles/groups retrieved, using the auth.sso.defaultRoles parameter. 3.2.2. User autocreation, autoupdate and autologin # The main advantage of OAuth2/OpenID Connect authentication is you won't need to create an account on TheHive for each OAuth2 user if you set the config parameter auth.sso.autocreate to true . However, by default, OAuth2 users won't be updated on SSO login unless you set auth.sso.autoupdate to true . If you set this last parameter, roles and name will be fetched from retrieved user info and will be updated in local database on each login of the user. With auth.sso.autologin set to true , each user connecting to TheHive will automatically be redirected to auth.oauth2.authorizationUrl . The only way to authenticate in TheHive using a local user will be either: Connecting to TheHive using https://my-hive-instance.com/index.html#!/login?code=BAD_CODE . You will get an Authentication Failure but will then be able to authenticate. Connecting to TheHive using a real OAuth2 account, then disconnect. On disconnection, you won't be redirected to authorization URL. 3.2.3. Debugging # To debug the OAuth2 feature, you can uncomment the following lines in /etc/thehive/logback.xml : <!-- Uncomment the next lines to log debug information for OAuth/OIDC login --> <logger name=\"org.elastic4play.services.auth\" level=\"DEBUG\" /> <logger name=\"services.OAuth2Srv\" level=\"DEBUG\" /> <logger name=\"services.mappers\" level=\"DEBUG\" /> 4. Streaming (a.k.a The Flow) # The user interface is automatically updated when data is changed in the back-end. To do this, the back-end sends events to all the connected front-ends. The mechanism used to notify the front-end is called long polling and its settings are: refresh : when there is no notification, close the connection after this duration (the default is 1 minute). cache : before polling a session must be created, in order to make sure no event is lost between two polls. If there is no poll during the cache setting, the session is destroyed (the default is 15 minutes). nextItemMaxWait , globalMaxWait : when an event occurs, it is not immediately sent to the front-ends. The back-end waits nextItemMaxWait and up to globalMaxWait in case another event can be included in the notification. This mechanism saves many HTTP requests. Default values are: # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s } 5. Entity size limit # The Play framework used by TheHive sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. This could be too small in most cases so you may want to change it with the following settings in the application.conf file: # Max textual content length play.http.parser.maxMemoryBuffer=1M # Max file size play.http.parser.maxDiskBuffer=1G Note : if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file. 6. Cortex # TheHive can use one or several Cortex analysis engines to get additional information on observables. When configured, analyzers available in Cortex become usable on TheHive. First you must enable CortexConnector , choose an identifier then specify the URL for each Cortex server: ## Enable the Cortex module play.modules.enabled += connectors.cortex.CortexConnector cortex { \"CORTEX-SERVER-ID\" { # URL of the Cortex server url = \"http://CORTEX_SERVER:CORTEX_PORT\" # Key of the Cortex user, mandatory for Cortex 2 key = \"API key\" } # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # Check job update time interval refreshDelay = 1 minute # Maximum number of successive errors before give up maxRetryOnError = 3 # Check remote Cortex status time interval statusCheckInterval = 1 minute } If you connect TheHive with Cortex 2, you must create a user in Cortex with the read, analyze roles, set an API key and add this key in the Cortex server definition in TheHive application.conf . For Cortex 1, authentication is not required, the key is not used. To create a user with the read, analyze role in Cortex 2, you must have at least one organization configured then you can connect to the Cortex 2 Web UI using a orgAdmin account for that organization to create the user and generate their API key. Please refer to the Cortex Quick Start Guide for more information. Cortex analyzes observables and outputs reports in JSON format. TheHive shows the report as-is by default. In order to make reports more readable, we provide report templates which are in a separate package and must be installed manually: - download the report template package from https://dl.bintray.com/thehive-project/binary/report-templates.zip - log in TheHive using an administrator account - go to Admin > Report templates menu - click on Import templates button and select the downloaded package HTTP client used by Cortex connector use global configuration (in play.ws ) but can be overridden in Cortex section and in each Cortex server configuration. Refer to section 8 for more detail on how to configure HTTP client. 7. MISP # TheHive has the ability to connect to one or several MISP instances in order to import and export events. Hence TheHive is able to: receive events as they are added or updated from multiple MISP instances. These events will appear within the Alerts pane. export cases as MISP events to one or several MISP instances. The exported cases will not be published automatically though as they need to be reviewed prior to publishing. We strongly advise you to review the categories and types of attributes at least, before publishing the corresponding MISP events. Note : Please note that only and all the observables marked as IOCs will be used to create the MISP event. Any other observable will not be shared. This is not configurable. Within the configuration file, you can register your MISP server(s) under the misp configuration keyword. Each server shall be identified using an arbitrary name, its url , the corresponding authentication key and optional tags to add each observable created from a MISP event. Any registered server will be used to import events as alerts. It can also be used to export cases to as MISP events, if the account used by TheHive on the MISP instance has sufficient rights. This means that TheHive can import events from configured MISP servers and export cases to the same configured MISP servers. Having different configuration for sources and destination servers is expected in a future version. Important Notes # TheHive requires MISP 2.4.73 or better . Make sure that your are using a compatible version of MISP before reporting problems. MISP 2.4.72 and below do not work correctly with TheHive. 7.1 Configuration # To sync with a MISP server and retrieve events or export cases, edit the application.conf file and adjust the example shown below to your setup: ## Enable the MISP module (import and export) play.modules.enabled += connectors.misp.MispConnector misp { \"MISP-SERVER-ID\" { # URL of the MISP instance. url = \"<The_URL_of_the_MISP_Server_goes_here>\" # Authentication key. key = \"<the_auth_key_goes_here>\" # Name of the case template created in TheHive that shall be used to import # MISP events as cases by default. caseTemplate = \"<Template_Name_goes_here>\" # Tags to add to each observable imported from an event available on # this instance. tags = [\"misp-server-id\"] # Truststore to use to validate the X.509 certificate of the MISP # instance if the default truststore is not sufficient. #ws.ssl.trustManager.stores = [ #{ # type: \"JKS\" # path: \"/path/to/truststore.jks\" #} #] # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } whitelist.tags = [\"whitelist-tag1\", \"whitelist-tag2\"] # MISP purpose defines if this instance can be used to import events (ImportOnly), export cases (ExportOnly) or both (ImportAndExport) # Default is ImportAndExport purpose = ImportAndExport } # Check remote TheHive status time interval statusCheckInterval = 1 minute # Interval between consecutive MISP event imports in hours (h) or # minutes (m). interval = 1h } The HTTP client used by the MISP connector uses a global configuration (in play.ws ) but it can be overridden within the MISP section of the configuation file and/or in the configuration section of each MISP server (in misp.MISP-SERVER-ID.ws ). Refer to section 8 for more details on how to configure the HTTP client. 7.2 Associate a Case Template to Alerts corresponding to MISP events # As stated in the subsection above, TheHive is able to automatically import MISP events (they will appear as alerts within the Alerts pane) and create cases out of them. This operation leverages the template engine. Thus you'll need to create a case template prior to importing MISP events. First, create a case template. Let's call it MISP-EVENT . Then update TheHive's configuration to add a 'caseTemplate' parameter as shown in the example below: misp { \"MISP-SERVER-ID\" { # URL of the MISP server url = \"<The_URL_of_the_MISP_Server_goes_here>\" # authentication key key = \"<the_auth_key_goes_here>\" # tags that must be automatically added to the case corresponding to the imported event tags = [\"misp\"] # case template caseTemplate = \"MISP-EVENT\" } Once the configuration file has been edited, restart TheHive. Every new import of a MISP event will generate a case using to the MISP-EVENT template by default. The template can be overridden though during the event import. MISP events will only be imported by TheHive if they have at least one attribute and were published. 7.3 Event Filters # When you first connect TheHive to a MISP instance, you can be overwhelmed by the number of alerts that will be generated, particularly if the MISP instance contains a lot of events. Indeed, every event, even those that date back to the beginning of the Internet, will generate an alert. To avoid alert fatigue, and starting from TheHive 3.0.4 (Cerana 0.4), you can exclude MISP events using different filters: the maximum number of attributes (max-attributes) the maximum size of the event's JSON message (max-size) the maximum age of the last publication (max-age) the organisation is black-listed (exclusion.organisation) one of the tags is black-listed (exclusion.tags) doesn't contain one of the whitelist tag (whitelist.tags) Please note that MISP event filters can be adapted to the configuration associated to each MISP server TheHive is connected with. As regards the max-age filter, it applies to the publication date of MISP events and not to the creation date. In the example below, the following MISP events won't generate alerts in TheHive: events that have more than 1000 attributes events which JSON message size is greater than 1MB events that have been published more than one week from the current date events that have been created by bad organisation or other orga events that contain tag1 or tag2 # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } Of course, you can omit some of the filters or all of them. 7.4 MISP Purpose # TheHive can interact with MISP in two ways: import a MISP event to create a case in TheHive and export a TheHive case to create a MISP event. By default, any MISP instance that is added to TheHive's configuration will be used for importing events and exporting cases ( ImportAndExport ). If you want to use MISP in only one way, you can set its purpose in the configuration as ImportOnly or ExportOnly . Starting from TheHive 3.3, when exporting a case to a MISP instance, you can export all its tags to the freshly created MISP event. This behaviour is not enabled by default. If you want to enable it you must set the exportCaseTags variable to true as shown below: misp { \"local\" { url = \"http://127.0.0.1\" key = \"<the_auth_key_goes_here>\" exportCaseTags = true [...] # additional parameters go here } } 8. HTTP Client Configuration # HTTP client can be configured by adding ws key in sections that needs to connect to remote HTTP service. The key can contain configuration items defined in play WS configuration : ws.followRedirects : Configures the client to follow 301 and 302 redirects (default is true). ws.useragent : To configure the User-Agent header field. ws.compressionEnabled : Set it to true to use gzip/deflater encoding (default is false). Timeouts # There are 3 different timeouts in WS. Reaching a timeout causes the WS request to interrupt. - ws.timeout.connection : The maximum time to wait when connecting to the remote host (default is 120 seconds). - ws.timeout.idle : The maximum time the request can stay idle (connection is established but waiting for more data) (default is 120 seconds). - ws.timeout.request : The total time you accept a request to take (it will be interrupted even if the remote host is still sending data) (default is 120 seconds). Proxy # Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. - ws.useProxyProperties : To use the JVM system\u2019s HTTP proxy settings (http.proxyHost, http.proxyPort) (default is true). This setting is ignored if ws.proxy settings is present. - ws.proxy.host : The hostname of the proxy server. - ws.proxy.post : The port of the proxy server. - ws.proxy.protocol : The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. - ws.proxy.user : The username of the credentials for the proxy server. - ws.proxy.password : The password for the credentials for the proxy server. - ws.proxy.ntlmDomain : The password for the credentials for the proxy server. - ws.proxy.encoding : The realm's charset. - ws.proxy.nonProxyHosts : The list of hosts on which proxy must not be used. SSL # SSL of HTTP client can be completely configured in application.conf file. Certificate manager # Certificate manager is used to store client certificates and certificate authorities. keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) ws.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] } Certificate authorities are configured using trustManager key. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. ws.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } Debugging # To debug the key manager / trust manager, set the following flags: ws.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true } Protocols # If you want to define a different default protocol, you can set it specifically in the client: ws.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] Ciphers # Cipher suites can be configured using ws.ssl.enabledCipherSuites : ws.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ] 9. Monitoring and Performance Metrics (deprecated) # Performance metrics (response time, call rate to Elasticsearch and HTTP request, throughput, memory used...) can be collected if enabled in configuration. Enable it by editing the application.conf file, and add: # Register module for dependency injection play.modules.enabled += connectors.metrics.MetricsModule metrics.enabled = true These metrics can optionally be sent to an external database (graphite, ganglia or influxdb) in order to monitor the health of the platform. This feature is disabled by default. metrics { name = default enabled = true rateUnit = SECONDS durationUnit = SECONDS showSamples = false jvm = true logback = true graphite { enabled = false host = \"127.0.0.1\" port = 2003 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS period = 10s } ganglia { enabled = false host = \"127.0.0.1\" port = 8649 mode = UNICAST ttl = 1 version = 3.1 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS tmax = 60 dmax = 0 period = 10s } influx { enabled = false url = \"http://127.0.0.1:8086\" user = root password = root database = thehive retention = default consistency = ALL #tags = { # tag1 = value1 # tag2 = value2 #} period = 10s } } 10. HTTPS # You can enable HTTPS on TheHive application or add a reverse proxy in front of TheHive. The latter solution is recommended. 10.1 HTTPS using a reverse proxy # You can choose any reverse proxy to add SSL on TheHive. Below an example of NGINX configuration: server { listen 443 ssl; server_name thehive.example.com; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; proxy_set_header Connection \"\"; # cf. https://github.com/akka/akka/issues/19542 } } 10.2 HTTPS without reverse proxy # This is not supported. 10.3 Strengthen security # When SSL is enable (with reverse proxy or not), you can configure cookie to be \"secure\" (usable only with HTTPS protocol). This is done by adding play.http.session.secure=true in the application.conf file. You can also enable HSTS . This header must be configured on the SSL termination component. For NGINX, use add_header directive, as show above.","title":"Configuration Guide"},{"location":"thehive/legacy/thehive3/admin/configuration/#configuration-guide","text":"The configuration file of TheHive is /etc/thehive/application.conf by default. This file uses the HOCON format . All configuration parameters should go in this file. You can have a look at the default settings .","title":"Configuration Guide"},{"location":"thehive/legacy/thehive3/admin/configuration/#table-of-contents","text":"1. Database 2. Datastore 3. Authentication 3.1 LDAP/AD 3.2 OAuth2/OpenID Connect 4. Streaming (a.k.a The Flow) 5. Entity size limit 6. Cortex 7. MISP 7.1 Configuration 7.2 Associate a Case Template to Alerts corresponding to MISP events 7.3 Event Filters 7.4 MISP Purpose 8. HTTP Client Configuration 9. Monitoring and Performance Metrics (deprecated) 10. HTTPS 10.1 HTTPS using a reverse proxy 10.2 HTTPS without reverse proxy 10.3 Strengthen security","title":"Table of Contents"},{"location":"thehive/legacy/thehive3/admin/configuration/#1-database","text":"TheHive uses the Elasticsearch search engine to store all persistent data. Elasticsearch is not part of TheHive package. It must be installed and configured as a standalone instance which can be located on the same machine. For more information on how to set up Elasticsearch, please refer to Elasticsearch installation guide . Three settings are required to connect to Elasticsearch: * the base name of the index * the name of the cluster * the address(es) and port(s) of the Elasticsearch instance The Defaults settings are: # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200/\" # Scroll keepalive keepalive = 1m # Size of the page for scroll pagesize = 50 # Number of shards nbshards = 5 # Number of replicas nbreplicas = 1 # Arbitrary settings settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } ## Authentication configuration #search.username = \"\" #search.password = \"\" ## SSL configuration #search.keyStore { # path = \"/path/to/keystore\" # type = \"JKS\" # or PKCS12 # password = \"keystore-password\" #} #search.trustStore { # path = \"/path/to/trustStore\" # type = \"JKS\" # or PKCS12 # password = \"trustStore-password\" #} } If you use a different configuration, modify the parameters accordingly in the application.conf file. If multiple Elasticsearch nodes are used as a cluster, you should add addresses of the master nodes in the url like this: search { uri = http://node1:9200,node2:9200/ ... TheHive uses the http port of Elasticsearch (9200/tcp by default). TheHive versions index schema (mapping) in Elasticsearch. Version numbers are appended to the index base name (the 8th version of the schema uses the index the_hive_8 if search.index = the_hive ). When too many documents are requested to TheHive, it uses the scroll feature: the results are retrieved through pagination. You can specify the size of the page ( search.pagesize ) and how long pages are kept in Elasticsearch (( search.keepalive ) before purging.","title":"1. Database"},{"location":"thehive/legacy/thehive3/admin/configuration/#2-datastore","text":"TheHive stores attachments as Elasticsearch documents. They are split in chunks and each chunk sent to Elasticsearch is identified by the hash of the entire attachment and the associated chunk number. The chunk size ( datastore.chunksize ) can be changed but any change will only affect new attachments. Existing ones won't be changed. An attachment is identified by its hash. The algorithm used is configurable ( datastore.hash.main ) but must not be changed after the first attachment insertion. Otherwise, previous files cannot be retrieved. Extra hash algorithms can be configured using datastore.hash.extra . These hashes are not used to identify the attachment but are shown in the user interface (the hash associated to the main algorithm is also shown). If you change extra algorithms, you should inform TheHive and ask it to recompute all hashes. Please note that the associated API call is currently disabled in Buckfast (v 2.10). It will be reinstated in the next release. Observables can contain malicious data. When you try to download an attachment from an observable (typically a file), it is automatically zipped and the resulting ZIP file is password-protected. The default password is malware but it can be changed with the datastore.attachment.password setting. Default values are: # Datastore datastore { name = data # Size of stored data chunks chunksize = 50k hash { # Main hash algorithm /!\\ Don't change this value main = \"SHA-256\" # Additional hash algorithms (used in attachments) extra = [\"SHA-1\", \"MD5\"] } attachment.password = \"malware\" }","title":"2. Datastore"},{"location":"thehive/legacy/thehive3/admin/configuration/#3-authentication","text":"TheHive supports local, LDAP, Active Directory (AD) or OAuth2/OpenID Connect for authentication. By default, it relies on local credentials stored in Elasticsearch. Authentication methods are stored in the auth.provider parameter, which is multi-valued. When a user logs in, each authentication method is tried in order until one succeeds. If no authentication method works, an error is returned and the user cannot log in. The default values within the configuration file are: auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # services.LocalAuthSrv : passwords are stored in user entity (in Elasticsearch). No configuration is required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key # oauth2 : use OAuth/OIDC to authenticate users. Configuration is under \"auth.oauth2\" and \"auth.sso\" keys provider = [local] # By default, basic authentication is disabled. You can enable it by setting \"method.basic\" to true. #method.basic = true ad { # The Windows domain name in DNS format. This parameter is required if you do not use # 'serverNames' below. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers instead of using 'domainFQDN # above. If this parameter is not set, TheHive uses 'domainFQDN'. #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Windows domain name using short format. This parameter is required. #domainName = \"MYDOMAIN\" # If 'true', use SSL to connect to the domain controller. #useSSL = true } ldap { # The LDAP server name or address. The port can be specified using the 'host:port' # syntax. This parameter is required if you don't use 'serverNames' below. #serverName = \"ldap.mydomain.local:389\" # If you have multiple LDAP servers, use the multi-valued setting 'serverNames' instead. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Account to use to bind to the LDAP server. This parameter is required. #bindDN = \"cn=thehive,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user in the directory server. Please note that {0} is replaced # by the actual user name. This parameter is required. #filter = \"(cn={0})\" # If 'true', use SSL to connect to the LDAP directory server. #useSSL = true } oauth2 { # URL of the authorization server #clientId = \"client-id\" #clientSecret = \"client-secret\" #redirectUri = \"https://my-thehive-instance.example/api/ssoLogin\" #responseType = \"code\" #grantType = \"authorization_code\" # URL from where to get the access token #authorizationUrl = \"https://auth-site.com/OAuth/Authorize\" #tokenUrl = \"https://auth-site.com/OAuth/Token\" # The endpoint from which to obtain user details using the OAuth token, after successful login #userUrl = \"https://auth-site.com/api/User\" #scope = [\"openid profile\"] } # Single-Sign On sso { # Autocreate user in database? #autocreate = false # Autoupdate its profile and roles? #autoupdate = false # Autologin user using SSO? #autologin = false # Attributes mappings #attributes { # login = \"sub\" # name = \"name\" # groups = \"groups\" # #roles = \"roles\" #} # Name of mapping class from user resource to backend user ('simple' or 'group') #mapper = group # Default roles for users with no groups mapped (\"read\", \"write\", \"admin\") #defaultRoles = [] #groups { # # URL to retrieve groups (leave empty if you are using OIDC) # #url = \"https://auth-site.com/api/Groups\" # # Group mappings, you can have multiple roles for each group: they are merged # mappings { # admin-profile-name = [\"admin\"] # editor-profile-name = [\"write\"] # reader-profile-name = [\"read\"] # } #} } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h }","title":"3. Authentication"},{"location":"thehive/legacy/thehive3/admin/configuration/#31-ldapad","text":"To enable authentication using AD or LDAP, edit the application.conf file and supply the values for your environment. Then you need to create an account on TheHive for each AD or LDAP user in Administration > Users page (which can only be accessed by an administrator). This is required as TheHive needs to look up the role associated with the user and that role is stored locally by TheHive. Obviously, you don't need to supply a password as TheHive will check the credentials against the remote directory. In order to use SSL on LDAP or AD, TheHive must be able to validate remote certificates. To that end, the Java truststore must contain certificate authorities used to generate the AD and/or LDAP certificates. The Default JVM truststore contains the main official authorities but LDAP and AD certificates are probably not issued by them. Use keytool to create the truststore: keytool -import -file /path/to/your/ca.cert -alias InternalCA -keystore /path/to/your/truststore.jks Then add -Djavax.net.ssl.trustStore=/path/to/your/truststore.jks parameter when you start TheHive or put it in the JAVA_OPTS environment variable before starting TheHive.","title":"3.1. LDAP/AD"},{"location":"thehive/legacy/thehive3/admin/configuration/#32-oauth2openid-connect","text":"To enable authentication using OAuth2/OpenID Connect, edit the application.conf file and supply the values of auth.oauth2 according to your environment. In addition, you need to supply: auth.sso.attributes.login : name of the attribute containing the OAuth2 user's login in retreived user info (mandatory) auth.sso.attributes.name : name of the attribute containing the OAuth2 user's name in retreived user info (mandatory) auth.sso.attributes.groups : name of the attribute containing the OAuth2 user's groups (mandatory using groups mappings) auth.sso.attributes.roles : name of the attribute containing the OAuth2 user's roles in retreived user info (mandatory using simple mapping)","title":"3.2. OAuth2/OpenID Connect"},{"location":"thehive/legacy/thehive3/admin/configuration/#important-notes","text":"Authenticate the user using an external OAuth2 authenticator server. The configuration is: clientId (string) client ID in the OAuth2 server. clientSecret (string) client secret in the OAuth2 server. redirectUri (string) the url of TheHive AOuth2 page (.../api/ssoLogin). responseType (string) type of the response. Currently only \"code\" is accepted. grantType (string) type of the grant. Currently only \"authorization_code\" is accepted. authorizationUrl (string) the url of the OAuth2 server. authorizationHeader (string) prefix of the authorization header to get user info: Bearer, token, ... tokenUrl (string) the token url of the OAuth2 server. userUrl (string) the url to get user information in OAuth2 server. scope (list of string) list of scope.","title":"Important notes"},{"location":"thehive/legacy/thehive3/admin/configuration/#example","text":"auth { provider = [local, oauth2] [..] sso { autocreate: false autoupdate: false mapper: \"simple\" attributes { login: \"login\" name: \"name\" roles: \"role\" } defaultRoles: [\"read\", \"write\"] defaultOrganization: \"demo\" } oauth2 { name: oauth2 clientId: \"Client_ID\" clientSecret: \"Client_ID\" redirectUri: \"http://localhost:9000/api/ssoLogin\" responseType: code grantType: \"authorization_code\" authorizationUrl: \"https://github.com/login/oauth/authorize\" authorizationHeader: \"token\" tokenUrl: \"https://github.com/login/oauth/access_token\" userUrl: \"https://api.github.com/user\" scope: [\"user\"] } [..] }","title":"Example"},{"location":"thehive/legacy/thehive3/admin/configuration/#321-roles-mappings","text":"You can choose a roles mapping with the auth.sso.mapper parameter. The available options are simple and group : Using simple mapping, we assume that the user info retrieved from auth.oauth2.userUrl contains the roles associated to the OAuth2 user. They can be: read , write or admin . Using groups mappings, we assume the retrieved user info contains groups that have to be associated to internal roles. In that case, you have to define mapppings in auth.sso.groups.mappings . If a user has multiple groups, mapped roles are merged. If you need to retreive groups from another endpoint that the one used for user info, you can provide it in auth.sso.groups.url . The retrieved groups can be a valid JSON array or a string listing them: { \"sub\": \"userid1\", \"name\": \"User name 1\", \"groups\": [\"admin-profile-name\", \"reader-profile-name\"] } OR { \"sub\": \"userid2\", \"name\": \"User name 2\", \"groups\": \"[admin-profile-name, reader-profile-name, \\\"another profile\\\", 'a last group']\" } OR { \"sub\": \"userid3\", \"name\": \"User name 3\", \"groups\": \"the-only-group-of-the-user\" } Finally, you can setup default roles associated with user with no roles/groups retrieved, using the auth.sso.defaultRoles parameter.","title":"3.2.1. Roles mappings"},{"location":"thehive/legacy/thehive3/admin/configuration/#322-user-autocreation-autoupdate-and-autologin","text":"The main advantage of OAuth2/OpenID Connect authentication is you won't need to create an account on TheHive for each OAuth2 user if you set the config parameter auth.sso.autocreate to true . However, by default, OAuth2 users won't be updated on SSO login unless you set auth.sso.autoupdate to true . If you set this last parameter, roles and name will be fetched from retrieved user info and will be updated in local database on each login of the user. With auth.sso.autologin set to true , each user connecting to TheHive will automatically be redirected to auth.oauth2.authorizationUrl . The only way to authenticate in TheHive using a local user will be either: Connecting to TheHive using https://my-hive-instance.com/index.html#!/login?code=BAD_CODE . You will get an Authentication Failure but will then be able to authenticate. Connecting to TheHive using a real OAuth2 account, then disconnect. On disconnection, you won't be redirected to authorization URL.","title":"3.2.2. User autocreation, autoupdate and autologin"},{"location":"thehive/legacy/thehive3/admin/configuration/#323-debugging","text":"To debug the OAuth2 feature, you can uncomment the following lines in /etc/thehive/logback.xml : <!-- Uncomment the next lines to log debug information for OAuth/OIDC login --> <logger name=\"org.elastic4play.services.auth\" level=\"DEBUG\" /> <logger name=\"services.OAuth2Srv\" level=\"DEBUG\" /> <logger name=\"services.mappers\" level=\"DEBUG\" />","title":"3.2.3. Debugging"},{"location":"thehive/legacy/thehive3/admin/configuration/#4-streaming-aka-the-flow","text":"The user interface is automatically updated when data is changed in the back-end. To do this, the back-end sends events to all the connected front-ends. The mechanism used to notify the front-end is called long polling and its settings are: refresh : when there is no notification, close the connection after this duration (the default is 1 minute). cache : before polling a session must be created, in order to make sure no event is lost between two polls. If there is no poll during the cache setting, the session is destroyed (the default is 15 minutes). nextItemMaxWait , globalMaxWait : when an event occurs, it is not immediately sent to the front-ends. The back-end waits nextItemMaxWait and up to globalMaxWait in case another event can be included in the notification. This mechanism saves many HTTP requests. Default values are: # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s }","title":"4. Streaming (a.k.a The Flow)"},{"location":"thehive/legacy/thehive3/admin/configuration/#5-entity-size-limit","text":"The Play framework used by TheHive sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. This could be too small in most cases so you may want to change it with the following settings in the application.conf file: # Max textual content length play.http.parser.maxMemoryBuffer=1M # Max file size play.http.parser.maxDiskBuffer=1G Note : if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"5. Entity size limit"},{"location":"thehive/legacy/thehive3/admin/configuration/#6-cortex","text":"TheHive can use one or several Cortex analysis engines to get additional information on observables. When configured, analyzers available in Cortex become usable on TheHive. First you must enable CortexConnector , choose an identifier then specify the URL for each Cortex server: ## Enable the Cortex module play.modules.enabled += connectors.cortex.CortexConnector cortex { \"CORTEX-SERVER-ID\" { # URL of the Cortex server url = \"http://CORTEX_SERVER:CORTEX_PORT\" # Key of the Cortex user, mandatory for Cortex 2 key = \"API key\" } # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # Check job update time interval refreshDelay = 1 minute # Maximum number of successive errors before give up maxRetryOnError = 3 # Check remote Cortex status time interval statusCheckInterval = 1 minute } If you connect TheHive with Cortex 2, you must create a user in Cortex with the read, analyze roles, set an API key and add this key in the Cortex server definition in TheHive application.conf . For Cortex 1, authentication is not required, the key is not used. To create a user with the read, analyze role in Cortex 2, you must have at least one organization configured then you can connect to the Cortex 2 Web UI using a orgAdmin account for that organization to create the user and generate their API key. Please refer to the Cortex Quick Start Guide for more information. Cortex analyzes observables and outputs reports in JSON format. TheHive shows the report as-is by default. In order to make reports more readable, we provide report templates which are in a separate package and must be installed manually: - download the report template package from https://dl.bintray.com/thehive-project/binary/report-templates.zip - log in TheHive using an administrator account - go to Admin > Report templates menu - click on Import templates button and select the downloaded package HTTP client used by Cortex connector use global configuration (in play.ws ) but can be overridden in Cortex section and in each Cortex server configuration. Refer to section 8 for more detail on how to configure HTTP client.","title":"6. Cortex"},{"location":"thehive/legacy/thehive3/admin/configuration/#7-misp","text":"TheHive has the ability to connect to one or several MISP instances in order to import and export events. Hence TheHive is able to: receive events as they are added or updated from multiple MISP instances. These events will appear within the Alerts pane. export cases as MISP events to one or several MISP instances. The exported cases will not be published automatically though as they need to be reviewed prior to publishing. We strongly advise you to review the categories and types of attributes at least, before publishing the corresponding MISP events. Note : Please note that only and all the observables marked as IOCs will be used to create the MISP event. Any other observable will not be shared. This is not configurable. Within the configuration file, you can register your MISP server(s) under the misp configuration keyword. Each server shall be identified using an arbitrary name, its url , the corresponding authentication key and optional tags to add each observable created from a MISP event. Any registered server will be used to import events as alerts. It can also be used to export cases to as MISP events, if the account used by TheHive on the MISP instance has sufficient rights. This means that TheHive can import events from configured MISP servers and export cases to the same configured MISP servers. Having different configuration for sources and destination servers is expected in a future version.","title":"7. MISP"},{"location":"thehive/legacy/thehive3/admin/configuration/#important-notes_1","text":"TheHive requires MISP 2.4.73 or better . Make sure that your are using a compatible version of MISP before reporting problems. MISP 2.4.72 and below do not work correctly with TheHive.","title":"Important Notes"},{"location":"thehive/legacy/thehive3/admin/configuration/#71-configuration","text":"To sync with a MISP server and retrieve events or export cases, edit the application.conf file and adjust the example shown below to your setup: ## Enable the MISP module (import and export) play.modules.enabled += connectors.misp.MispConnector misp { \"MISP-SERVER-ID\" { # URL of the MISP instance. url = \"<The_URL_of_the_MISP_Server_goes_here>\" # Authentication key. key = \"<the_auth_key_goes_here>\" # Name of the case template created in TheHive that shall be used to import # MISP events as cases by default. caseTemplate = \"<Template_Name_goes_here>\" # Tags to add to each observable imported from an event available on # this instance. tags = [\"misp-server-id\"] # Truststore to use to validate the X.509 certificate of the MISP # instance if the default truststore is not sufficient. #ws.ssl.trustManager.stores = [ #{ # type: \"JKS\" # path: \"/path/to/truststore.jks\" #} #] # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } whitelist.tags = [\"whitelist-tag1\", \"whitelist-tag2\"] # MISP purpose defines if this instance can be used to import events (ImportOnly), export cases (ExportOnly) or both (ImportAndExport) # Default is ImportAndExport purpose = ImportAndExport } # Check remote TheHive status time interval statusCheckInterval = 1 minute # Interval between consecutive MISP event imports in hours (h) or # minutes (m). interval = 1h } The HTTP client used by the MISP connector uses a global configuration (in play.ws ) but it can be overridden within the MISP section of the configuation file and/or in the configuration section of each MISP server (in misp.MISP-SERVER-ID.ws ). Refer to section 8 for more details on how to configure the HTTP client.","title":"7.1 Configuration"},{"location":"thehive/legacy/thehive3/admin/configuration/#72-associate-a-case-template-to-alerts-corresponding-to-misp-events","text":"As stated in the subsection above, TheHive is able to automatically import MISP events (they will appear as alerts within the Alerts pane) and create cases out of them. This operation leverages the template engine. Thus you'll need to create a case template prior to importing MISP events. First, create a case template. Let's call it MISP-EVENT . Then update TheHive's configuration to add a 'caseTemplate' parameter as shown in the example below: misp { \"MISP-SERVER-ID\" { # URL of the MISP server url = \"<The_URL_of_the_MISP_Server_goes_here>\" # authentication key key = \"<the_auth_key_goes_here>\" # tags that must be automatically added to the case corresponding to the imported event tags = [\"misp\"] # case template caseTemplate = \"MISP-EVENT\" } Once the configuration file has been edited, restart TheHive. Every new import of a MISP event will generate a case using to the MISP-EVENT template by default. The template can be overridden though during the event import. MISP events will only be imported by TheHive if they have at least one attribute and were published.","title":"7.2 Associate a Case Template to Alerts corresponding to MISP events"},{"location":"thehive/legacy/thehive3/admin/configuration/#73-event-filters","text":"When you first connect TheHive to a MISP instance, you can be overwhelmed by the number of alerts that will be generated, particularly if the MISP instance contains a lot of events. Indeed, every event, even those that date back to the beginning of the Internet, will generate an alert. To avoid alert fatigue, and starting from TheHive 3.0.4 (Cerana 0.4), you can exclude MISP events using different filters: the maximum number of attributes (max-attributes) the maximum size of the event's JSON message (max-size) the maximum age of the last publication (max-age) the organisation is black-listed (exclusion.organisation) one of the tags is black-listed (exclusion.tags) doesn't contain one of the whitelist tag (whitelist.tags) Please note that MISP event filters can be adapted to the configuration associated to each MISP server TheHive is connected with. As regards the max-age filter, it applies to the publication date of MISP events and not to the creation date. In the example below, the following MISP events won't generate alerts in TheHive: events that have more than 1000 attributes events which JSON message size is greater than 1MB events that have been published more than one week from the current date events that have been created by bad organisation or other orga events that contain tag1 or tag2 # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } Of course, you can omit some of the filters or all of them.","title":"7.3 Event Filters"},{"location":"thehive/legacy/thehive3/admin/configuration/#74-misp-purpose","text":"TheHive can interact with MISP in two ways: import a MISP event to create a case in TheHive and export a TheHive case to create a MISP event. By default, any MISP instance that is added to TheHive's configuration will be used for importing events and exporting cases ( ImportAndExport ). If you want to use MISP in only one way, you can set its purpose in the configuration as ImportOnly or ExportOnly . Starting from TheHive 3.3, when exporting a case to a MISP instance, you can export all its tags to the freshly created MISP event. This behaviour is not enabled by default. If you want to enable it you must set the exportCaseTags variable to true as shown below: misp { \"local\" { url = \"http://127.0.0.1\" key = \"<the_auth_key_goes_here>\" exportCaseTags = true [...] # additional parameters go here } }","title":"7.4 MISP Purpose"},{"location":"thehive/legacy/thehive3/admin/configuration/#8-http-client-configuration","text":"HTTP client can be configured by adding ws key in sections that needs to connect to remote HTTP service. The key can contain configuration items defined in play WS configuration : ws.followRedirects : Configures the client to follow 301 and 302 redirects (default is true). ws.useragent : To configure the User-Agent header field. ws.compressionEnabled : Set it to true to use gzip/deflater encoding (default is false).","title":"8. HTTP Client Configuration"},{"location":"thehive/legacy/thehive3/admin/configuration/#timeouts","text":"There are 3 different timeouts in WS. Reaching a timeout causes the WS request to interrupt. - ws.timeout.connection : The maximum time to wait when connecting to the remote host (default is 120 seconds). - ws.timeout.idle : The maximum time the request can stay idle (connection is established but waiting for more data) (default is 120 seconds). - ws.timeout.request : The total time you accept a request to take (it will be interrupted even if the remote host is still sending data) (default is 120 seconds).","title":"Timeouts"},{"location":"thehive/legacy/thehive3/admin/configuration/#proxy","text":"Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. - ws.useProxyProperties : To use the JVM system\u2019s HTTP proxy settings (http.proxyHost, http.proxyPort) (default is true). This setting is ignored if ws.proxy settings is present. - ws.proxy.host : The hostname of the proxy server. - ws.proxy.post : The port of the proxy server. - ws.proxy.protocol : The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. - ws.proxy.user : The username of the credentials for the proxy server. - ws.proxy.password : The password for the credentials for the proxy server. - ws.proxy.ntlmDomain : The password for the credentials for the proxy server. - ws.proxy.encoding : The realm's charset. - ws.proxy.nonProxyHosts : The list of hosts on which proxy must not be used.","title":"Proxy"},{"location":"thehive/legacy/thehive3/admin/configuration/#ssl","text":"SSL of HTTP client can be completely configured in application.conf file.","title":"SSL"},{"location":"thehive/legacy/thehive3/admin/configuration/#certificate-manager","text":"Certificate manager is used to store client certificates and certificate authorities. keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) ws.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] } Certificate authorities are configured using trustManager key. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. ws.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] }","title":"Certificate manager"},{"location":"thehive/legacy/thehive3/admin/configuration/#debugging","text":"To debug the key manager / trust manager, set the following flags: ws.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true }","title":"Debugging"},{"location":"thehive/legacy/thehive3/admin/configuration/#protocols","text":"If you want to define a different default protocol, you can set it specifically in the client: ws.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"]","title":"Protocols"},{"location":"thehive/legacy/thehive3/admin/configuration/#ciphers","text":"Cipher suites can be configured using ws.ssl.enabledCipherSuites : ws.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ]","title":"Ciphers"},{"location":"thehive/legacy/thehive3/admin/configuration/#9-monitoring-and-performance-metrics-deprecated","text":"Performance metrics (response time, call rate to Elasticsearch and HTTP request, throughput, memory used...) can be collected if enabled in configuration. Enable it by editing the application.conf file, and add: # Register module for dependency injection play.modules.enabled += connectors.metrics.MetricsModule metrics.enabled = true These metrics can optionally be sent to an external database (graphite, ganglia or influxdb) in order to monitor the health of the platform. This feature is disabled by default. metrics { name = default enabled = true rateUnit = SECONDS durationUnit = SECONDS showSamples = false jvm = true logback = true graphite { enabled = false host = \"127.0.0.1\" port = 2003 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS period = 10s } ganglia { enabled = false host = \"127.0.0.1\" port = 8649 mode = UNICAST ttl = 1 version = 3.1 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS tmax = 60 dmax = 0 period = 10s } influx { enabled = false url = \"http://127.0.0.1:8086\" user = root password = root database = thehive retention = default consistency = ALL #tags = { # tag1 = value1 # tag2 = value2 #} period = 10s } }","title":"9. Monitoring and Performance Metrics (deprecated)"},{"location":"thehive/legacy/thehive3/admin/configuration/#10-https","text":"You can enable HTTPS on TheHive application or add a reverse proxy in front of TheHive. The latter solution is recommended.","title":"10. HTTPS"},{"location":"thehive/legacy/thehive3/admin/configuration/#101-https-using-a-reverse-proxy","text":"You can choose any reverse proxy to add SSL on TheHive. Below an example of NGINX configuration: server { listen 443 ssl; server_name thehive.example.com; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; proxy_set_header Connection \"\"; # cf. https://github.com/akka/akka/issues/19542 } }","title":"10.1 HTTPS using a reverse proxy"},{"location":"thehive/legacy/thehive3/admin/configuration/#102-https-without-reverse-proxy","text":"This is not supported.","title":"10.2 HTTPS without reverse proxy"},{"location":"thehive/legacy/thehive3/admin/configuration/#103-strengthen-security","text":"When SSL is enable (with reverse proxy or not), you can configure cookie to be \"secure\" (usable only with HTTPS protocol). This is done by adding play.http.session.secure=true in the application.conf file. You can also enable HSTS . This header must be configured on the SSL termination component. For NGINX, use add_header directive, as show above.","title":"10.3 Strengthen security"},{"location":"thehive/legacy/thehive3/admin/default-configuration/","text":"You can find the default configuration settings of TheHive below: # maximum number of similar cases maxSimilarCases = 100 # ElasticSearch search { # Name of the index index = the_hive # Name of the ElasticSearch cluster cluster = hive # Address of the ElasticSearch instance host = [\"127.0.0.1:9300\"] # Scroll keepalive keepalive = 1m # Size of the page for scroll pagesize = 50 # Arbitrary settings settings { # Maximum number of nested fields mapping.nested_fields.limit = 50 } } # Datastore datastore { # Size of stored data chunks chunksize = 50k hash { # Main hash algorithm /!\\ Don't change this value main = \"SHA-256\" # Additional hash algorithms (used in attachments) extra = [\"SHA-1\", \"MD5\"] } attachment.password = \"malware\" } auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # local : passwords are stored in user entity (in ElasticSearch). No configuration are required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key provider = [local] ad { # The name of the Microsoft Windows domaine using the DNS format. This parameter is required. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers. If not set, TheHive uses \"domainFQDN\". #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Microsoft Windows domain name using the short format. This parameter is required. #domainName = \"MYDOMAIN\" # Use SSL to connect to the domain controller(s). #useSSL = true } ldap { # LDAP server name or address. Port can be specified (host:port). This parameter is required. #serverName = \"ldap.mydomain.local:389\" # If you have multiple ldap servers, use the multi-valued settings. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Use SSL to connect to directory server #useSSL = true # Account to use to bind on LDAP server. This parameter is required. #bindDN = \"cn=thehive,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user {0} is replaced by user name. This parameter is required. #filter = \"(cn={0})\" } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h } # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s } # Cortex configuration ######## cortex { #\"CORTEX-SERVER-ID\" { # # URL of MISP server # url = \"\" # #HTTP client configuration, more details in section 8 # ws { # ws.useProxyProperties = true # proxy { # # The hostname of the proxy server. # #host = \"\" # # The port of the proxy server. # #post = 0 # # The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. # #protocol = \"http\" # # The username of the credentials for the proxy server. # #user = \"\" # # The password for the credentials for the proxy server. # #password = \"\" # # The password for the credentials for the proxy server. # #ntlmDomain = \"\" # # The realm's charset. # #encoding = \"\" # # The list of host on which proxy must not be used. # #nonProxyHosts = \"\" # } # ssl { # keyManager { # used for client certificate authentication # stores = [{ # type: \"pkcs12\" // JKS or PEM # path: \"mycert.p12\" # password: \"password1\" # }] # } # # Add certificate authorities to trust remote certificate # trustManager { # stores = [{ # type: \"JKS\" // JKS or PEM # path: \"keystore.jks\" # password: \"password1\" # }] # } # debug = { # ssl = false # trustmanager = false # keymanager = false # sslctx = false # handshake = false # verbose = false # data = false # certpath = false # } # # # default SSL protocol # #protocol = \"TLSv1.2\" # # # list of enabled SSL protocols # #ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] # # # SSL Cipher suite # #enabledCipherSuites = [ # # \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", # # \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", # #] # } # } #} } # MISP configuration ######## misp { #\"MISP-SERVER-ID\" { # # URL of MISP server # url = \"\" # # authentication key # key = \"\" # #tags to be added to imported artifact # tags = [\"misp\"] # # # filters: # # the maximum number of attributes (max-attributes) # #max-attributes = 1000 # # the maximum size of the event json message # #max-size = 1 MiB # # the age of the last publication # #max-age = 7 days # exclusion { # # the organisation is black-listed # #organisation = [\"bad organisation\", \"other orga\"] # # one of the tags is black-listed # #tags = [\"tag1\", \"tag2\"] # } # # ws { # ws.useProxyProperties = true # proxy { # # The hostname of the proxy server. # #host = \"\" # # The port of the proxy server. # #post = 0 # # The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. # #protocol = \"http\" # # The username of the credentials for the proxy server. # #user = \"\" # # The password for the credentials for the proxy server. # #password = \"\" # # The password for the credentials for the proxy server. # #ntlmDomain = \"\" # # The realm's charset. # #encoding = \"\" # # The list of host on which proxy must not be used. # #nonProxyHosts = \"\" # } # # ssl { # keyManager { # used for client certificate authentication # stores = [{ # type: \"pkcs12\" // JKS or PEM # path: \"mycert.p12\" # password: \"password1\" # }] # } # # Add certificate authorities to trust remote certificate # trustManager { # stores = [{ # type: \"JKS\" // JKS or PEM # path: \"keystore.jks\" # password: \"password1\" # }] # } # debug = { # ssl = false # trustmanager = false # keymanager = false # sslctx = false # handshake = false # verbose = false # data = false # certpath = false # } # # # default SSL protocol # #protocol = \"TLSv1.2\" # # # list of enabled SSL protocols # #ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] # # # SSL Cipher suite # #enabledCipherSuites = [ # # \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", # # \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", # #] # } # } #} # Interval between two MISP event import interval = 1h } # Metrics configuration ######## metrics { name = default enabled = false rateUnit = SECONDS durationUnit = SECONDS jvm = true logback = true graphite { enabled = false host = \"127.0.0.1\" port = 2003 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS period = 10s } ganglia { enabled = false host = \"127.0.0.1\" port = 8649 mode = UNICAST ttl = 1 version = 3.1 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS tmax = 60 dmax = 0 period = 10s } influx { enabled = false url = \"http://127.0.0.1:8086\" user = root password = root database = thehive retention = default consistency = ALL #tags = { # tag1 = value1 # tag2 = value2 #} period = 10s } }","title":"Default configuration"},{"location":"thehive/legacy/thehive3/admin/schema_version/","text":"Schema version # The data of TheHive is stored in an ElasticSearch index. The name of the index is suffixed by the revision of the schema. When the schema of TheHive database changes, a new one is created and the version is incremented. By default, index base name is \"the_hive\" but can be configured ( index.index in application.conf). The following table show for each version of TheHive the default name of the index: TheHive version Index name 2.9.1 the_hive_7 2.9.2 the_hive_7 2.10.0 the_hive_8 2.10.1 the_hive_8 2.10.2 the_hive_8 2.11.0 the_hive_9 2.11.1 the_hive_9 2.11.2 the_hive_9 2.11.3 the_hive_9 2.12.0 the_hive_10 2.12.1 the_hive_10 2.13.0 the_hive_10 2.13.1 the_hive_10 2.13.2 the_hive_11 3.0.0 the_hive_12 3.0.1 the_hive_12 3.0.2 the_hive_12 3.0.3 the_hive_12 3.0.4 the_hive_13","title":"Schema version"},{"location":"thehive/legacy/thehive3/admin/schema_version/#schema-version","text":"The data of TheHive is stored in an ElasticSearch index. The name of the index is suffixed by the revision of the schema. When the schema of TheHive database changes, a new one is created and the version is incremented. By default, index base name is \"the_hive\" but can be configured ( index.index in application.conf). The following table show for each version of TheHive the default name of the index: TheHive version Index name 2.9.1 the_hive_7 2.9.2 the_hive_7 2.10.0 the_hive_8 2.10.1 the_hive_8 2.10.2 the_hive_8 2.11.0 the_hive_9 2.11.1 the_hive_9 2.11.2 the_hive_9 2.11.3 the_hive_9 2.12.0 the_hive_10 2.12.1 the_hive_10 2.13.0 the_hive_10 2.13.1 the_hive_10 2.13.2 the_hive_11 3.0.0 the_hive_12 3.0.1 the_hive_12 3.0.2 the_hive_12 3.0.3 the_hive_12 3.0.4 the_hive_13","title":"Schema version"},{"location":"thehive/legacy/thehive3/admin/updating/","text":"Update TheHive # TheHive is simple to update. You only need to replace your current package files by new ones. If the schema of the data changes between the two versions, the first request to the application asks the user to start a data migration. In this case, authentication is not required. This process creates a new index in ElasticSearch (suffixed by the version of the schema) and copies all the data on it (before adapting its format). It is always possible to rollback to the previous version but all modifications done on the new version will be lost.","title":"Update TheHive"},{"location":"thehive/legacy/thehive3/admin/updating/#update-thehive","text":"TheHive is simple to update. You only need to replace your current package files by new ones. If the schema of the data changes between the two versions, the first request to the application asks the user to start a data migration. In this case, authentication is not required. This process creates a new index in ElasticSearch (suffixed by the version of the schema) and copies all the data on it (before adapting its format). It is always possible to rollback to the previous version but all modifications done on the new version will be lost.","title":"Update TheHive"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/","text":"This document is related to upgrading TheHive and Elasticsearch on Ubuntu 16.04 . Upgrade TheHive to version 3.4 # Perform a backup of data # curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/opt/backup\", \"compress\": true } }' Next: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"<INDEX>\" }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/the_hive_152019060701_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"the_hive_15\" }' Output example: { \"snapshot\" : { \"snapshot\" : \"the_hive_152019060701_1\" , \"uuid\" : \"ZKhBL2BHTAS2g71Xby2OgQ\" , \"version_id\" : 5061699 , \"version\" : \"5.6.16\" , \"indices\" : [ \"the_hive_15\" ], \"state\" : \"SUCCESS\" , \"start_time\" : \"2019-06-07T13:07:38.844Z\" , \"start_time_in_millis\" : 1559912858844 , \"end_time\" : \"2019-06-07T13:07:40.640Z\" , \"end_time_in_millis\" : 1559912860640 , \"duration_in_millis\" : 1796 , \"failures\" : [ ], \"shards\" : { \"total\" : 5 , \"failed\" : 0 , \"successful\" : 5 } } } You can find more information about backup and restore in the dedicated documentation . Stop TheHive service # service thehive stop Update TheHive configuration # Current /etc/thehive/application.conf # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch cluster name. cluster = hive # ElasticSearch instance address. #host = [\"127.0.0.1:9300\"] [..] } New /etc/thehive/application.conf : Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200\" [] } cluster { name = hive } Upgrade TheHive # apt upgrade thehive Restart TheHive service # Ensure everything is working. Upgrade Elasticsearch from version 5.x to 6.x # This is greatly inspired by the official documentation : https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html with additional info we had to set up to make everything work. Upgrading from earlier 5.x versions requires a full cluster restart . Disable shard allocation # curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": \"none\" } } ' Output: {\"acknowledged\":true,\"persistent\":{\"cluster\":{\"routing\":{\"allocation\":{\"enable\":\"none\"}}}},\"transient\":{}} curl -X POST \"localhost:9200/_flush/synced\" Output : {\"_shards\":{\"total\":60,\"successful\":30,\"failed\":0},\"cortex_4\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_2\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_3\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_13\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_15\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_14\":{\"total\":10,\"successful\":5,\"failed\":0}} shut down a single node # sudo -i service elasticsearch stop Upgrade the node you shut down # wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - sudo apt-get install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/6.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list sudo apt-get update && sudo apt-get install elasticsearch Upgrade plugins # /usr/share/elasticsearch/bin/elasticsearch-plugin list ## for all plugin: /usr/share/elasticsearch/bin/elasticsearch-plugin install $plugin Update Elasticsearch configuration # Add path.logs and path.data in `/etc/elasticsearch/elasticsearch.yml: http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.index.queue_size : 100000 thread_pool.search.queue_size : 100000 thread_pool.bulk.queue_size : 100000 path.repo : [ \"/opt/backup\" ] path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" Set $JAVA_HOME in /etc/default/elasticsearch for example: [..] JAVA_HOME=/usr/lib/jvm/java-8-oracle/ [..] On Ubuntu 16.04 we had to set read persmissions manually to this file: chmod o+r /etc/default/elasticsearch Restart the node # sudo update-rc.d elasticsearch defaults 95 10 sudo -i service elasticsearch start Check that elasticsearch is running # curl -X GET \"localhost:9200/\" curl -X GET \"localhost:9200/_cat/nodes\" Reenable shard allocation # Once the node has joined the cluster: curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": null } } ' Output: {\"acknowledged\":true,\"persistent\":{},\"transient\":{}} Wait for the node to recover # Before upgrading the next node, wait for the cluster to finish shard allocation. You can check progress by submitting a _cat/health request: curl -X GET \"localhost:9200/_cat/health?v\" Resources # https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/restart-upgrade.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/deb.html","title":"Upgrade to thehive 3 4 and es 6 x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-thehive-to-version-34","text":"","title":"Upgrade TheHive to version 3.4"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#perform-a-backup-of-data","text":"curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/opt/backup\", \"compress\": true } }' Next: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"<INDEX>\" }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/the_hive_152019060701_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"the_hive_15\" }' Output example: { \"snapshot\" : { \"snapshot\" : \"the_hive_152019060701_1\" , \"uuid\" : \"ZKhBL2BHTAS2g71Xby2OgQ\" , \"version_id\" : 5061699 , \"version\" : \"5.6.16\" , \"indices\" : [ \"the_hive_15\" ], \"state\" : \"SUCCESS\" , \"start_time\" : \"2019-06-07T13:07:38.844Z\" , \"start_time_in_millis\" : 1559912858844 , \"end_time\" : \"2019-06-07T13:07:40.640Z\" , \"end_time_in_millis\" : 1559912860640 , \"duration_in_millis\" : 1796 , \"failures\" : [ ], \"shards\" : { \"total\" : 5 , \"failed\" : 0 , \"successful\" : 5 } } } You can find more information about backup and restore in the dedicated documentation .","title":"Perform a backup of data"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#stop-thehive-service","text":"service thehive stop","title":"Stop TheHive service"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#update-thehive-configuration","text":"Current /etc/thehive/application.conf # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch cluster name. cluster = hive # ElasticSearch instance address. #host = [\"127.0.0.1:9300\"] [..] } New /etc/thehive/application.conf : Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200\" [] } cluster { name = hive }","title":"Update TheHive configuration"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-thehive","text":"apt upgrade thehive","title":"Upgrade TheHive"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#restart-thehive-service","text":"Ensure everything is working.","title":"Restart TheHive service"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-elasticsearch-from-version-5x-to-6x","text":"This is greatly inspired by the official documentation : https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html with additional info we had to set up to make everything work. Upgrading from earlier 5.x versions requires a full cluster restart .","title":"Upgrade Elasticsearch from version 5.x to 6.x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#disable-shard-allocation","text":"curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": \"none\" } } ' Output: {\"acknowledged\":true,\"persistent\":{\"cluster\":{\"routing\":{\"allocation\":{\"enable\":\"none\"}}}},\"transient\":{}} curl -X POST \"localhost:9200/_flush/synced\" Output : {\"_shards\":{\"total\":60,\"successful\":30,\"failed\":0},\"cortex_4\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_2\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_3\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_13\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_15\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_14\":{\"total\":10,\"successful\":5,\"failed\":0}}","title":"Disable shard allocation"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#shut-down-a-single-node","text":"sudo -i service elasticsearch stop","title":"shut down a single node"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-the-node-you-shut-down","text":"wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - sudo apt-get install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/6.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list sudo apt-get update && sudo apt-get install elasticsearch","title":"Upgrade the node you shut down"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-plugins","text":"/usr/share/elasticsearch/bin/elasticsearch-plugin list ## for all plugin: /usr/share/elasticsearch/bin/elasticsearch-plugin install $plugin","title":"Upgrade plugins"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#update-elasticsearch-configuration","text":"Add path.logs and path.data in `/etc/elasticsearch/elasticsearch.yml: http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.index.queue_size : 100000 thread_pool.search.queue_size : 100000 thread_pool.bulk.queue_size : 100000 path.repo : [ \"/opt/backup\" ] path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" Set $JAVA_HOME in /etc/default/elasticsearch for example: [..] JAVA_HOME=/usr/lib/jvm/java-8-oracle/ [..] On Ubuntu 16.04 we had to set read persmissions manually to this file: chmod o+r /etc/default/elasticsearch","title":"Update Elasticsearch configuration"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#restart-the-node","text":"sudo update-rc.d elasticsearch defaults 95 10 sudo -i service elasticsearch start","title":"Restart the node"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#check-that-elasticsearch-is-running","text":"curl -X GET \"localhost:9200/\" curl -X GET \"localhost:9200/_cat/nodes\"","title":"Check that elasticsearch is running"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#reenable-shard-allocation","text":"Once the node has joined the cluster: curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": null } } ' Output: {\"acknowledged\":true,\"persistent\":{},\"transient\":{}}","title":"Reenable shard allocation"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#wait-for-the-node-to-recover","text":"Before upgrading the next node, wait for the cluster to finish shard allocation. You can check progress by submitting a _cat/health request: curl -X GET \"localhost:9200/_cat/health?v\"","title":"Wait for the node to recover"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#resources","text":"https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/restart-upgrade.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/deb.html","title":"Resources"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/","text":"Migration from Elasticsearch 6.8.2 to ES 7.x # \u26a0\ufe0f IMPORTANT NOTE This migration process is intended for single node of Elasticsearch database The current version of this document is provided for testing purpose ONLY! This guide has been written and tested to migrate data from ES 6.8.2 to ES 7.8.1, and TheHive 3.4.2 to TheHive 3.5.0-RC1 only! This guide starts with Elasticsearch version 6.8.2 up and running, indexes and data. To test this guide, we recommend using a backup of you production server. (see Backup and Restore page for more information) This guide is illustrated with TheHive index. The process is identical for Cortex, you just have to adjust index names. Prerequisite # The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Identify if your index should be reindexed # You can easily identify if indexes should be reindexed or not. On the index named the_hive_15 run the following command: curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created' if the output is similar to \"5xxxxxx\" then reindexing is required, you should follow this guide. If it is \"6xxxxxx\" then the index can be read by Elasticsearch 7.8.x. Upgrade Elasticsearch, and TheHive-3.5.0. Migration guide # Current status # Current context is: - Elasticsearch 6.8.2 - TheHive 3.4.2 All up and running. Start by identifying indices on you Elasticsearch instance. curl http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb The index name is the_hive_15 . Record this somewhere. Stop services # Before starting updating the database, lets stop applications: sudo service thehive stop Create a new index # The First operation lies in creating a new index named new_the_hive_15 with settings from current index the_hive_15 (ensure to keep index version, needed for future upgrade). curl -XPUT 'http://localhost:9200/new_the_hive_15' \\ -H 'Content-Type: application/json' \\ -d \" $( curl http://localhost:9200/the_hive_15 | \\ jq '.the_hive_15 | del(.settings.index.provided_name, .settings.index.creation_date, .settings.index.uuid, .settings.index.version, .settings.index.mapping.single_type, .mappings.doc._all)' ) \" Check the new index is well created: curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open new_the_hive_15 A2KLoZPpSXygutlfy_RNCQ 5 1 0 0 1.1kb 1.1kb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb Proceed to Reindex # Next operation lies in running the reindex command in the newly created index: curl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_reindex -d '{ \"conflicts\": \"proceed\", \"source\": { \"index\": \"the_hive_15\" }, \"dest\": { \"index\": \"new_the_hive_15\" } }' After a moment, you should get a similar output: { \"took\" : 5119 , \"timed_out\" : false , \"total\" : 5889 , \"updated\" : 0 , \"created\" : 5889 , \"deleted\" : 0 , \"batches\" : 6 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [] } Ensure new index has been created # Run the following command, and ensure the new index is like the current one (size can vary): curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb Delete old indices # This is the thrilling part. Now the new index new_the_hive_15 is created and similar the_hive_15, older indexes should be completely deleted from the database. To delete index named the_hive_15 , run the following command: curl -XDELETE http://localhost:9200/the_hive_15 Run the same command for older indexes if exist (the_hive_14, the_hive_13....). Elasticsearch 7.x cannot run with index created with Elasticsearch 5.x. Create an alias # Before stopping Elasticsearch service, let\u2019s create an alias to keep index names in the future. curl -XPOST -H 'Content-Type: application/json' 'http://localhost:9200/_aliases' -d '{ \"actions\": [ { \"add\": { \"index\": \"new_the_hive_15\", \"alias\": \"the_hive_15\" } } ] }' Doing so will allow TheHive 3.5.0 to find the index without updating the configuration file. Check the alias has been well created by running the following command curl -XGET http://localhost:9200/_alias?pretty The output should look like: { \"new_the_hive_15\" : { \"aliases\" : { \"the_hive_15\" : { } } } } Stop Elasticsearch version 6.8.2 # sudo service elasticsearch stop Update Elasticsearch # Update the configuration of Elastisearch. Configuration file should look like this: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Now, upgrade Elasticsearch to version 7.x following the documentation for your Operating System, and ensure the service start successfully. Install or update to TheHive 3.5.0 # DEB package # If using Debian based Linux operating system, configure it to follow our beta repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then install it by running: sudo apt install thehive or sudo apt install thehive = 3 .5.0-1 RPM # Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=http://rpm.thehive-project.org/release/noarch gpgcheck=1 Then install it by running: sudo yum install thehive or sudo yum install thehive-3.5.0-1 Install binaries # cd /opt wget https://download.thehive-project.org/thehive-3.5.0-1.zip unzip thehive-3.5.0-1.zip ln -s thehive-3.5.0-1 thehive Docker images # Docker images are also provided on Dockerhub. docker pull thehiveproject/thehive:3.5.0-1 Update Database # Connect to TheHive, the maintenance page should ask to update. Once updated, ensure a new index named the_hive_16 has been created. curl -XGET http://localhost:9200/_cat/indices \\? v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb yellow open the_hive_16 Nz0vCKqhRK2xkx1t_WF-0g 5 1 30977 0 26.1mb 26.1mb","title":"Migration from Elasticsearch 6.8.2 to ES 7.x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#migration-from-elasticsearch-682-to-es-7x","text":"\u26a0\ufe0f IMPORTANT NOTE This migration process is intended for single node of Elasticsearch database The current version of this document is provided for testing purpose ONLY! This guide has been written and tested to migrate data from ES 6.8.2 to ES 7.8.1, and TheHive 3.4.2 to TheHive 3.5.0-RC1 only! This guide starts with Elasticsearch version 6.8.2 up and running, indexes and data. To test this guide, we recommend using a backup of you production server. (see Backup and Restore page for more information) This guide is illustrated with TheHive index. The process is identical for Cortex, you just have to adjust index names.","title":"Migration from Elasticsearch 6.8.2 to ES 7.x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#prerequisite","text":"The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ .","title":"Prerequisite"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#identify-if-your-index-should-be-reindexed","text":"You can easily identify if indexes should be reindexed or not. On the index named the_hive_15 run the following command: curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created' if the output is similar to \"5xxxxxx\" then reindexing is required, you should follow this guide. If it is \"6xxxxxx\" then the index can be read by Elasticsearch 7.8.x. Upgrade Elasticsearch, and TheHive-3.5.0.","title":"Identify if your index should be reindexed"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#migration-guide","text":"","title":"Migration guide"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#current-status","text":"Current context is: - Elasticsearch 6.8.2 - TheHive 3.4.2 All up and running. Start by identifying indices on you Elasticsearch instance. curl http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb The index name is the_hive_15 . Record this somewhere.","title":"Current status"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#stop-services","text":"Before starting updating the database, lets stop applications: sudo service thehive stop","title":"Stop services"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#create-a-new-index","text":"The First operation lies in creating a new index named new_the_hive_15 with settings from current index the_hive_15 (ensure to keep index version, needed for future upgrade). curl -XPUT 'http://localhost:9200/new_the_hive_15' \\ -H 'Content-Type: application/json' \\ -d \" $( curl http://localhost:9200/the_hive_15 | \\ jq '.the_hive_15 | del(.settings.index.provided_name, .settings.index.creation_date, .settings.index.uuid, .settings.index.version, .settings.index.mapping.single_type, .mappings.doc._all)' ) \" Check the new index is well created: curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open new_the_hive_15 A2KLoZPpSXygutlfy_RNCQ 5 1 0 0 1.1kb 1.1kb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb","title":"Create a new index"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#proceed-to-reindex","text":"Next operation lies in running the reindex command in the newly created index: curl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_reindex -d '{ \"conflicts\": \"proceed\", \"source\": { \"index\": \"the_hive_15\" }, \"dest\": { \"index\": \"new_the_hive_15\" } }' After a moment, you should get a similar output: { \"took\" : 5119 , \"timed_out\" : false , \"total\" : 5889 , \"updated\" : 0 , \"created\" : 5889 , \"deleted\" : 0 , \"batches\" : 6 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [] }","title":"Proceed to Reindex"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#ensure-new-index-has-been-created","text":"Run the following command, and ensure the new index is like the current one (size can vary): curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb","title":"Ensure new index has been created"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#delete-old-indices","text":"This is the thrilling part. Now the new index new_the_hive_15 is created and similar the_hive_15, older indexes should be completely deleted from the database. To delete index named the_hive_15 , run the following command: curl -XDELETE http://localhost:9200/the_hive_15 Run the same command for older indexes if exist (the_hive_14, the_hive_13....). Elasticsearch 7.x cannot run with index created with Elasticsearch 5.x.","title":"Delete old indices"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#create-an-alias","text":"Before stopping Elasticsearch service, let\u2019s create an alias to keep index names in the future. curl -XPOST -H 'Content-Type: application/json' 'http://localhost:9200/_aliases' -d '{ \"actions\": [ { \"add\": { \"index\": \"new_the_hive_15\", \"alias\": \"the_hive_15\" } } ] }' Doing so will allow TheHive 3.5.0 to find the index without updating the configuration file. Check the alias has been well created by running the following command curl -XGET http://localhost:9200/_alias?pretty The output should look like: { \"new_the_hive_15\" : { \"aliases\" : { \"the_hive_15\" : { } } } }","title":"Create an alias"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#stop-elasticsearch-version-682","text":"sudo service elasticsearch stop","title":"Stop Elasticsearch version 6.8.2"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#update-elasticsearch","text":"Update the configuration of Elastisearch. Configuration file should look like this: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Now, upgrade Elasticsearch to version 7.x following the documentation for your Operating System, and ensure the service start successfully.","title":"Update Elasticsearch"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#install-or-update-to-thehive-350","text":"","title":"Install or update to TheHive 3.5.0"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#deb-package","text":"If using Debian based Linux operating system, configure it to follow our beta repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then install it by running: sudo apt install thehive or sudo apt install thehive = 3 .5.0-1","title":"DEB package"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#rpm","text":"Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=http://rpm.thehive-project.org/release/noarch gpgcheck=1 Then install it by running: sudo yum install thehive or sudo yum install thehive-3.5.0-1","title":"RPM"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#install-binaries","text":"cd /opt wget https://download.thehive-project.org/thehive-3.5.0-1.zip unzip thehive-3.5.0-1.zip ln -s thehive-3.5.0-1 thehive","title":"Install binaries"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#docker-images","text":"Docker images are also provided on Dockerhub. docker pull thehiveproject/thehive:3.5.0-1","title":"Docker images"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#update-database","text":"Connect to TheHive, the maintenance page should ask to update. Once updated, ensure a new index named the_hive_16 has been created. curl -XGET http://localhost:9200/_cat/indices \\? v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb yellow open the_hive_16 Nz0vCKqhRK2xkx1t_WF-0g 5 1 30977 0 26.1mb 26.1mb","title":"Update Database"},{"location":"thehive/legacy/thehive3/admin/webhooks/","text":"WebHooks # Starting from version 2.13, TheHive supports webhooks . When enabled, TheHive will send each action that has been performed on it (add case, update case, add task, ...), in real time, to an HTTP endpoint. You can then create a program or application on the HTTP endpoint to react on specific events. Configuration Data Sent to the HTTP Endpoint Sample Webhook Server Application Dependencies Python Script Run Configuration # Webhooks are configured using the webhook key in the configuration file ( /etc/thehive/application.conf by default). A minimal configuration contains an arbitrary name and an URL. The URL corresponds to the HTTP endpoint: webhooks { myLocalWebHook { url = \"http://my_HTTP_endpoint/webhook\" } } Proxy and SSL configuration can be added in the same manner as for MISP or Cortex: webhooks { securedWebHook { url = \"https://my_HTTP_endpoint/webhook\" ws { ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } proxy { host: \"10.1.0.1\" port: 3128 } } } } Data Sent to the HTTP Endpoint # For each action performed on it, TheHive sends an audit trail entry in JSON format to the HTTP endpoint. Here is an example corresponding to the creation of a case: { \"operation\": \"Creation\", # Creation, Update or Delete \"objectType\": \"case\", # Type of object \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", # Object ID \"startDate\": 1505476659427, # When the operation has been done \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:426\", # HTTP request ID which has done the operation \"details\": { # Attributes used for creation of update \"customFields\": {}, \"metrics\": {}, \"description\": \"Example of case creation\", \"flag\": false, \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [] }, \"base\": true, # Internal information used to determine the main operation when there are several operations for the same request \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", # ID of the root parent of the object (internal use) \"object\": { # The object after the operation \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } } For an update, the data will look like: { \"operation\": \"Update\", \"details\": { \"severity\": 3 }, \"objectType\": \"case\", \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", \"base\": true, \"startDate\": 1505477372601, \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:446\", \"object\": { \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 3, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"updatedBy\": \"me\", \"updatedAt\": 1505477372246, \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } } Sample Webhook Server Application # The following application is a sample intended to help you get started with webhooks. It is very basic as it listens to a local port and displays the contents of the received POST JSON data. Dependencies # Install dependencies: sudo pip install flask Python Script # Create a simple Python script (e.g. webhooktest.py ): from flask import Flask, request import json app = Flask(__name__) @app.route('/',methods=['POST']) def foo(): data = json.loads(request.data) print(json.dumps(data, indent=4)) return \"OK\" if __name__ == '__main__': app.run() Run # Run the server: python webhooktest.py","title":"WebHooks"},{"location":"thehive/legacy/thehive3/admin/webhooks/#webhooks","text":"Starting from version 2.13, TheHive supports webhooks . When enabled, TheHive will send each action that has been performed on it (add case, update case, add task, ...), in real time, to an HTTP endpoint. You can then create a program or application on the HTTP endpoint to react on specific events. Configuration Data Sent to the HTTP Endpoint Sample Webhook Server Application Dependencies Python Script Run","title":"WebHooks"},{"location":"thehive/legacy/thehive3/admin/webhooks/#configuration","text":"Webhooks are configured using the webhook key in the configuration file ( /etc/thehive/application.conf by default). A minimal configuration contains an arbitrary name and an URL. The URL corresponds to the HTTP endpoint: webhooks { myLocalWebHook { url = \"http://my_HTTP_endpoint/webhook\" } } Proxy and SSL configuration can be added in the same manner as for MISP or Cortex: webhooks { securedWebHook { url = \"https://my_HTTP_endpoint/webhook\" ws { ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } proxy { host: \"10.1.0.1\" port: 3128 } } } }","title":"Configuration"},{"location":"thehive/legacy/thehive3/admin/webhooks/#data-sent-to-the-http-endpoint","text":"For each action performed on it, TheHive sends an audit trail entry in JSON format to the HTTP endpoint. Here is an example corresponding to the creation of a case: { \"operation\": \"Creation\", # Creation, Update or Delete \"objectType\": \"case\", # Type of object \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", # Object ID \"startDate\": 1505476659427, # When the operation has been done \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:426\", # HTTP request ID which has done the operation \"details\": { # Attributes used for creation of update \"customFields\": {}, \"metrics\": {}, \"description\": \"Example of case creation\", \"flag\": false, \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [] }, \"base\": true, # Internal information used to determine the main operation when there are several operations for the same request \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", # ID of the root parent of the object (internal use) \"object\": { # The object after the operation \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } } For an update, the data will look like: { \"operation\": \"Update\", \"details\": { \"severity\": 3 }, \"objectType\": \"case\", \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", \"base\": true, \"startDate\": 1505477372601, \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:446\", \"object\": { \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 3, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"updatedBy\": \"me\", \"updatedAt\": 1505477372246, \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } }","title":"Data Sent to the HTTP Endpoint"},{"location":"thehive/legacy/thehive3/admin/webhooks/#sample-webhook-server-application","text":"The following application is a sample intended to help you get started with webhooks. It is very basic as it listens to a local port and displays the contents of the received POST JSON data.","title":"Sample Webhook Server Application"},{"location":"thehive/legacy/thehive3/admin/webhooks/#dependencies","text":"Install dependencies: sudo pip install flask","title":"Dependencies"},{"location":"thehive/legacy/thehive3/admin/webhooks/#python-script","text":"Create a simple Python script (e.g. webhooktest.py ): from flask import Flask, request import json app = Flask(__name__) @app.route('/',methods=['POST']) def foo(): data = json.loads(request.data) print(json.dumps(data, indent=4)) return \"OK\" if __name__ == '__main__': app.run()","title":"Python Script"},{"location":"thehive/legacy/thehive3/admin/webhooks/#run","text":"Run the server: python webhooktest.py","title":"Run"},{"location":"thehive/legacy/thehive3/api/","text":"TheHive API # TheHive exposes REST APIs through JSON over HTTP. HTTP request format Authentication Model Alert Case Observable Task Log User Connectors","title":"TheHive API"},{"location":"thehive/legacy/thehive3/api/#thehive-api","text":"TheHive exposes REST APIs through JSON over HTTP. HTTP request format Authentication Model Alert Case Observable Task Log User Connectors","title":"TheHive API"},{"location":"thehive/legacy/thehive3/api/alert/","text":"Alert # Model definition # Required attributes: - title (text) : title of the alert - description (text) : description of the alert - severity (number) : severity of the alert (1: low; 2: medium; 3: high) default=2 - date (date) : date and time when the alert was raised default=now - tags (multi-string) : case tags default=empty - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - status (AlertStatus) : status of the alert ( New , Updated , Ignored , Imported ) default=New - type (string) : type of the alert (read only) - source (string) : source of the alert (read only) - sourceRef (string) : source reference of the alert (read only) - artifacts (multi-artifact) : artifact of the alert. It is a array of JSON object containing artifact attributes default=empty - follow (boolean) : if true, the alert becomes active when updated default=true Optional attributes: - caseTemplate (string) : case template to use when a case is created from this alert. If the alert specifies a non-existent case template or doesn't supply one, TheHive will import the alert into a case using a case template that has the exact same name as the alert's type if it exists. For example, if you raise an alert with a type value of splunk and you do not provide the caseTemplate attribute or supply a non-existent one (for example splink ), TheHive will import the alert using the case template called splunk if it exists. Otherwise, the alert will be imported using an empty case (i.e. from scratch). Attributes generated by the backend: - lastSyncDate (date) : date of the last synchronization - case (string) : id of the case, if created Alert ID is computed from type , source and sourceRef . Alert Manipulation # Alert methods # HTTP Method URI Action GET /api/alert List alerts POST /api/alert/_search Find alerts PATCH /api/alert/_bulk Update alerts in bulk POST /api/alert/_stats Compute stats on alerts POST /api/alert Create an alert GET /api/alert/:alertId Get an alert PATCH /api/alert/:alertId Update an alert DELETE /api/alert/:alertId Delete an alert POST /api/alert/:alertId/markAsRead Mark an alert as read POST /api/alert/:alertId/markAsUnread Mark an alert as unread POST /api/alert/:alertId/createCase Create a case from an alert POST /api/alert/:alertId/follow Follow an alert POST /api/alert/:alertId/unfollow Unfollow an alert POST /api/alert/:alertId/merge/:caseId Merge an alert in a case POST /api/alert/merge/_bulk Merge several alerts in one case Get an alert # An alert's details can be retrieve using the url: GET /api/alert/:alertId The alert ID is obtained by List alerts or Find alerts API. If the parameter similarity is set to \"1\" or \"true\", this API returns information on cases which have similar observables. With this feature, output will contain the similarCases attribute which list case details with: - artifactCount: number of observables in the original case - iocCount: number of observables marked as IOC in original case - similarArtifactCount: number of observables which are in alert and in case - similarIocCount: number of IOCs which are in alert and in case warning IOCs are observables Examples # Get alert without similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Get alert with similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810?similarity=1 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\", \"similarCases\": [ { \"_id\": \"AVwwrym-Rw5vhyJUfdJW\", \"artifactCount\": 5, \"endDate\": null, \"id\": \"AVwwrym-Rw5vhyJUfdJW\", \"iocCount\": 1, \"resolutionStatus\": null, \"severity\": 1, \"similarArtifactCount\": 2, \"similarIocCount\": 1, \"startDate\": 1495465039000, \"status\": \"Open\", \"tags\": [ \"src:MISP\" ], \"caseId\": 1405, \"title\": \"TEST TheHive\", \"tlp\": 2 } ] } Create an alert # An alert can be created using the following url: POST /api/alert Required case attributes (cf. models) must be provided. If an alert with the same tuple type , source and sourceRef already exists, TheHive will refuse to create it. This call returns attributes of the created alert. Examples # Creation of a simple alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"New Alert\", \"description\": \"N/A\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\" }' It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Creation of another alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"Other alert\", \"description\": \"alert description\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"severity\": 3, \"tlp\": 3, \"artifacts\": [ { \"dataType\": \"ip\", \"data\": \"127.0.0.1\", \"message\": \"localhost\" }, { \"dataType\": \"domain\", \"data\": \"thehive-project.org\", \"tags\": [\"home\", \"TheHive\"] }, { \"dataType\": \"file\", \"data\": \"logo.svg;image/svg+xml;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\", \"message\": \"logo\" } ], \"caseTemplate\": \"external-alert\" }' Merge an alert # An alert can be merge in a case using the URL: POST /api/alert/:alertId/merge/:caseId Each observable of the alert will be added to the case if it doesn't exist in the case. The description of the alert will be appended to the case's description. The HTTP response contains the updated case. Example # Merge the alert ce2c00f17132359cb3c50dfbb1901810 in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810/merge/AVXeF-pZmeHK_2HEYj2z The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script ### Merged with alert #10 my alert title This is my alert description\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" } Bulk merge alert # This API merge several alerts with one case: POST /api/alert/merge/_bulk The observable of each alert listed in alertIds field will be imported into the case (identified by caseId field). The description of the case is not modified. The HTTP response contains the case. Example # Merge the alerts ce2c00f17132359cb3c50dfbb1901810 and a97148693200f731cfa5237ff2edf67b in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert/merge/_bulk -d '{ \"caseId\": \"AVXeF-pZmeHK_2HEYj2z\", \"alertIds\": [\"ce2c00f17132359cb3c50dfbb1901810\", \"a97148693200f731cfa5237ff2edf67b\"] }' The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" }","title":"Alert"},{"location":"thehive/legacy/thehive3/api/alert/#alert","text":"","title":"Alert"},{"location":"thehive/legacy/thehive3/api/alert/#model-definition","text":"Required attributes: - title (text) : title of the alert - description (text) : description of the alert - severity (number) : severity of the alert (1: low; 2: medium; 3: high) default=2 - date (date) : date and time when the alert was raised default=now - tags (multi-string) : case tags default=empty - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - status (AlertStatus) : status of the alert ( New , Updated , Ignored , Imported ) default=New - type (string) : type of the alert (read only) - source (string) : source of the alert (read only) - sourceRef (string) : source reference of the alert (read only) - artifacts (multi-artifact) : artifact of the alert. It is a array of JSON object containing artifact attributes default=empty - follow (boolean) : if true, the alert becomes active when updated default=true Optional attributes: - caseTemplate (string) : case template to use when a case is created from this alert. If the alert specifies a non-existent case template or doesn't supply one, TheHive will import the alert into a case using a case template that has the exact same name as the alert's type if it exists. For example, if you raise an alert with a type value of splunk and you do not provide the caseTemplate attribute or supply a non-existent one (for example splink ), TheHive will import the alert using the case template called splunk if it exists. Otherwise, the alert will be imported using an empty case (i.e. from scratch). Attributes generated by the backend: - lastSyncDate (date) : date of the last synchronization - case (string) : id of the case, if created Alert ID is computed from type , source and sourceRef .","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/alert/#alert-manipulation","text":"","title":"Alert Manipulation"},{"location":"thehive/legacy/thehive3/api/alert/#alert-methods","text":"HTTP Method URI Action GET /api/alert List alerts POST /api/alert/_search Find alerts PATCH /api/alert/_bulk Update alerts in bulk POST /api/alert/_stats Compute stats on alerts POST /api/alert Create an alert GET /api/alert/:alertId Get an alert PATCH /api/alert/:alertId Update an alert DELETE /api/alert/:alertId Delete an alert POST /api/alert/:alertId/markAsRead Mark an alert as read POST /api/alert/:alertId/markAsUnread Mark an alert as unread POST /api/alert/:alertId/createCase Create a case from an alert POST /api/alert/:alertId/follow Follow an alert POST /api/alert/:alertId/unfollow Unfollow an alert POST /api/alert/:alertId/merge/:caseId Merge an alert in a case POST /api/alert/merge/_bulk Merge several alerts in one case","title":"Alert methods"},{"location":"thehive/legacy/thehive3/api/alert/#get-an-alert","text":"An alert's details can be retrieve using the url: GET /api/alert/:alertId The alert ID is obtained by List alerts or Find alerts API. If the parameter similarity is set to \"1\" or \"true\", this API returns information on cases which have similar observables. With this feature, output will contain the similarCases attribute which list case details with: - artifactCount: number of observables in the original case - iocCount: number of observables marked as IOC in original case - similarArtifactCount: number of observables which are in alert and in case - similarIocCount: number of IOCs which are in alert and in case warning IOCs are observables","title":"Get an alert"},{"location":"thehive/legacy/thehive3/api/alert/#examples","text":"Get alert without similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Get alert with similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810?similarity=1 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\", \"similarCases\": [ { \"_id\": \"AVwwrym-Rw5vhyJUfdJW\", \"artifactCount\": 5, \"endDate\": null, \"id\": \"AVwwrym-Rw5vhyJUfdJW\", \"iocCount\": 1, \"resolutionStatus\": null, \"severity\": 1, \"similarArtifactCount\": 2, \"similarIocCount\": 1, \"startDate\": 1495465039000, \"status\": \"Open\", \"tags\": [ \"src:MISP\" ], \"caseId\": 1405, \"title\": \"TEST TheHive\", \"tlp\": 2 } ] }","title":"Examples"},{"location":"thehive/legacy/thehive3/api/alert/#create-an-alert","text":"An alert can be created using the following url: POST /api/alert Required case attributes (cf. models) must be provided. If an alert with the same tuple type , source and sourceRef already exists, TheHive will refuse to create it. This call returns attributes of the created alert.","title":"Create an alert"},{"location":"thehive/legacy/thehive3/api/alert/#examples_1","text":"Creation of a simple alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"New Alert\", \"description\": \"N/A\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\" }' It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Creation of another alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"Other alert\", \"description\": \"alert description\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"severity\": 3, \"tlp\": 3, \"artifacts\": [ { \"dataType\": \"ip\", \"data\": \"127.0.0.1\", \"message\": \"localhost\" }, { \"dataType\": \"domain\", \"data\": \"thehive-project.org\", \"tags\": [\"home\", \"TheHive\"] }, { \"dataType\": \"file\", \"data\": \"logo.svg;image/svg+xml;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\", \"message\": \"logo\" } ], \"caseTemplate\": \"external-alert\" }'","title":"Examples"},{"location":"thehive/legacy/thehive3/api/alert/#merge-an-alert","text":"An alert can be merge in a case using the URL: POST /api/alert/:alertId/merge/:caseId Each observable of the alert will be added to the case if it doesn't exist in the case. The description of the alert will be appended to the case's description. The HTTP response contains the updated case.","title":"Merge an alert"},{"location":"thehive/legacy/thehive3/api/alert/#example","text":"Merge the alert ce2c00f17132359cb3c50dfbb1901810 in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810/merge/AVXeF-pZmeHK_2HEYj2z The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script ### Merged with alert #10 my alert title This is my alert description\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" }","title":"Example"},{"location":"thehive/legacy/thehive3/api/alert/#bulk-merge-alert","text":"This API merge several alerts with one case: POST /api/alert/merge/_bulk The observable of each alert listed in alertIds field will be imported into the case (identified by caseId field). The description of the case is not modified. The HTTP response contains the case.","title":"Bulk merge alert"},{"location":"thehive/legacy/thehive3/api/alert/#example_1","text":"Merge the alerts ce2c00f17132359cb3c50dfbb1901810 and a97148693200f731cfa5237ff2edf67b in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert/merge/_bulk -d '{ \"caseId\": \"AVXeF-pZmeHK_2HEYj2z\", \"alertIds\": [\"ce2c00f17132359cb3c50dfbb1901810\", \"a97148693200f731cfa5237ff2edf67b\"] }' The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" }","title":"Example"},{"location":"thehive/legacy/thehive3/api/artifact/","text":"Observable # Model definition # Required attributes: data (string) : content of the observable (read only). An observable can't contain data and attachment attributes attachment (attachment) : observable file content (read-only). An observable can't contain data and attachment attributes dataType (enumeration) : type of the observable (read only) message (text) : description of the observable in the context of the case startDate (date) : date of the observable creation default=now tlp (number) : TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 ioc (boolean) : indicates if the observable is an IOC default=false status (artifactStatus) : status of the observable ( Ok or Deleted ) default=Ok Optional attributes: - tags (multi-string) : observable tags Observable manipulation # Observable methods # HTTP Method URI Action POST /api/case/artifact/_search Find observables POST /api/case/artifact/_stats Compute stats on observables POST /api/case/:caseId/artifact Create an observable GET /api/case/artifact/:artifactId Get an observable DELETE /api/case/artifact/:artifactId Remove an observable PATCH /api/case/artifact/:artifactId Update an observable GET /api/case/artifact/:artifactId/similar Get list of similar observables PATCH /api/case/artifact/_bulk Update observables in bulk List Observables of a Case # Complete observable list of a case can be retrieved by performing a search: POST /api/case/artifact/_search Parameters: - query : { \"_parent\": { \"_type\": \"case\", \"_query\": { \"_id\": \"<<caseId>>\" } } } - range : all \\<\\<caseId>> must be replaced by case id (not the case number !)","title":"Observable"},{"location":"thehive/legacy/thehive3/api/artifact/#observable","text":"","title":"Observable"},{"location":"thehive/legacy/thehive3/api/artifact/#model-definition","text":"Required attributes: data (string) : content of the observable (read only). An observable can't contain data and attachment attributes attachment (attachment) : observable file content (read-only). An observable can't contain data and attachment attributes dataType (enumeration) : type of the observable (read only) message (text) : description of the observable in the context of the case startDate (date) : date of the observable creation default=now tlp (number) : TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 ioc (boolean) : indicates if the observable is an IOC default=false status (artifactStatus) : status of the observable ( Ok or Deleted ) default=Ok Optional attributes: - tags (multi-string) : observable tags","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/artifact/#observable-manipulation","text":"","title":"Observable manipulation"},{"location":"thehive/legacy/thehive3/api/artifact/#observable-methods","text":"HTTP Method URI Action POST /api/case/artifact/_search Find observables POST /api/case/artifact/_stats Compute stats on observables POST /api/case/:caseId/artifact Create an observable GET /api/case/artifact/:artifactId Get an observable DELETE /api/case/artifact/:artifactId Remove an observable PATCH /api/case/artifact/:artifactId Update an observable GET /api/case/artifact/:artifactId/similar Get list of similar observables PATCH /api/case/artifact/_bulk Update observables in bulk","title":"Observable methods"},{"location":"thehive/legacy/thehive3/api/artifact/#list-observables-of-a-case","text":"Complete observable list of a case can be retrieved by performing a search: POST /api/case/artifact/_search Parameters: - query : { \"_parent\": { \"_type\": \"case\", \"_query\": { \"_id\": \"<<caseId>>\" } } } - range : all \\<\\<caseId>> must be replaced by case id (not the case number !)","title":"List Observables of a Case"},{"location":"thehive/legacy/thehive3/api/authentication/","text":"Authentication # Most API calls require authentication. Credentials can be provided using a session cookie, an API key or directly using HTTP basic authentication (when enabled). Session cookie is suitable for browser authentication, not for a dedicated tool. The easiest solution if you want to write a tool that leverages TheHive's API is to use API key authentication. API keys can be generated using the Web interface of the product, under the user admin area. For example, to list cases, use the following curl command: # Using API key curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case TheHive also supports basic authentication (disabled by default). You can enable it by adding auth.method.basic=true in the configuration file. # Using basic authentication curl -u mylogin:mypassword http://127.0.0.1:9000/api/case","title":"Authentication"},{"location":"thehive/legacy/thehive3/api/authentication/#authentication","text":"Most API calls require authentication. Credentials can be provided using a session cookie, an API key or directly using HTTP basic authentication (when enabled). Session cookie is suitable for browser authentication, not for a dedicated tool. The easiest solution if you want to write a tool that leverages TheHive's API is to use API key authentication. API keys can be generated using the Web interface of the product, under the user admin area. For example, to list cases, use the following curl command: # Using API key curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case TheHive also supports basic authentication (disabled by default). You can enable it by adding auth.method.basic=true in the configuration file. # Using basic authentication curl -u mylogin:mypassword http://127.0.0.1:9000/api/case","title":"Authentication"},{"location":"thehive/legacy/thehive3/api/case/","text":"Case # Model definition # Required attributes: - title (text) : title of the case - description (text) : description of the case - severity (number) : severity of the case (1: low; 2: medium; 3: high) default=2 - startDate (date) : date and time of the begin of the case default=now - owner (string) : user to whom the case has been assigned default=use who create the case - flag (boolean) : flag of the case default=false - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - tags (multi-string) : case tags default=empty Optional attributes: - resolutionStatus (caseResolutionStatus) : resolution status of the case ( Indeterminate , FalsePositive , TruePositive , Other or Duplicated ) - impactStatus (caseImpactStatus) : impact status of the case ( NoImpact , WithImpact or NotApplicable ) - summary (text) : summary of the case, to be provided when closing a case - endDate (date) : resolution date - metrics (metrics) : list of metrics Attributes generated by the backend: - status (caseStatus) : status of the case ( Open , Resolved or Deleted ) default=Open - caseId (number) : Id of the case (auto-generated) - mergeInto (string) : ID of the case created by the merge - mergeFrom (multi-string) : IDs of the cases that were merged Case Manipulation # Case methods # HTTP Method URI Action GET /api/case List cases POST /api/case/_search Find cases PATCH /api/case/_bulk Update cases in bulk POST /api/case/_stats Compute stats on cases POST /api/case Create a case GET /api/case/:caseId Get a case PATCH /api/case/:caseId Update a case DELETE /api/case/:caseId Remove a case GET /api/case/:caseId/links Get list of cases linked to this case POST /api/case/:caseId1/_merge/:caseId2 Merge two cases Create a Case # A case can be created using the following url : POST /api/case Required case attributes (cf. models) must be provided. This call returns attributes of the created case. Examples # Creation of a simple case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" }' It returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_type\":\"case\" } Creation of another case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My second case\", \"description\": \"This case has been created by my custom script, its severity is high, tlp is red and it contains tags\", \"severity\": 3, \"tlp\": 3, \"tags\": [\"automatic\", \"creation\"] }' Creating a case with Tasks & Customfields: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" \"tasks\": [{ \"title\": \"mytask\", \"description\": \"description of my task\" }], \"customFields\": { \"cvss\": { \"number\": 9, }, \"businessImpact\": { \"string\": \"HIGH\" } } }' For the customFields object, the attribute names should correspond to the ExternalReference (cvss and businessImpact in the example above) not to the name of custom fields.","title":"Case"},{"location":"thehive/legacy/thehive3/api/case/#case","text":"","title":"Case"},{"location":"thehive/legacy/thehive3/api/case/#model-definition","text":"Required attributes: - title (text) : title of the case - description (text) : description of the case - severity (number) : severity of the case (1: low; 2: medium; 3: high) default=2 - startDate (date) : date and time of the begin of the case default=now - owner (string) : user to whom the case has been assigned default=use who create the case - flag (boolean) : flag of the case default=false - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - tags (multi-string) : case tags default=empty Optional attributes: - resolutionStatus (caseResolutionStatus) : resolution status of the case ( Indeterminate , FalsePositive , TruePositive , Other or Duplicated ) - impactStatus (caseImpactStatus) : impact status of the case ( NoImpact , WithImpact or NotApplicable ) - summary (text) : summary of the case, to be provided when closing a case - endDate (date) : resolution date - metrics (metrics) : list of metrics Attributes generated by the backend: - status (caseStatus) : status of the case ( Open , Resolved or Deleted ) default=Open - caseId (number) : Id of the case (auto-generated) - mergeInto (string) : ID of the case created by the merge - mergeFrom (multi-string) : IDs of the cases that were merged","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/case/#case-manipulation","text":"","title":"Case Manipulation"},{"location":"thehive/legacy/thehive3/api/case/#case-methods","text":"HTTP Method URI Action GET /api/case List cases POST /api/case/_search Find cases PATCH /api/case/_bulk Update cases in bulk POST /api/case/_stats Compute stats on cases POST /api/case Create a case GET /api/case/:caseId Get a case PATCH /api/case/:caseId Update a case DELETE /api/case/:caseId Remove a case GET /api/case/:caseId/links Get list of cases linked to this case POST /api/case/:caseId1/_merge/:caseId2 Merge two cases","title":"Case methods"},{"location":"thehive/legacy/thehive3/api/case/#create-a-case","text":"A case can be created using the following url : POST /api/case Required case attributes (cf. models) must be provided. This call returns attributes of the created case.","title":"Create a Case"},{"location":"thehive/legacy/thehive3/api/case/#examples","text":"Creation of a simple case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" }' It returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_type\":\"case\" } Creation of another case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My second case\", \"description\": \"This case has been created by my custom script, its severity is high, tlp is red and it contains tags\", \"severity\": 3, \"tlp\": 3, \"tags\": [\"automatic\", \"creation\"] }' Creating a case with Tasks & Customfields: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" \"tasks\": [{ \"title\": \"mytask\", \"description\": \"description of my task\" }], \"customFields\": { \"cvss\": { \"number\": 9, }, \"businessImpact\": { \"string\": \"HIGH\" } } }' For the customFields object, the attribute names should correspond to the ExternalReference (cvss and businessImpact in the example above) not to the name of custom fields.","title":"Examples"},{"location":"thehive/legacy/thehive3/api/log/","text":"Log # Model definition # Required attributes: - message (text) : content of the Log - startDate (date) : date of the log submission default=now - status (logStatus) : status of the log ( Ok or Deleted ) default=Ok Optional attributes: - attachment (attachment) : file attached to the log Log manipulation # Log methods # HTTP Method URI Action GET /api/case/task/:taskId/log Get logs of the task POST /api/case/task/:taskId/log/_search Find logs in specified task POST /api/case/task/log/_search Find logs POST /api/case/task/:taskId/log Create a log PATCH /api/case/task/log/:logId Update a log DELETE /api/case/task/log/:logId Remove a log GET /api/case/task/log/:logId Get a log Create a log # The URL used to create a task is: POST /api/case/task/<<taskId>>/log \\<\\<taskId>> must be replaced by task id Required log attributes (cf. models) must be provided. This call returns attributes of the created log. Examples # Creation of a simple log in task AVqqeXc9yQ6w1DNC8aDj : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -d '{ \"message\": \"Some message\" }' It returns: { \"startDate\": 1488919949497, \"createdBy\": \"admin\", \"createdAt\": 1488919949495, \"user\": \"myuser\", \"message\":\"Some message\", \"status\": \"Ok\", \"id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_type\":\"case_task_log\" } If log contains an attachment, the request must be in multipart format: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -F '_json={\"message\": \"Screenshot of fake site\"};type=application/json' -F 'attachment=@screenshot1.png;type=image/png' It returns: { \"createdBy\": \"myuser\", \"message\": \"Screenshot of fake site\", \"createdAt\": 1488920587391, \"startDate\": 1488920587394, \"user\": \"myuser\", \"status\": \"Ok\", \"attachment\": { \"name\": \"screenshot1.png\", \"hashes\": [ \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\", \"8b81e038ae0809488f20b5ec7dc91e488ef601e2\", \"c5883708f42a00c3ab1fba5bbb65786c\" ], \"size\": 15296, \"contentType\": \"image/png\", \"id\": \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\" }, \"id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_type\": \"case_task_log\" }","title":"Log"},{"location":"thehive/legacy/thehive3/api/log/#log","text":"","title":"Log"},{"location":"thehive/legacy/thehive3/api/log/#model-definition","text":"Required attributes: - message (text) : content of the Log - startDate (date) : date of the log submission default=now - status (logStatus) : status of the log ( Ok or Deleted ) default=Ok Optional attributes: - attachment (attachment) : file attached to the log","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/log/#log-manipulation","text":"","title":"Log manipulation"},{"location":"thehive/legacy/thehive3/api/log/#log-methods","text":"HTTP Method URI Action GET /api/case/task/:taskId/log Get logs of the task POST /api/case/task/:taskId/log/_search Find logs in specified task POST /api/case/task/log/_search Find logs POST /api/case/task/:taskId/log Create a log PATCH /api/case/task/log/:logId Update a log DELETE /api/case/task/log/:logId Remove a log GET /api/case/task/log/:logId Get a log","title":"Log methods"},{"location":"thehive/legacy/thehive3/api/log/#create-a-log","text":"The URL used to create a task is: POST /api/case/task/<<taskId>>/log \\<\\<taskId>> must be replaced by task id Required log attributes (cf. models) must be provided. This call returns attributes of the created log.","title":"Create a log"},{"location":"thehive/legacy/thehive3/api/log/#examples","text":"Creation of a simple log in task AVqqeXc9yQ6w1DNC8aDj : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -d '{ \"message\": \"Some message\" }' It returns: { \"startDate\": 1488919949497, \"createdBy\": \"admin\", \"createdAt\": 1488919949495, \"user\": \"myuser\", \"message\":\"Some message\", \"status\": \"Ok\", \"id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_type\":\"case_task_log\" } If log contains an attachment, the request must be in multipart format: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -F '_json={\"message\": \"Screenshot of fake site\"};type=application/json' -F 'attachment=@screenshot1.png;type=image/png' It returns: { \"createdBy\": \"myuser\", \"message\": \"Screenshot of fake site\", \"createdAt\": 1488920587391, \"startDate\": 1488920587394, \"user\": \"myuser\", \"status\": \"Ok\", \"attachment\": { \"name\": \"screenshot1.png\", \"hashes\": [ \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\", \"8b81e038ae0809488f20b5ec7dc91e488ef601e2\", \"c5883708f42a00c3ab1fba5bbb65786c\" ], \"size\": 15296, \"contentType\": \"image/png\", \"id\": \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\" }, \"id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_type\": \"case_task_log\" }","title":"Examples"},{"location":"thehive/legacy/thehive3/api/model/","text":"TheHive Model Definition # Field Types # string : textual data (example \"malware\"). text : textual data. The difference between string and text is in the way content can be searched. string is searchable as-is whereas text , words (token) are searchable, not the whole content (example \"Ten users have received this ransomware\"). date : date and time using timestamps with milliseconds format. boolean : true or false number : numeric value metrics : JSON object that contains only numbers Field can be prefixed with multi- in order to indicate that multiple values can be provided. Common Attributes # All entities share the following attributes: - createdBy (text) : login of the user who created the entity - createdAt (date) : date and time of the creation - updatedBy (text) : login of the user who last updated the entity - upadtedAt (date) : date and time of the last update - user (text) : same value as createdBy (this field is deprecated) These attributes are handled by the back-end and can't be directly updated.","title":"TheHive Model Definition"},{"location":"thehive/legacy/thehive3/api/model/#thehive-model-definition","text":"","title":"TheHive Model Definition"},{"location":"thehive/legacy/thehive3/api/model/#field-types","text":"string : textual data (example \"malware\"). text : textual data. The difference between string and text is in the way content can be searched. string is searchable as-is whereas text , words (token) are searchable, not the whole content (example \"Ten users have received this ransomware\"). date : date and time using timestamps with milliseconds format. boolean : true or false number : numeric value metrics : JSON object that contains only numbers Field can be prefixed with multi- in order to indicate that multiple values can be provided.","title":"Field Types"},{"location":"thehive/legacy/thehive3/api/model/#common-attributes","text":"All entities share the following attributes: - createdBy (text) : login of the user who created the entity - createdAt (date) : date and time of the creation - updatedBy (text) : login of the user who last updated the entity - upadtedAt (date) : date and time of the last update - user (text) : same value as createdBy (this field is deprecated) These attributes are handled by the back-end and can't be directly updated.","title":"Common Attributes"},{"location":"thehive/legacy/thehive3/api/request/","text":"Request formats # TheHive accepts several parameter formats within a HTTP request. They can be used indifferently. Input data can be: - a query string - URL-encoded form - multi-part - JSON Hence, the requests below are equivalent. Query String # curl -XPOST 'http://127.0.0.1:9000/api/login?user=me&password=secret' URL-encoded Form # curl -XPOST 'http://127.0.0.1:9000/api/login' -d user=me -d password=secret JSON # curl -XPOST http://127.0.0.1:9000/api/login -H 'Content-Type: application/json' -d '{ \"user\": \"me\", \"password\": \"secret\" }' Multi-part # curl -XPOST http://127.0.0.1:9000/api/login -F '_json=<-;type=application/json' << _EOF_ { \"user\": \"me\", \"password\": \"secret\" } _EOF_ Response Format # TheHive outputs JSON data.","title":"Request"},{"location":"thehive/legacy/thehive3/api/request/#request-formats","text":"TheHive accepts several parameter formats within a HTTP request. They can be used indifferently. Input data can be: - a query string - URL-encoded form - multi-part - JSON Hence, the requests below are equivalent.","title":"Request formats"},{"location":"thehive/legacy/thehive3/api/request/#query-string","text":"curl -XPOST 'http://127.0.0.1:9000/api/login?user=me&password=secret'","title":"Query String"},{"location":"thehive/legacy/thehive3/api/request/#url-encoded-form","text":"curl -XPOST 'http://127.0.0.1:9000/api/login' -d user=me -d password=secret","title":"URL-encoded Form"},{"location":"thehive/legacy/thehive3/api/request/#json","text":"curl -XPOST http://127.0.0.1:9000/api/login -H 'Content-Type: application/json' -d '{ \"user\": \"me\", \"password\": \"secret\" }'","title":"JSON"},{"location":"thehive/legacy/thehive3/api/request/#multi-part","text":"curl -XPOST http://127.0.0.1:9000/api/login -F '_json=<-;type=application/json' << _EOF_ { \"user\": \"me\", \"password\": \"secret\" } _EOF_","title":"Multi-part"},{"location":"thehive/legacy/thehive3/api/request/#response-format","text":"TheHive outputs JSON data.","title":"Response Format"},{"location":"thehive/legacy/thehive3/api/task/","text":"Task # Model definition # Required attributes: - title (text) : title of the task - status (taskStatus) : status of the task ( Waiting , InProgress , Completed or Cancel ) default=Waiting - flag (boolean) : flag of the task default=false Optional attributes: - owner (string) : user who owns the task. This is automatically set to current user when status is set to InProgress - description (text) : task details - startDate (date) : date of the beginning of the task. This is automatically set when status is set to Open - endDate (date) : date of the end of the task. This is automatically set when status is set to Completed Task manipulation # Task methods # HTTP Method URI Action POST /api/case/:caseId/task/_search Find tasks in a case (deprecated) POST /api/case/task/_search Find tasks POST /api/case/task/_stats Compute stats on tasks GET /api/case/task/:taskId Get a task PATCH /api/case/task/:taskId Update a task POST /api/case/:caseId/task Create a task Create a task # The URL used to create a task is: POST /api/case/<<caseId>>/task \\<\\<caseId>> must be replaced by case id (not the case number !) Required task attributes (cf. models) must be provided. This call returns attributes of the created task. Examples # Creation of a simple task in case AVqqdpY2yQ6w1DNC8aDh : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Do something\" }' It returns: { \"createdAt\": 1488918771513, \"status\": \"Waiting\", \"createdBy\": \"myuser\", \"title\": \"Do something\", \"order\": 0, \"user\": \"myuser\", \"flag\": false, \"id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_type\":\"case_task\" } Creation of another task: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Analyze the malware\", \"description\": \"The malware XXX is analyzed using sandbox ...\", \"owner\": \"Joe\", \"status\": \"InProgress\" }'","title":"Task"},{"location":"thehive/legacy/thehive3/api/task/#task","text":"","title":"Task"},{"location":"thehive/legacy/thehive3/api/task/#model-definition","text":"Required attributes: - title (text) : title of the task - status (taskStatus) : status of the task ( Waiting , InProgress , Completed or Cancel ) default=Waiting - flag (boolean) : flag of the task default=false Optional attributes: - owner (string) : user who owns the task. This is automatically set to current user when status is set to InProgress - description (text) : task details - startDate (date) : date of the beginning of the task. This is automatically set when status is set to Open - endDate (date) : date of the end of the task. This is automatically set when status is set to Completed","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/task/#task-manipulation","text":"","title":"Task manipulation"},{"location":"thehive/legacy/thehive3/api/task/#task-methods","text":"HTTP Method URI Action POST /api/case/:caseId/task/_search Find tasks in a case (deprecated) POST /api/case/task/_search Find tasks POST /api/case/task/_stats Compute stats on tasks GET /api/case/task/:taskId Get a task PATCH /api/case/task/:taskId Update a task POST /api/case/:caseId/task Create a task","title":"Task methods"},{"location":"thehive/legacy/thehive3/api/task/#create-a-task","text":"The URL used to create a task is: POST /api/case/<<caseId>>/task \\<\\<caseId>> must be replaced by case id (not the case number !) Required task attributes (cf. models) must be provided. This call returns attributes of the created task.","title":"Create a task"},{"location":"thehive/legacy/thehive3/api/task/#examples","text":"Creation of a simple task in case AVqqdpY2yQ6w1DNC8aDh : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Do something\" }' It returns: { \"createdAt\": 1488918771513, \"status\": \"Waiting\", \"createdBy\": \"myuser\", \"title\": \"Do something\", \"order\": 0, \"user\": \"myuser\", \"flag\": false, \"id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_type\":\"case_task\" } Creation of another task: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Analyze the malware\", \"description\": \"The malware XXX is analyzed using sandbox ...\", \"owner\": \"Joe\", \"status\": \"InProgress\" }'","title":"Examples"},{"location":"thehive/legacy/thehive3/api/user/","text":"User # Model definition # Required attributes: - login / id (string) : login of the user - userName (text) : Full name of the user - roles (multi-userRole) : Array containing roles of the user ( read , write or admin ) - status (userStatus) : Ok or Locked default=Ok - preference (string) : JSON object containing user preference default={} Optional attributes: - avatar (string) : avatar of user. It is an image encoded in base 64 - password (string) : user password if local authentication is used Attributes generated by the backend: - key (uuid) : API key to authenticate this user (deprecated) User Manipulation # User methods # HTTP Method URI Action GET /api/logout Logout POST /api/login User login GET /api/user/current Get current user POST /api/user/_search Find user POST /api/user Create a user GET /api/user/:userId Get a user DELETE /api/user/:userId Delete a user PATCH /api/user/:userId Update user details POST /api/user/:userId/password/set Set password POST /api/user/:userId/password/change Change password with-key (boolean) Create a User # A user can be created using the following URL: POST /api/user Required case attributes (cf. models) must be provided. This call returns attributes of the created user. This call is authenticated and requires admin role. Examples # Creation of a user: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/user -d '{ \"login\": \"georges\", \"name\": \"Georges Abitbol\", \"roles\": [\"read\", \"write\"], \"password\": \"La classe\" }' It returns: { \"createdBy\": \"myuser\", \"name\":\"Georges Abitbol\", \"roles\": [\"read\", \"write\" ], \"_id\": \"georges\", \"user\": \"myuser\", \"createdAt\": 1496561862924, \"status\": \"Ok\", \"id\": \"georges\", \"_type\": \"user\", \"has-key\":false } If external authentication is used (LDAP or AD) password field must not be provided.","title":"User"},{"location":"thehive/legacy/thehive3/api/user/#user","text":"","title":"User"},{"location":"thehive/legacy/thehive3/api/user/#model-definition","text":"Required attributes: - login / id (string) : login of the user - userName (text) : Full name of the user - roles (multi-userRole) : Array containing roles of the user ( read , write or admin ) - status (userStatus) : Ok or Locked default=Ok - preference (string) : JSON object containing user preference default={} Optional attributes: - avatar (string) : avatar of user. It is an image encoded in base 64 - password (string) : user password if local authentication is used Attributes generated by the backend: - key (uuid) : API key to authenticate this user (deprecated)","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/user/#user-manipulation","text":"","title":"User Manipulation"},{"location":"thehive/legacy/thehive3/api/user/#user-methods","text":"HTTP Method URI Action GET /api/logout Logout POST /api/login User login GET /api/user/current Get current user POST /api/user/_search Find user POST /api/user Create a user GET /api/user/:userId Get a user DELETE /api/user/:userId Delete a user PATCH /api/user/:userId Update user details POST /api/user/:userId/password/set Set password POST /api/user/:userId/password/change Change password with-key (boolean)","title":"User methods"},{"location":"thehive/legacy/thehive3/api/user/#create-a-user","text":"A user can be created using the following URL: POST /api/user Required case attributes (cf. models) must be provided. This call returns attributes of the created user. This call is authenticated and requires admin role.","title":"Create a User"},{"location":"thehive/legacy/thehive3/api/user/#examples","text":"Creation of a user: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/user -d '{ \"login\": \"georges\", \"name\": \"Georges Abitbol\", \"roles\": [\"read\", \"write\"], \"password\": \"La classe\" }' It returns: { \"createdBy\": \"myuser\", \"name\":\"Georges Abitbol\", \"roles\": [\"read\", \"write\" ], \"_id\": \"georges\", \"user\": \"myuser\", \"createdAt\": 1496561862924, \"status\": \"Ok\", \"id\": \"georges\", \"_type\": \"user\", \"has-key\":false } If external authentication is used (LDAP or AD) password field must not be provided.","title":"Examples"},{"location":"thehive/legacy/thehive3/api/connectors/","text":"Connectors API # TheHive offers an API to manipulate its various connectors Cortex MISP Metrics","title":"Connectors API"},{"location":"thehive/legacy/thehive3/api/connectors/#connectors-api","text":"TheHive offers an API to manipulate its various connectors Cortex MISP Metrics","title":"Connectors API"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/","text":"Cortex manipulation through TheHive # Cortex can be manipulated through TheHive with JSON over HTTP Job Analyzer Report","title":"Cortex manipulation through TheHive"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/#cortex-manipulation-through-thehive","text":"Cortex can be manipulated through TheHive with JSON over HTTP Job Analyzer Report","title":"Cortex manipulation through TheHive"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/","text":"Author : R\u00e9mi ALLAIN (rallain@cyberprotect.fr) - Cyberprotect, SDN International Analyzer # Model definition # Attributes: - id (string) : Analyzer id - name (string) : Analyzer name - version (string) : Analyzer version - description (text) : Analyzer description - dataTypeList (multi-string) : List of data type this analyzer can manage - cortexIds (string) : List of Cortex server id Analyzer manipulation # Analyzer methods # HTTP Method URI Action GET /api/connector/cortex/analyzer List all analyzers GET /api/connector/cortex/analyzer/:analyzerId Get details of an analyzer GET /api/connector/cortex/analyzer/type/:dataType List analyzers matching the dataType","title":"Analyzer"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#analyzer","text":"","title":"Analyzer"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#model-definition","text":"Attributes: - id (string) : Analyzer id - name (string) : Analyzer name - version (string) : Analyzer version - description (text) : Analyzer description - dataTypeList (multi-string) : List of data type this analyzer can manage - cortexIds (string) : List of Cortex server id","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#analyzer-manipulation","text":"","title":"Analyzer manipulation"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#analyzer-methods","text":"HTTP Method URI Action GET /api/connector/cortex/analyzer List all analyzers GET /api/connector/cortex/analyzer/:analyzerId Get details of an analyzer GET /api/connector/cortex/analyzer/type/:dataType List analyzers matching the dataType","title":"Analyzer methods"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/","text":"Job # Model definition # Required attributes: - analyzerId (string): identifier of the analyzer used by the job - status (enumeration): status of the job ( InProgress , Success , Failure ) default= InProgress - artifactId (string): identifier of the artifact to analyze - startDate (date): job start date Optional attributes: - endDate (date): job end date - report (string): raw content of the report sent back by the analyzer - cortexId (string): identifier of the cortex server - cortexJobId (string): identifier of the job in the cortex server Job manipulation # Job methods # HTTP Method URI Action POST /api/connector/cortex/job Create a new Cortex job GET /api/connector/cortex/job/:jobId Get a cortex job POST /api/connector/cortex/job/_search Search for cortex jobs Create a new Cortex job # Creating a new job can be done by performing the following query POST /api/connector/cortex/job Parameters: - cortexId : identifier of the Cortex server - artifactId : identifier of the artifact as found with an artifact search - analyzerId : name of the analyzer used by the job","title":"Job"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#job","text":"","title":"Job"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#model-definition","text":"Required attributes: - analyzerId (string): identifier of the analyzer used by the job - status (enumeration): status of the job ( InProgress , Success , Failure ) default= InProgress - artifactId (string): identifier of the artifact to analyze - startDate (date): job start date Optional attributes: - endDate (date): job end date - report (string): raw content of the report sent back by the analyzer - cortexId (string): identifier of the cortex server - cortexJobId (string): identifier of the job in the cortex server","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#job-manipulation","text":"","title":"Job manipulation"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#job-methods","text":"HTTP Method URI Action POST /api/connector/cortex/job Create a new Cortex job GET /api/connector/cortex/job/:jobId Get a cortex job POST /api/connector/cortex/job/_search Search for cortex jobs","title":"Job methods"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#create-a-new-cortex-job","text":"Creating a new job can be done by performing the following query POST /api/connector/cortex/job Parameters: - cortexId : identifier of the Cortex server - artifactId : identifier of the artifact as found with an artifact search - analyzerId : name of the analyzer used by the job","title":"Create a new Cortex job"},{"location":"thehive/legacy/thehive3/api/connectors/misp/","text":"MISP connector # MISP and TheHive can interact between each other in both ways: * TheHive is able to import events from a MISP instance as alerts and create cases from them * TheHive is able to export a case into MISP as an event and update it with the artifacts flagged as IOC as MISP attributes It is possible to use the API to control those behaviours. MISP imports # API methods # HTTP Method URI Action GET /api/connector/misp/_syncAlerts Synchronize from all MISP instances all MISP events published since the last synchronization GET /api/connector/misp/_syncAllAlerts Synchronize from all MISP instances all MISP published events since the beginning GET /api/connector/misp/_syncArtifacts Synchronize all artifacts from already imported alerts from all MISP instances MISP exports # API methods # HTTP Method URI Action POST /api/connector/misp/export/:caseId/:mispName Export a case to MISP Exporting a case to MISP # Exporting a case to MISP can be done by performing the following query POST /api/connector/misp/export/:caseId/:mispName With: * caseId: the elasticsearch id of the case * mispName: the name given to the MISP instance in TheHive configuration No parameters need to be sent in the query body. The response of this query will be a JSON table containing all artifacts sent as attributes in the MISP event.","title":"MISP connector"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#misp-connector","text":"MISP and TheHive can interact between each other in both ways: * TheHive is able to import events from a MISP instance as alerts and create cases from them * TheHive is able to export a case into MISP as an event and update it with the artifacts flagged as IOC as MISP attributes It is possible to use the API to control those behaviours.","title":"MISP connector"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#misp-imports","text":"","title":"MISP imports"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#api-methods","text":"HTTP Method URI Action GET /api/connector/misp/_syncAlerts Synchronize from all MISP instances all MISP events published since the last synchronization GET /api/connector/misp/_syncAllAlerts Synchronize from all MISP instances all MISP published events since the beginning GET /api/connector/misp/_syncArtifacts Synchronize all artifacts from already imported alerts from all MISP instances","title":"API methods"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#misp-exports","text":"","title":"MISP exports"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#api-methods_1","text":"HTTP Method URI Action POST /api/connector/misp/export/:caseId/:mispName Export a case to MISP","title":"API methods"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#exporting-a-case-to-misp","text":"Exporting a case to MISP can be done by performing the following query POST /api/connector/misp/export/:caseId/:mispName With: * caseId: the elasticsearch id of the case * mispName: the name given to the MISP instance in TheHive configuration No parameters need to be sent in the query body. The response of this query will be a JSON table containing all artifacts sent as attributes in the MISP event.","title":"Exporting a case to MISP"},{"location":"thehive/legacy/thehive3/installation/install-guide/","text":"Installation Guide # \u26a0\ufe0f Please read carrefully this documentation. Depending on you make a fresh installation or update an existing version, install version 3 or version 4, repository or packages names may vary. Current supported versions of TheHive are: - Version 3.5.0 and later that supports only Elasticsearch 7.x. - Version 4.0 and later. Instruction to install TheHive supporting Elasticsearch 6.x (EoL in Nov. 2020) are still detailled in this documentation. Before installing TheHive, you need to choose the installation option which suits your environment as described below. Once you have a chosen an option and installed the software, read the Configuration Guide . We also advise reading the Administration Guide . Table of Contents # Installation Options RPM DEB Docker Binary Build it Yourself Elasticsearch Installation System Package Start the Service Elasticsearch inside a Docker Installation Options # TheHive is available as: an RPM package a DEB package a Docker image a binary package In addition, TheHive can be also be built from the source code . RPM # RPM packages are published on a our RPM repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C Run the following command to import the GPG key : sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY Release versions # The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then you will able to install TheHive 3.5.0+ the package using yum : yum install thehive or install TheHive 4.0.0+ : yum install thehive4 Stable versions (or legacy versions) # The Stable repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x**, but version 6.x. Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/stable/noarch gpgcheck = 1 Then you will able to install TheHive 3.4.4 package using yum : yum install thehive Following beta versions # To follow beta versions of TheHive, use the following setup: And setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then you will able to install beta version of TheHive 3.x package using yum : yum install thehive or install beta version of TheHive 4.x : yum install thehive4 \u26a0\ufe0f We do not recommend that configuration for production servers Once the package is installed, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide . DEB # Debian packages are published on a our DEB packages repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C Release versions # The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup apt configuration with the release repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.5.0+ the package using apt command: apt install thehive or install TheHive 4.0.0+ : apt install thehive4 Stable versions (or legacy versions) # The main repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x , but version 6.x. Setup apt configuration with the main repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org stable main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.4.4 package using apt command: apt install thehive Beta versions # To follow beta versions of TheHive, use the following commands: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive \u26a0\ufe0f We do not recommend that configuration for production servers Docker # To use the Docker image, you must use Docker (courtesy of Captain Obvious). TheHive requires Elasticsearch to run. You can use docker-compose to start them together in Docker or install and configure Elasticsearch manually. Use Docker-compose # Docker-compose can start multiple dockers and link them together. The following docker-compose.yml file starts Elasticsearch and TheHive: version: \"2\" services: elasticsearch: image: elasticsearch:7.9.1 environment: - http.host=0.0.0.0 - discovery.type=single-node ulimits: nofile: soft: 65536 hard: 65536 cortex: image: thehiveproject/cortex:3.1.0-1 depends_on: - elasticsearch ports: - \"0.0.0.0:9001:9001\" thehive: image: thehiveproject/thehive:3.5.0-1 depends_on: - elasticsearch - cortex ports: - \"0.0.0.0:9000:9000\" command: --cortex-port 9001 Put this file in an empty folder and run docker-compose up . TheHive is exposed on 9000/tcp port and Cortex on 9001/tcp. These ports can be changed by modifying the docker-compose file. You can specify a custom TheHive configuration file ( application.conf ) by adding the following lines in the thehive section of your docker-compose file: volumes: - /path/to/application.conf:/etc/thehive/application.conf To take effect, be sure that: - '/path/to/application.conf' is readable for the user who runs the docker daemon (typically 644) - you specified command: --no-config in your docker-compose.yml file You should define where the data (i.e. the Elasticsearch database) will be located on your operating system by adding the following lines in the elasticsearch section of your docker-compose file: volumes: - /path/to/data:/usr/share/elasticsearch/data Running ElasticSearch in production mode requires a minimum vm.max_map_count of 262144. ElasticSearch documentation provides instructions on how to query and change this value. If you want to make Cortex be available on TheHive, you must create an account on Cortex, define an API key for it and provide that key to TheHive container using parameter --cortex-key or environment TH_CORTEX_KEY . Manual Installation of Elasticsearch # Elasticsearch can be installed on the same server as TheHive or on a different one. You can then configure TheHive according to the documentation and run TheHive docker as follow: docker run --volume /path/to/thehive/application.conf:/etc/thehive/application.conf thehiveproject/thehive:latest --no-config You can add the --publish docker option to expose TheHive HTTP service. Customize the Docker Image # By default, the TheHive Docker image has minimal configuration: - choose a random secret ( play.http.secret.key ) - search for the Elasticsearch instance (host named elasticsearch ) and add it to configuration - search for a Cortex instance (host named cortex ) and add it to configuration This behavior can be disabled by adding --no-config to the Docker command line: docker run thehiveproject/thehive:latest --no-config Or by adding the line command: --no-config in the thehive section of docker-compose file. It is possible to start database migration at startup with the parameter --auto-migration . If the initial administrator doesn't exist yet, you can request its creation with --create-admin followed by the user login and its password. You can also create a normal user with --create-user followed by the user login and its roles and its password. The image accepts more options. All options are available using environment variables. For boolean variable, 1 means true and other value means false. For multivalued variables, values are separated by coma. This is possible only with --create-admin . Option Env variable Description --no-config TH_NO_CONFIG Do not try to configure TheHive (add the secret and Elasticsearch) --no-config-secret TH_NO_CONFIG_SECRET Do not add the random secret to the configuration --secret <secret> TH_SECRET Cryptographic secret needed to secure sessions --show-secret TH_SHOW_SECRET Show the generated secret --no-config-es TH_NO_CONFIG_ES Do not add the Elasticsearch hosts to configuration --es-uri <uri> TH_CONFIG_ES Use this string to configure elasticsearch hosts (format: http(s)://host:port,host:port(/prefix)?querystring) --es-hostname <host> TH_ES_HOSTNAME Resolve this hostname to find Elasticsearch instances --no-config-cortex TH_NO_CONFIG_CORTEX Do not add Cortex configuration --cortex-proto <proto> TH_CORTEX_PROTO Define the protocol to connect to Cortex (default: http ) --cortex-port <port> TH_CORTEX_PORT Define the port to connect to Cortex (default: 9001 ) --cortex-url <url> TH_CORTEX_URL Add the Cortex connection --cortex-hostname <host> TH_CORTEX_HOSTNAME Resolve this hostname to find the Cortex instance --cortex-key <key> TH_CORTEX_KEY Define Cortex key --auto-migration TH_AUTO_MIGRATION Migrate the database, if needed --create-admin <user> <password TH_CREATE_ADMIN_LOGIN TH_CREATE_ADMIN_PASSWORD Create the first admin user, if not exist yet --create-user <user> <role> <password> TH_CREATE_USER_LOGIN TH_CREATE_USER_ROLE TH_CREATE_USER_PASSWORD Create a user, only in conjunction with admin creation Note : please remember that you must install and configure Elasticsearch . What to Do Next? # Once the Docker image is up and running, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide . Pre-release Versions # If you would like to use pre-release, beta versions of our Docker images and help us find bugs to the benefit of the whole community, please use thehiveproject/thehive:version-RCx . For example thehiveproject/thehive:3.1.0-RC1 . Binary # The following section contains the instructions to manually install TheHive using binaries on Ubuntu 20.04 LTS . 1. Minimal Ubuntu Installation # Install a minimal Ubuntu 20.04 system with the following software: Java runtime environment 1.8+ (JRE) Elasticsearch 7.x Make sure your system is up-to-date: sudo apt-get update sudo apt-get upgrade 2. Install a Java Virtual Machine # You can install either Oracle Java or OpenJDK. The latter is recommended. sudo apt-get install openjdk-11-jre-headless 3. Install Elasticsearch # To install Elasticsearch, please read the Elasticsearch Installation section below. 4. Install TheHive # Binary packages can be downloaded from Bintray . The latest version is called thehive-latest.zip . Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip ln -s thehive-x.x.x thehive Note : if you would like to use pre-release, beta versions of and help us find bugs to the benefit of the whole community, please download https://download.thehive-project.org/thehive-version-RCx.zip . For example https://download.thehive-project.org/thehive-3.5.0-RC1-1.zip . 5. First start # It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: sudo addgroup thehive sudo adduser --system thehive sudo cp /opt/thehive/package/thehive.service /usr/lib/systemd/system sudo chown -R thehive:thehive /opt/thehive sudo chgrp thehive /etc/thehive/application.conf sudo chmod 640 /etc/thehive/application.conf sudo systemctl enable thehive sudo service thehive start The only required parameter in order to start TheHive is the key of the server ( play.http.secret.key ). This key is used to authenticate cookies that contain data. If TheHive runs in cluster mode, all instances must share the same key. You can generate the minimal configuration with the following commands (they assume that you have created a dedicated user for TheHive, named thehive ): sudo mkdir /etc/thehive ( cat << _EOF_ # Secret key # ~~~~~ # The secret key is used to secure cryptographics functions. # If you deploy your application to several instances be sure to use the same key! play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ ) | sudo tee -a /etc/thehive/application.conf Now you can start TheHive. To do so, change your current directory to the TheHive installation directory ( /opt/thehive in this guide), then execute: bin/thehive -Dconfig.file = /etc/thehive/application.conf Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . Please note that the service may take some time to start. The first time you connect you will have to create the database schema. Click \"Migrate database\" to create the DB schema. Once done, you should be redirected to the page for creating the administrator's account. Once created, you should be redirected to the login page. Warning : at this stage, if you missed the creation of the admin account, you will not be able to do it unless you delete TheHive's index from Elasticsearch. In the case you made a mistake, first find out what is the current index of TheHive by running the following command on a host where the Elasticsearch DB used by TheHive is located: $ curl http://127.0.0.1:9200/_cat/indices?v The indexes that TheHive uses always start with the_hive_ following by a number. Let's assume that the output of the command is: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open cortex_1 PC_pLFGBS5G2TNQYr4ajgw 5 1 609 6 2 .1mb 2 .1mb yellow open the_hive_13 ft7GGTfhTr-4lSzZw5r1DQ 5 1 180131 3 51 .3mb 51 .3mb The index used by TheHive is the_hive_13 . To delete it, run the following command: $ curl -X DELETE http://127.0.0.1:9200/the_hive_13 Then reload the page or restart TheHive. 6. Update # To update TheHive from binaries, just stop the service, download the latest package, rebuild the link /opt/thehive and restart the service. service thehive stop cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip rm /opt/thehive && ln -s thehive-x.x.x thehive chown -R thehive:thehive /opt/thehive /opt/thehive-x.x.x service thehive start 7. Configuration # To configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide . Build it Yourself # The following section contains a step-by-step guide to build TheHive from its sources. 1. Pre-requisites # The following software are required to download and build TheHive: Java Development Kit 11 (JDK) git: use the system package or download it Node.js with its package manager (NPM) Grunt: after installing Node.js, run sudo npm install -g grunt-cli Bower: after installing Node.js, run sudo npm install -g bower Elasticsearch 5.6 2. Build # To install the requirements and build TheHive from sources, please follow the instructions below depending on your operating system. 2.1. CentOS/RHEL # Packages sudo yum -y install git bzip2 Installation of OpenJDK sudo yum -y install java-11-openjdk-devel Installation of Node.js Install the EPEL repository. You should have the extras repository enabled, then: sudo yum -y install epel-release Then, you can install Node.js, Grunt, and Bower: sudo yum -y install nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below. 2.2. Ubuntu # Packages sudo apt-get install git wget Installation of Oracle JDK sudo apt install openjdk-11-jdk-headless Installation of Node.js, Grunt and Bower sudo apt-get install curl curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below. 2.3. TheHive # Download The Source git clone https://github.com/TheHive-Project/TheHive.git Build the Project cd TheHive ./sbt clean stage This operation may take some time to complete as it will download all dependencies then build the back-end. This command cleans previous build files and creates an autonomous package in the target/universal/stage directory. This packages contains TheHive binaries with required libraries ( /lib ), configuration files ( /conf ) and startup scripts ( /bin ). Binaries are built and stored in TheHive/target/universal/stage/ . You can install them in /opt/thehive for example. sudo cp -r TheHive/target/universal/stage /opt/thehive Configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide . 2.4 Configure and Start Elasticsearch # Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive thread_pool.search.queue_size: 100000 Start the service: service elasticsearch restart 3. First start # Follow the first start section of the binary installation method above to start using TheHive. 4. Build the Front-end Only # Building the back-end builds also the front-end, so you don't need to build it separately. This section is useful only for troubleshooting or for installing the front-end on a reverse proxy. Go to the front-end directory: cd TheHive/ui Install Node.js libraries, which are required by this step, bower libraries (JavaScript libraries downloaded by the browser). Then build the front-end : npm install bower install grunt build This step generates static files (HTML, JavaScript and related resources) in the dist directory. They can be readily imported on a HTTP server. Elasticsearch Installation # If, for some reason, you need to install Elasticsearch, it can be installed using a system package or a Docker image. Version 5.X must be used. From version 6, Elasticsearch drops mapping type . System Package # Install the Elasticsearch package provided by Elastic Debian, Ubuntu # # PGP key installation sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key D88E42B4 # Alternative PGP key installation # wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - # Debian repository configuration echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list # Install https support for apt sudo apt install apt-transport-https # Elasticsearch installation sudo apt update && sudo apt install elasticsearch The Debian package does not start up the service by default, to prevent the instance from accidentally joining a cluster, without being configured appropriately. CentOS, RedHat, OpenSuSE # # PGP key installation sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch Create the file elasticsearch.repo in /etc/yum.repos.d/ for RedHat and CentOS, or in /etc/zypp/repos.d/ for OpenSuSE distributions, and add the following lines: [elasticsearch-5.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md Then, you can use the following command: # On CentOS and older Red Hat based distributions. sudo yum install elasticsearch # On Fedora and other newer Red Hat distributions. sudo dnf install elasticsearch # On OpenSUSE based distributions. sudo zypper install elasticsearch If you prefer using Elasticsearch inside a docker, see Elasticsearch inside a Docker . Configuration # It is highly recommended to avoid exposing this service to an untrusted zone. If Elasticsearch and TheHive run on the same host (and not in a docker), edit /etc/elasticsearch/elasticsearch.yml and set network.host parameter with 127.0.0.1 . TheHive use dynamic scripts to make partial updates. Hence, they must be activated using script.inline: true . The cluster name must also be set ( hive for example). Threadpool queue size must be set with a high value ( 100000 ). The default size will get the queue easily overloaded. Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 cluster.name: hive thread_pool.search.queue_size: 100000 Start the Service # Now that Elasticsearch is configured, start it as a service and check whether it's running: sudo systemctl enable elasticsearch.service sudo systemctl start elasticsearch.service sudo systemctl status elasticsearch.service The status should be active (running) . If it's not running, you can check for the reason in the logs: sudo journalctl -u elasticsearch.service Note that by default, the database is stored in /var/lib/elasticsearch and the logs in /var/log/elasticsearch Elasticsearch inside a Docker # You can also start Elasticsearch inside a docker. Use the following command and do not forget to specify the absolute path for persistent data on your host : docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:7.9.1","title":"Installation Guide"},{"location":"thehive/legacy/thehive3/installation/install-guide/#installation-guide","text":"\u26a0\ufe0f Please read carrefully this documentation. Depending on you make a fresh installation or update an existing version, install version 3 or version 4, repository or packages names may vary. Current supported versions of TheHive are: - Version 3.5.0 and later that supports only Elasticsearch 7.x. - Version 4.0 and later. Instruction to install TheHive supporting Elasticsearch 6.x (EoL in Nov. 2020) are still detailled in this documentation. Before installing TheHive, you need to choose the installation option which suits your environment as described below. Once you have a chosen an option and installed the software, read the Configuration Guide . We also advise reading the Administration Guide .","title":"Installation Guide"},{"location":"thehive/legacy/thehive3/installation/install-guide/#table-of-contents","text":"Installation Options RPM DEB Docker Binary Build it Yourself Elasticsearch Installation System Package Start the Service Elasticsearch inside a Docker","title":"Table of Contents"},{"location":"thehive/legacy/thehive3/installation/install-guide/#installation-options","text":"TheHive is available as: an RPM package a DEB package a Docker image a binary package In addition, TheHive can be also be built from the source code .","title":"Installation Options"},{"location":"thehive/legacy/thehive3/installation/install-guide/#rpm","text":"RPM packages are published on a our RPM repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C Run the following command to import the GPG key : sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY","title":"RPM"},{"location":"thehive/legacy/thehive3/installation/install-guide/#release-versions","text":"The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then you will able to install TheHive 3.5.0+ the package using yum : yum install thehive or install TheHive 4.0.0+ : yum install thehive4","title":"Release versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#stable-versions-or-legacy-versions","text":"The Stable repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x**, but version 6.x. Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/stable/noarch gpgcheck = 1 Then you will able to install TheHive 3.4.4 package using yum : yum install thehive","title":"Stable versions (or legacy versions)"},{"location":"thehive/legacy/thehive3/installation/install-guide/#following-beta-versions","text":"To follow beta versions of TheHive, use the following setup: And setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then you will able to install beta version of TheHive 3.x package using yum : yum install thehive or install beta version of TheHive 4.x : yum install thehive4 \u26a0\ufe0f We do not recommend that configuration for production servers Once the package is installed, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"Following beta versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#deb","text":"Debian packages are published on a our DEB packages repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C","title":"DEB"},{"location":"thehive/legacy/thehive3/installation/install-guide/#release-versions_1","text":"The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup apt configuration with the release repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.5.0+ the package using apt command: apt install thehive or install TheHive 4.0.0+ : apt install thehive4","title":"Release versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#stable-versions-or-legacy-versions_1","text":"The main repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x , but version 6.x. Setup apt configuration with the main repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org stable main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.4.4 package using apt command: apt install thehive","title":"Stable versions (or legacy versions)"},{"location":"thehive/legacy/thehive3/installation/install-guide/#beta-versions","text":"To follow beta versions of TheHive, use the following commands: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive \u26a0\ufe0f We do not recommend that configuration for production servers","title":"Beta versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#docker","text":"To use the Docker image, you must use Docker (courtesy of Captain Obvious). TheHive requires Elasticsearch to run. You can use docker-compose to start them together in Docker or install and configure Elasticsearch manually.","title":"Docker"},{"location":"thehive/legacy/thehive3/installation/install-guide/#use-docker-compose","text":"Docker-compose can start multiple dockers and link them together. The following docker-compose.yml file starts Elasticsearch and TheHive: version: \"2\" services: elasticsearch: image: elasticsearch:7.9.1 environment: - http.host=0.0.0.0 - discovery.type=single-node ulimits: nofile: soft: 65536 hard: 65536 cortex: image: thehiveproject/cortex:3.1.0-1 depends_on: - elasticsearch ports: - \"0.0.0.0:9001:9001\" thehive: image: thehiveproject/thehive:3.5.0-1 depends_on: - elasticsearch - cortex ports: - \"0.0.0.0:9000:9000\" command: --cortex-port 9001 Put this file in an empty folder and run docker-compose up . TheHive is exposed on 9000/tcp port and Cortex on 9001/tcp. These ports can be changed by modifying the docker-compose file. You can specify a custom TheHive configuration file ( application.conf ) by adding the following lines in the thehive section of your docker-compose file: volumes: - /path/to/application.conf:/etc/thehive/application.conf To take effect, be sure that: - '/path/to/application.conf' is readable for the user who runs the docker daemon (typically 644) - you specified command: --no-config in your docker-compose.yml file You should define where the data (i.e. the Elasticsearch database) will be located on your operating system by adding the following lines in the elasticsearch section of your docker-compose file: volumes: - /path/to/data:/usr/share/elasticsearch/data Running ElasticSearch in production mode requires a minimum vm.max_map_count of 262144. ElasticSearch documentation provides instructions on how to query and change this value. If you want to make Cortex be available on TheHive, you must create an account on Cortex, define an API key for it and provide that key to TheHive container using parameter --cortex-key or environment TH_CORTEX_KEY .","title":"Use Docker-compose"},{"location":"thehive/legacy/thehive3/installation/install-guide/#manual-installation-of-elasticsearch","text":"Elasticsearch can be installed on the same server as TheHive or on a different one. You can then configure TheHive according to the documentation and run TheHive docker as follow: docker run --volume /path/to/thehive/application.conf:/etc/thehive/application.conf thehiveproject/thehive:latest --no-config You can add the --publish docker option to expose TheHive HTTP service.","title":"Manual Installation of Elasticsearch"},{"location":"thehive/legacy/thehive3/installation/install-guide/#customize-the-docker-image","text":"By default, the TheHive Docker image has minimal configuration: - choose a random secret ( play.http.secret.key ) - search for the Elasticsearch instance (host named elasticsearch ) and add it to configuration - search for a Cortex instance (host named cortex ) and add it to configuration This behavior can be disabled by adding --no-config to the Docker command line: docker run thehiveproject/thehive:latest --no-config Or by adding the line command: --no-config in the thehive section of docker-compose file. It is possible to start database migration at startup with the parameter --auto-migration . If the initial administrator doesn't exist yet, you can request its creation with --create-admin followed by the user login and its password. You can also create a normal user with --create-user followed by the user login and its roles and its password. The image accepts more options. All options are available using environment variables. For boolean variable, 1 means true and other value means false. For multivalued variables, values are separated by coma. This is possible only with --create-admin . Option Env variable Description --no-config TH_NO_CONFIG Do not try to configure TheHive (add the secret and Elasticsearch) --no-config-secret TH_NO_CONFIG_SECRET Do not add the random secret to the configuration --secret <secret> TH_SECRET Cryptographic secret needed to secure sessions --show-secret TH_SHOW_SECRET Show the generated secret --no-config-es TH_NO_CONFIG_ES Do not add the Elasticsearch hosts to configuration --es-uri <uri> TH_CONFIG_ES Use this string to configure elasticsearch hosts (format: http(s)://host:port,host:port(/prefix)?querystring) --es-hostname <host> TH_ES_HOSTNAME Resolve this hostname to find Elasticsearch instances --no-config-cortex TH_NO_CONFIG_CORTEX Do not add Cortex configuration --cortex-proto <proto> TH_CORTEX_PROTO Define the protocol to connect to Cortex (default: http ) --cortex-port <port> TH_CORTEX_PORT Define the port to connect to Cortex (default: 9001 ) --cortex-url <url> TH_CORTEX_URL Add the Cortex connection --cortex-hostname <host> TH_CORTEX_HOSTNAME Resolve this hostname to find the Cortex instance --cortex-key <key> TH_CORTEX_KEY Define Cortex key --auto-migration TH_AUTO_MIGRATION Migrate the database, if needed --create-admin <user> <password TH_CREATE_ADMIN_LOGIN TH_CREATE_ADMIN_PASSWORD Create the first admin user, if not exist yet --create-user <user> <role> <password> TH_CREATE_USER_LOGIN TH_CREATE_USER_ROLE TH_CREATE_USER_PASSWORD Create a user, only in conjunction with admin creation Note : please remember that you must install and configure Elasticsearch .","title":"Customize the Docker Image"},{"location":"thehive/legacy/thehive3/installation/install-guide/#what-to-do-next","text":"Once the Docker image is up and running, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"What to Do Next?"},{"location":"thehive/legacy/thehive3/installation/install-guide/#pre-release-versions","text":"If you would like to use pre-release, beta versions of our Docker images and help us find bugs to the benefit of the whole community, please use thehiveproject/thehive:version-RCx . For example thehiveproject/thehive:3.1.0-RC1 .","title":"Pre-release Versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#binary","text":"The following section contains the instructions to manually install TheHive using binaries on Ubuntu 20.04 LTS .","title":"Binary"},{"location":"thehive/legacy/thehive3/installation/install-guide/#1-minimal-ubuntu-installation","text":"Install a minimal Ubuntu 20.04 system with the following software: Java runtime environment 1.8+ (JRE) Elasticsearch 7.x Make sure your system is up-to-date: sudo apt-get update sudo apt-get upgrade","title":"1. Minimal Ubuntu Installation"},{"location":"thehive/legacy/thehive3/installation/install-guide/#2-install-a-java-virtual-machine","text":"You can install either Oracle Java or OpenJDK. The latter is recommended. sudo apt-get install openjdk-11-jre-headless","title":"2. Install a Java Virtual Machine"},{"location":"thehive/legacy/thehive3/installation/install-guide/#3-install-elasticsearch","text":"To install Elasticsearch, please read the Elasticsearch Installation section below.","title":"3. Install Elasticsearch"},{"location":"thehive/legacy/thehive3/installation/install-guide/#4-install-thehive","text":"Binary packages can be downloaded from Bintray . The latest version is called thehive-latest.zip . Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip ln -s thehive-x.x.x thehive Note : if you would like to use pre-release, beta versions of and help us find bugs to the benefit of the whole community, please download https://download.thehive-project.org/thehive-version-RCx.zip . For example https://download.thehive-project.org/thehive-3.5.0-RC1-1.zip .","title":"4. Install TheHive"},{"location":"thehive/legacy/thehive3/installation/install-guide/#5-first-start","text":"It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: sudo addgroup thehive sudo adduser --system thehive sudo cp /opt/thehive/package/thehive.service /usr/lib/systemd/system sudo chown -R thehive:thehive /opt/thehive sudo chgrp thehive /etc/thehive/application.conf sudo chmod 640 /etc/thehive/application.conf sudo systemctl enable thehive sudo service thehive start The only required parameter in order to start TheHive is the key of the server ( play.http.secret.key ). This key is used to authenticate cookies that contain data. If TheHive runs in cluster mode, all instances must share the same key. You can generate the minimal configuration with the following commands (they assume that you have created a dedicated user for TheHive, named thehive ): sudo mkdir /etc/thehive ( cat << _EOF_ # Secret key # ~~~~~ # The secret key is used to secure cryptographics functions. # If you deploy your application to several instances be sure to use the same key! play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ ) | sudo tee -a /etc/thehive/application.conf Now you can start TheHive. To do so, change your current directory to the TheHive installation directory ( /opt/thehive in this guide), then execute: bin/thehive -Dconfig.file = /etc/thehive/application.conf Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . Please note that the service may take some time to start. The first time you connect you will have to create the database schema. Click \"Migrate database\" to create the DB schema. Once done, you should be redirected to the page for creating the administrator's account. Once created, you should be redirected to the login page. Warning : at this stage, if you missed the creation of the admin account, you will not be able to do it unless you delete TheHive's index from Elasticsearch. In the case you made a mistake, first find out what is the current index of TheHive by running the following command on a host where the Elasticsearch DB used by TheHive is located: $ curl http://127.0.0.1:9200/_cat/indices?v The indexes that TheHive uses always start with the_hive_ following by a number. Let's assume that the output of the command is: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open cortex_1 PC_pLFGBS5G2TNQYr4ajgw 5 1 609 6 2 .1mb 2 .1mb yellow open the_hive_13 ft7GGTfhTr-4lSzZw5r1DQ 5 1 180131 3 51 .3mb 51 .3mb The index used by TheHive is the_hive_13 . To delete it, run the following command: $ curl -X DELETE http://127.0.0.1:9200/the_hive_13 Then reload the page or restart TheHive.","title":"5. First start"},{"location":"thehive/legacy/thehive3/installation/install-guide/#6-update","text":"To update TheHive from binaries, just stop the service, download the latest package, rebuild the link /opt/thehive and restart the service. service thehive stop cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip rm /opt/thehive && ln -s thehive-x.x.x thehive chown -R thehive:thehive /opt/thehive /opt/thehive-x.x.x service thehive start","title":"6. Update"},{"location":"thehive/legacy/thehive3/installation/install-guide/#7-configuration","text":"To configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"7. Configuration"},{"location":"thehive/legacy/thehive3/installation/install-guide/#build-it-yourself","text":"The following section contains a step-by-step guide to build TheHive from its sources.","title":"Build it Yourself"},{"location":"thehive/legacy/thehive3/installation/install-guide/#1-pre-requisites","text":"The following software are required to download and build TheHive: Java Development Kit 11 (JDK) git: use the system package or download it Node.js with its package manager (NPM) Grunt: after installing Node.js, run sudo npm install -g grunt-cli Bower: after installing Node.js, run sudo npm install -g bower Elasticsearch 5.6","title":"1. Pre-requisites"},{"location":"thehive/legacy/thehive3/installation/install-guide/#2-build","text":"To install the requirements and build TheHive from sources, please follow the instructions below depending on your operating system.","title":"2. Build"},{"location":"thehive/legacy/thehive3/installation/install-guide/#21-centosrhel","text":"Packages sudo yum -y install git bzip2 Installation of OpenJDK sudo yum -y install java-11-openjdk-devel Installation of Node.js Install the EPEL repository. You should have the extras repository enabled, then: sudo yum -y install epel-release Then, you can install Node.js, Grunt, and Bower: sudo yum -y install nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below.","title":"2.1. CentOS/RHEL"},{"location":"thehive/legacy/thehive3/installation/install-guide/#22-ubuntu","text":"Packages sudo apt-get install git wget Installation of Oracle JDK sudo apt install openjdk-11-jdk-headless Installation of Node.js, Grunt and Bower sudo apt-get install curl curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below.","title":"2.2. Ubuntu"},{"location":"thehive/legacy/thehive3/installation/install-guide/#23-thehive","text":"Download The Source git clone https://github.com/TheHive-Project/TheHive.git Build the Project cd TheHive ./sbt clean stage This operation may take some time to complete as it will download all dependencies then build the back-end. This command cleans previous build files and creates an autonomous package in the target/universal/stage directory. This packages contains TheHive binaries with required libraries ( /lib ), configuration files ( /conf ) and startup scripts ( /bin ). Binaries are built and stored in TheHive/target/universal/stage/ . You can install them in /opt/thehive for example. sudo cp -r TheHive/target/universal/stage /opt/thehive Configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"2.3. TheHive"},{"location":"thehive/legacy/thehive3/installation/install-guide/#24-configure-and-start-elasticsearch","text":"Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive thread_pool.search.queue_size: 100000 Start the service: service elasticsearch restart","title":"2.4 Configure and Start Elasticsearch"},{"location":"thehive/legacy/thehive3/installation/install-guide/#3-first-start","text":"Follow the first start section of the binary installation method above to start using TheHive.","title":"3. First start"},{"location":"thehive/legacy/thehive3/installation/install-guide/#4-build-the-front-end-only","text":"Building the back-end builds also the front-end, so you don't need to build it separately. This section is useful only for troubleshooting or for installing the front-end on a reverse proxy. Go to the front-end directory: cd TheHive/ui Install Node.js libraries, which are required by this step, bower libraries (JavaScript libraries downloaded by the browser). Then build the front-end : npm install bower install grunt build This step generates static files (HTML, JavaScript and related resources) in the dist directory. They can be readily imported on a HTTP server.","title":"4. Build the Front-end Only"},{"location":"thehive/legacy/thehive3/installation/install-guide/#elasticsearch-installation","text":"If, for some reason, you need to install Elasticsearch, it can be installed using a system package or a Docker image. Version 5.X must be used. From version 6, Elasticsearch drops mapping type .","title":"Elasticsearch Installation"},{"location":"thehive/legacy/thehive3/installation/install-guide/#system-package","text":"Install the Elasticsearch package provided by Elastic","title":"System Package"},{"location":"thehive/legacy/thehive3/installation/install-guide/#debian-ubuntu","text":"# PGP key installation sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key D88E42B4 # Alternative PGP key installation # wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - # Debian repository configuration echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list # Install https support for apt sudo apt install apt-transport-https # Elasticsearch installation sudo apt update && sudo apt install elasticsearch The Debian package does not start up the service by default, to prevent the instance from accidentally joining a cluster, without being configured appropriately.","title":"Debian, Ubuntu"},{"location":"thehive/legacy/thehive3/installation/install-guide/#centos-redhat-opensuse","text":"# PGP key installation sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch Create the file elasticsearch.repo in /etc/yum.repos.d/ for RedHat and CentOS, or in /etc/zypp/repos.d/ for OpenSuSE distributions, and add the following lines: [elasticsearch-5.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md Then, you can use the following command: # On CentOS and older Red Hat based distributions. sudo yum install elasticsearch # On Fedora and other newer Red Hat distributions. sudo dnf install elasticsearch # On OpenSUSE based distributions. sudo zypper install elasticsearch If you prefer using Elasticsearch inside a docker, see Elasticsearch inside a Docker .","title":"CentOS, RedHat, OpenSuSE"},{"location":"thehive/legacy/thehive3/installation/install-guide/#configuration","text":"It is highly recommended to avoid exposing this service to an untrusted zone. If Elasticsearch and TheHive run on the same host (and not in a docker), edit /etc/elasticsearch/elasticsearch.yml and set network.host parameter with 127.0.0.1 . TheHive use dynamic scripts to make partial updates. Hence, they must be activated using script.inline: true . The cluster name must also be set ( hive for example). Threadpool queue size must be set with a high value ( 100000 ). The default size will get the queue easily overloaded. Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 cluster.name: hive thread_pool.search.queue_size: 100000","title":"Configuration"},{"location":"thehive/legacy/thehive3/installation/install-guide/#start-the-service","text":"Now that Elasticsearch is configured, start it as a service and check whether it's running: sudo systemctl enable elasticsearch.service sudo systemctl start elasticsearch.service sudo systemctl status elasticsearch.service The status should be active (running) . If it's not running, you can check for the reason in the logs: sudo journalctl -u elasticsearch.service Note that by default, the database is stored in /var/lib/elasticsearch and the logs in /var/log/elasticsearch","title":"Start the Service"},{"location":"thehive/legacy/thehive3/installation/install-guide/#elasticsearch-inside-a-docker","text":"You can also start Elasticsearch inside a docker. Use the following command and do not forget to specify the absolute path for persistent data on your host : docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:7.9.1","title":"Elasticsearch inside a Docker"},{"location":"thehive/operations/backup-restore/","text":"Backup and restore # This guide has only been tested on single node Cassandra server Note https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupRestore.html Overview # To be restored successfully, TheHive requires following data beeing saved: The database Files optionnally, the index. Cassandra # Pre requisites # To backup or export database from Cassandra, following information are required: Cassandra admin password keyspace used by thehive (default = thehive ). This can be checked in the application.conf configuration file, in the database configuration in storage , cql and keyspace attribute. Tip This information can be found in TheHive configuration: [..] db.janusgraph { storage { backend: cql hostname: [\"127.0.0.1\"] cql { cluster-name: thp keyspace: thehive } } [..] Backup # Following actions should be performed to backup the data successfully: Save the database schema Create a snapshot Save the data and the schema Save the database schema # This can be done with the following command: cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <SERVER_IP> -e \"DESCRIBE KEYSPACE <KEYSPACE>\" > schema.cql Create a snapshot and an archive # Considering that your keyspace is thehive and backup_name is the name of the snapshot, run the following commands: Before taking snapshots nodetool cleanup thehive Take a snapshot nodetool snapshot thehive -t backup_name Create and archive with the snapshot data: tar cjf backup.tbz /var/lib/cassandra/data/thehive/*/snapshots/backup_name/ Remove old snapshots (if necessary) nodetool -h localhost -p 7199 clearsnapshot -t <snapshotname> Example # Example of script to generate backups of TheHive keyspace #!/bin/bash ## Create a CQL file with the schema of the KEYSPACE ## and an tbz archive containing the snapshot ## Complete variables before running: ## KEYSPACE: Identify the right keyspace to save in cassandra ## SNAPSHOT: choose a name for the backup IP = 10 .1.1.1 SOURCE_KEYSPACE = \"thehive\" SNAPSHOT = \"thehive_20210413\" # Backup Cassandra nodetool cleanup ${ SOURCE_KEYSPACE } nodetool snapshot ${ SOURCE_KEYSPACE } -t ${ SNAPSHOT } echo -n \"Cassandra admin password\" : read -s CASSANDRA_PASSWORD ## Save schema cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"DESCRIBE KEYSPACE ${ SOURCE_KEYSPACE } \" > schema_ ${ SNAPSHOT } .cql ## Create archive if [[ $? == 0 ]] then tar cjf ${ SNAPSHOT } .tbz /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } /*/snapshots/ ${ SNAPSHOT } / fi Restore data # Pre requisites # Following data is required to restore TheHive database successfully: The database schema (example: schema.cql ) A backup of the database (example: backup.tbz ) Keyspace to restore does not exist in the database (or it will be overwritten) Restore # Restore keyspace cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"source 'schema.cql';\" Unarchive backup files: tar jxf /PATH/TO/backup.tbz -C /tmp/cassandra_backup And restore snapshots files: cd /var/lib/cassandra/data/thehive for I in ` ls /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE> ` ; do cp /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE>/ $I /snapshots/<BACKUP_NAME>/* /var/lib/cassandra/data/<KEYSPACE>/ $I / ; done Ensure Cassandra user keep ownership on the files: chown -R cassandra:cassandra /var/lib/cassandra/data/<KEYSPACE> Refresh tables for TABLE in ` cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"use <KEYSPACE>; DESC tables ;\" ` do nodetool refresh <KEYSPACE> <TABLE_NAME> done Ensure no Commitlog file exist before restarting Cassandra service. ( /var/lib/cassandra/commitlog ) Example of script to restore TheHive keyspace in Cassandra #!/bin/bash ## Restore a KEYSPACE and its data from a CQL file with the schema of the ## KEYSPACE and an tbz archive containing the snapshot ## Complete variables before running: ## IP: IP of cassandra server ## TMP: choose a TMP folder !!! this folder will be removed if exists. ## SOURCE_KEYSPACE: KEYSPACE used in the backup ## TARGET_KEYSPACE: new KEYSPACE name ; use same name of SOURCE_KEYSPACE if no changes ## SNAPSHOT: choose a name for the backup IP = 10 .10.10.10 TMP = /tmp/cassandra_backup SOURCE_KEYSPACE = \"thehive\" TARGET_KEYSPACE = \"thehive\" SNAPSHOT = \"thehive_20210920\" ## Uncompress data in TMP folder rm -rf ${ TMP } && mkdir ${ TMP } tar jxf ${ SNAPSHOT } .tbz -C ${ TMP } ## Define new KEYSPACE NAME sed -i \"s/ ${ SOURCE_KEYSPACE } / ${ TARGET_KEYSPACE } /g\" schema_ ${ SNAPSHOT } .cql ## Read Cassandra password echo -n \"Cassandra admin password: \" read -s CASSANDRA_PASSWORD ## Restore keyspace cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"source 'schema_ ${ SNAPSHOT } .cql';\" ## Restore data for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do cp -r ${ TMP } /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } / ${ TABLE } -*/snapshots/ ${ SNAPSHOT } /* /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / $TABLE -*/ done ## Change ownership chown -R cassandra:cassandra /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ## Refresh tables for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do nodetool refresh ${ TARGET_KEYSPACE } ${ TABLE } done Files # Backup # Wether you use local or distributed files system storage, copy the content of the folder/bucket. Restore # Restore the saved files into the destination folder/bucket that will be used by TheHive.","title":"Backup & restore"},{"location":"thehive/operations/backup-restore/#backup-and-restore","text":"This guide has only been tested on single node Cassandra server Note https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupRestore.html","title":"Backup and restore"},{"location":"thehive/operations/backup-restore/#overview","text":"To be restored successfully, TheHive requires following data beeing saved: The database Files optionnally, the index.","title":"Overview"},{"location":"thehive/operations/backup-restore/#cassandra","text":"","title":"Cassandra"},{"location":"thehive/operations/backup-restore/#pre-requisites","text":"To backup or export database from Cassandra, following information are required: Cassandra admin password keyspace used by thehive (default = thehive ). This can be checked in the application.conf configuration file, in the database configuration in storage , cql and keyspace attribute. Tip This information can be found in TheHive configuration: [..] db.janusgraph { storage { backend: cql hostname: [\"127.0.0.1\"] cql { cluster-name: thp keyspace: thehive } } [..]","title":"Pre requisites"},{"location":"thehive/operations/backup-restore/#backup","text":"Following actions should be performed to backup the data successfully: Save the database schema Create a snapshot Save the data and the schema","title":"Backup"},{"location":"thehive/operations/backup-restore/#save-the-database-schema","text":"This can be done with the following command: cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <SERVER_IP> -e \"DESCRIBE KEYSPACE <KEYSPACE>\" > schema.cql","title":"Save the database schema"},{"location":"thehive/operations/backup-restore/#create-a-snapshot-and-an-archive","text":"Considering that your keyspace is thehive and backup_name is the name of the snapshot, run the following commands: Before taking snapshots nodetool cleanup thehive Take a snapshot nodetool snapshot thehive -t backup_name Create and archive with the snapshot data: tar cjf backup.tbz /var/lib/cassandra/data/thehive/*/snapshots/backup_name/ Remove old snapshots (if necessary) nodetool -h localhost -p 7199 clearsnapshot -t <snapshotname>","title":"Create a snapshot and an archive"},{"location":"thehive/operations/backup-restore/#example","text":"Example of script to generate backups of TheHive keyspace #!/bin/bash ## Create a CQL file with the schema of the KEYSPACE ## and an tbz archive containing the snapshot ## Complete variables before running: ## KEYSPACE: Identify the right keyspace to save in cassandra ## SNAPSHOT: choose a name for the backup IP = 10 .1.1.1 SOURCE_KEYSPACE = \"thehive\" SNAPSHOT = \"thehive_20210413\" # Backup Cassandra nodetool cleanup ${ SOURCE_KEYSPACE } nodetool snapshot ${ SOURCE_KEYSPACE } -t ${ SNAPSHOT } echo -n \"Cassandra admin password\" : read -s CASSANDRA_PASSWORD ## Save schema cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"DESCRIBE KEYSPACE ${ SOURCE_KEYSPACE } \" > schema_ ${ SNAPSHOT } .cql ## Create archive if [[ $? == 0 ]] then tar cjf ${ SNAPSHOT } .tbz /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } /*/snapshots/ ${ SNAPSHOT } / fi","title":"Example"},{"location":"thehive/operations/backup-restore/#restore-data","text":"","title":"Restore data"},{"location":"thehive/operations/backup-restore/#pre-requisites_1","text":"Following data is required to restore TheHive database successfully: The database schema (example: schema.cql ) A backup of the database (example: backup.tbz ) Keyspace to restore does not exist in the database (or it will be overwritten)","title":"Pre requisites"},{"location":"thehive/operations/backup-restore/#restore","text":"Restore keyspace cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"source 'schema.cql';\" Unarchive backup files: tar jxf /PATH/TO/backup.tbz -C /tmp/cassandra_backup And restore snapshots files: cd /var/lib/cassandra/data/thehive for I in ` ls /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE> ` ; do cp /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE>/ $I /snapshots/<BACKUP_NAME>/* /var/lib/cassandra/data/<KEYSPACE>/ $I / ; done Ensure Cassandra user keep ownership on the files: chown -R cassandra:cassandra /var/lib/cassandra/data/<KEYSPACE> Refresh tables for TABLE in ` cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"use <KEYSPACE>; DESC tables ;\" ` do nodetool refresh <KEYSPACE> <TABLE_NAME> done Ensure no Commitlog file exist before restarting Cassandra service. ( /var/lib/cassandra/commitlog ) Example of script to restore TheHive keyspace in Cassandra #!/bin/bash ## Restore a KEYSPACE and its data from a CQL file with the schema of the ## KEYSPACE and an tbz archive containing the snapshot ## Complete variables before running: ## IP: IP of cassandra server ## TMP: choose a TMP folder !!! this folder will be removed if exists. ## SOURCE_KEYSPACE: KEYSPACE used in the backup ## TARGET_KEYSPACE: new KEYSPACE name ; use same name of SOURCE_KEYSPACE if no changes ## SNAPSHOT: choose a name for the backup IP = 10 .10.10.10 TMP = /tmp/cassandra_backup SOURCE_KEYSPACE = \"thehive\" TARGET_KEYSPACE = \"thehive\" SNAPSHOT = \"thehive_20210920\" ## Uncompress data in TMP folder rm -rf ${ TMP } && mkdir ${ TMP } tar jxf ${ SNAPSHOT } .tbz -C ${ TMP } ## Define new KEYSPACE NAME sed -i \"s/ ${ SOURCE_KEYSPACE } / ${ TARGET_KEYSPACE } /g\" schema_ ${ SNAPSHOT } .cql ## Read Cassandra password echo -n \"Cassandra admin password: \" read -s CASSANDRA_PASSWORD ## Restore keyspace cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"source 'schema_ ${ SNAPSHOT } .cql';\" ## Restore data for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do cp -r ${ TMP } /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } / ${ TABLE } -*/snapshots/ ${ SNAPSHOT } /* /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / $TABLE -*/ done ## Change ownership chown -R cassandra:cassandra /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ## Refresh tables for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do nodetool refresh ${ TARGET_KEYSPACE } ${ TABLE } done","title":"Restore"},{"location":"thehive/operations/backup-restore/#files","text":"","title":"Files"},{"location":"thehive/operations/backup-restore/#backup_1","text":"Wether you use local or distributed files system storage, copy the content of the folder/bucket.","title":"Backup"},{"location":"thehive/operations/backup-restore/#restore_1","text":"Restore the saved files into the destination folder/bucket that will be used by TheHive.","title":"Restore"},{"location":"thehive/operations/cassandra-security/","text":"Security in Apache Cassandra # References Internal authentication https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureInternalAuthenticationTOC.html Node to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLNodeToNode.html Client to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLClientToNode.html https://docs.janusgraph.org/basics/configuration-reference/#storagecqlssl Authentication with Cassandra # Create an account and grant permissions on keyspace CREATE ROLE thehive WITH PASSWORD = 'thehive1234' AND LOGIN = true ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO thehive ; Configure TheHive with the account Update /etc/thehive/application.conf accordingly: db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"xxx.xxx.xxx.xxx\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" cql { cluster-name: thp keyspace: thehive } } Cassandra node to node encryption # This document addresses communication between Cassandra servers, when a Cassandra cluster contains several nodes. server_encryption_options : internode_encryption : all keystore : /path/to/keystore.jks keystore_password : keystorepassword truststore : /path/to/truststore.jks truststore_password : truststorepassword # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] require_client_auth : false Cassandra dedicated port for SSL (optional) # Optionally, you can setup a dedicated port for SSL communication. Update /etc/cassandra/cassandra.yml configuration file on each node: native_transport_port_ssl : 9142 Note By doing so, all SSL communications will be done using this port. Without this parameter, SSL is setup on native_transport_port . (everything is explained in the cassandra.yaml configuration file). Client to node encryption # This guide explains how to secure connection between Cassandra server and Cassandra clients (TheHive). This document doesn\u2019t address communication between Cassandra servers, when a Cassandra cluster contains several nodes. Requirements # The setup requires a valid X509 certificate for the Cassandra service. It must have standard properties of server certificate: key usage: Digital Signature, Non Repudiation, Key Encipherment, Key Agreement Extended Key Usage: TLS Web Server Authentication Cert Type: SSL Server It also must have a \"Subject Alternative Name\" with the identifier (DNS name or/and IP address) of the Cassandra server seen by the client. The format of the certificate file is PKCS12 (file with extention p12). Then create a truststore containing the certificate authority used to generate the certificate for Cassandra. The truststore must be in Java format (JKS). If you CA file is ca.crt, you can generate the truststore file with the following command: keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks This command ask a password for file integrity checking. The command keytool is available in any JDK distribution. Configuring Cassandra # Locate the section client_encryption_options and set the following options: client_encryption_options : enabled : true # If enabled and optional is set to true encrypted and unencrypted connections are handled. optional : false keystore : /pat/to/keystore.jks keystore_password : keystorepassword require_client_auth : false # Set trustore and truststore_password if require_client_auth is true # truststore: conf/.truststore # truststore_password: cassandra # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] Then the service cassandra must be restarted. Configuring TheHive # db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"ip_node_1\", \"ip_node_2\", \"ip_node_3\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" port: 9142 # if alternative port has been set in Cassandra configuration cql { cluster-name: thp keyspace: thehive ssl { enabled: true truststore { location: \"/path/to/truststore.jks\" password: \"truststorepassword\" } } } } Then the service thehive must be restarted.","title":"Cassandra & security"},{"location":"thehive/operations/cassandra-security/#security-in-apache-cassandra","text":"References Internal authentication https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureInternalAuthenticationTOC.html Node to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLNodeToNode.html Client to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLClientToNode.html https://docs.janusgraph.org/basics/configuration-reference/#storagecqlssl","title":"Security in Apache Cassandra"},{"location":"thehive/operations/cassandra-security/#authentication-with-cassandra","text":"Create an account and grant permissions on keyspace CREATE ROLE thehive WITH PASSWORD = 'thehive1234' AND LOGIN = true ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO thehive ; Configure TheHive with the account Update /etc/thehive/application.conf accordingly: db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"xxx.xxx.xxx.xxx\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" cql { cluster-name: thp keyspace: thehive } }","title":"Authentication with Cassandra"},{"location":"thehive/operations/cassandra-security/#cassandra-node-to-node-encryption","text":"This document addresses communication between Cassandra servers, when a Cassandra cluster contains several nodes. server_encryption_options : internode_encryption : all keystore : /path/to/keystore.jks keystore_password : keystorepassword truststore : /path/to/truststore.jks truststore_password : truststorepassword # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] require_client_auth : false","title":"Cassandra node to node encryption"},{"location":"thehive/operations/cassandra-security/#cassandra-dedicated-port-for-ssl-optional","text":"Optionally, you can setup a dedicated port for SSL communication. Update /etc/cassandra/cassandra.yml configuration file on each node: native_transport_port_ssl : 9142 Note By doing so, all SSL communications will be done using this port. Without this parameter, SSL is setup on native_transport_port . (everything is explained in the cassandra.yaml configuration file).","title":"Cassandra dedicated port for SSL (optional)"},{"location":"thehive/operations/cassandra-security/#client-to-node-encryption","text":"This guide explains how to secure connection between Cassandra server and Cassandra clients (TheHive). This document doesn\u2019t address communication between Cassandra servers, when a Cassandra cluster contains several nodes.","title":"Client to node encryption"},{"location":"thehive/operations/cassandra-security/#requirements","text":"The setup requires a valid X509 certificate for the Cassandra service. It must have standard properties of server certificate: key usage: Digital Signature, Non Repudiation, Key Encipherment, Key Agreement Extended Key Usage: TLS Web Server Authentication Cert Type: SSL Server It also must have a \"Subject Alternative Name\" with the identifier (DNS name or/and IP address) of the Cassandra server seen by the client. The format of the certificate file is PKCS12 (file with extention p12). Then create a truststore containing the certificate authority used to generate the certificate for Cassandra. The truststore must be in Java format (JKS). If you CA file is ca.crt, you can generate the truststore file with the following command: keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks This command ask a password for file integrity checking. The command keytool is available in any JDK distribution.","title":"Requirements"},{"location":"thehive/operations/cassandra-security/#configuring-cassandra","text":"Locate the section client_encryption_options and set the following options: client_encryption_options : enabled : true # If enabled and optional is set to true encrypted and unencrypted connections are handled. optional : false keystore : /pat/to/keystore.jks keystore_password : keystorepassword require_client_auth : false # Set trustore and truststore_password if require_client_auth is true # truststore: conf/.truststore # truststore_password: cassandra # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] Then the service cassandra must be restarted.","title":"Configuring Cassandra"},{"location":"thehive/operations/cassandra-security/#configuring-thehive","text":"db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"ip_node_1\", \"ip_node_2\", \"ip_node_3\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" port: 9142 # if alternative port has been set in Cassandra configuration cql { cluster-name: thp keyspace: thehive ssl { enabled: true truststore { location: \"/path/to/truststore.jks\" password: \"truststorepassword\" } } } } Then the service thehive must be restarted.","title":"Configuring TheHive"},{"location":"thehive/operations/fail2ban/","text":"Fail2ban # Adding TheHive into Fail2Ban # Considering TheHive logs sit in /var/log/thehive/application.log and fail2ban configuration is in /etc/fail2ban : Add a filter file in /etc/fail2ban/filter.d named thehive.conf with the following content: [INCLUDES] before = common.conf [Definition] failregex = ^.*- <HOST> (?:POST \\/api\\/login|GET .*) .*returned 401.*$ ignoreregex = Add a jail file in /etc/fail2ban/jail.d/ named thehive.local with the following content: [thehive] enabled = true port = 80,443 filter = thehive action = iptables-multiport[name=thehive, port=\"80,443\"] logpath = /var/log/thehive/application.log maxretry = 5 bantime = 14400 findtime = 1200 This will ban any IP address for 4 hours after 5 failed authentication are identified during a period of 20 min. Reload the configuration with the command fail2ban-client reload Manage banned IP addresses # Review banned IP addresses: fail2ban-client status thehive Unban an IP address: fail2ban-client set thehive unbanip <IP ADDRESS>","title":"Use fail2ban"},{"location":"thehive/operations/fail2ban/#fail2ban","text":"","title":"Fail2ban"},{"location":"thehive/operations/fail2ban/#adding-thehive-into-fail2ban","text":"Considering TheHive logs sit in /var/log/thehive/application.log and fail2ban configuration is in /etc/fail2ban : Add a filter file in /etc/fail2ban/filter.d named thehive.conf with the following content: [INCLUDES] before = common.conf [Definition] failregex = ^.*- <HOST> (?:POST \\/api\\/login|GET .*) .*returned 401.*$ ignoreregex = Add a jail file in /etc/fail2ban/jail.d/ named thehive.local with the following content: [thehive] enabled = true port = 80,443 filter = thehive action = iptables-multiport[name=thehive, port=\"80,443\"] logpath = /var/log/thehive/application.log maxretry = 5 bantime = 14400 findtime = 1200 This will ban any IP address for 4 hours after 5 failed authentication are identified during a period of 20 min. Reload the configuration with the command fail2ban-client reload","title":"Adding TheHive into Fail2Ban"},{"location":"thehive/operations/fail2ban/#manage-banned-ip-addresses","text":"Review banned IP addresses: fail2ban-client status thehive Unban an IP address: fail2ban-client set thehive unbanip <IP ADDRESS>","title":"Manage banned IP addresses"},{"location":"thehive/operations/https/","text":"","title":"Configure HTTPS"},{"location":"thehive/operations/migration/","text":"Migration to TheHive 4 # TheHive 4.x is delivered with a tool to migrate your data from TheHive 3.x. stored in Elasticsearch. Supported versions # The tool is provided with TheHive 4.x. Depending on the target version, the migration tool supports specific versions. Here is a summary: Migrating from Possible target version TheHive 3.4.x TheHive 4.0.x TheHive 3.5.x TheHive 4.1.x Using TheHive 3.4.x You can only migrate to TheHive 4.0.x. After that, an update to TheHive 4.1.x will be possible. Technically, many reasons explain this limitation: A new database format has been introduced with TheHive 3.4.0, Elasticsearch 6.x came with changes in data structure (mostly related to the definition of document relations) So, if you want to migrate your data from TheHive 3 to TheHive 4.0, you are invited to update your current instance to TheHive 3.4.0+ before. Using TheHive 3.5.x You can only process to the migration to TheHive 4.1.x. Using older versions You need to update your database at least to TheHive 3.4.0. How it works # All packages of TheHive4 distributed come with the migration program which can be used to import data from TheHive 3.4.0+. By default, it is installed in /opt/thehive/bin/migrate . Pre-requisite # In order to migrate the data: TheHive 4 must be installed on the system running the migration tool; TheHive4 must be configured ; in particular database and file storage ; The service thehive must be stopped ( service thehive stop ) . This tools must also have access to Elasticsearch database (http://ES:9200) used by TheHive 3, and the configuration file of TheHive 3.x instance. Configuration of TheHive 4 # Warning In TheHive4, users are identified by their email addresses. Thus, a domain will be appended to usernames in order to migrate users from TheHive 3. TheHive 4.0 comes with a default domain named thehive.local . Starting the migration without explicitely specifying a domain name will result in migrating all users with a username formatted like user@thehive.local . Change the default domain name used to import existing users in the configuration file of TheHive4 ( /etc/thehive/application.conf ) ; add or update the setting named auth.defaultUserDomain : auth.defaultUserDomain : \"mydomain.com\" This way, the domain mydomain.com will be appended to user accounts imported from TheHive 3.4+ ( user@mydomain.com ). Run the migration # Prepare, install and configure your new instance of TheHive 4.x by following the associated guides . Once TheHive4 configuration file ( /etc/thehive/application.conf ) is correctly filled you can run migration tool. The program comes with a large set of options: # /opt/thehive/bin/migrate --help TheHive migration tool 4.0.4-1 Usage: migrate [options] -v, --version -h, --help -c, --config <file> global configuration file -i, --input <file> TheHive3 configuration file -o, --output <file> TheHive4 configuration file -d, --drop-database Drop TheHive4 database before migration -m, --main-organisation <organisation> -u, --es-uri http://ip1:port,ip2:port TheHive3 ElasticSearch URI -i, --es-index <index> TheHive3 ElasticSearch index name -a, --es-keepalive <duration> TheHive3 ElasticSearch keepalive -p, --es-pagesize <value> TheHive3 ElasticSearch page size --max-case-age <duration> migrate only cases whose age is less than <duration> --min-case-age <duration> migrate only cases whose age is greater than <duration> --case-from-date <date> migrate only cases created from <date> --case-until-date <date> migrate only cases created until <date> --case-from-number <number> migrate only cases from this case number --case-until-number <number> migrate only cases until this case number --max-alert-age <duration> migrate only alerts whose age is less than <duration> --min-alert-age <duration> migrate only alerts whose age is greater than <duration> --alert-from-date <date> migrate only alerts created from <date> --alert-until-date <date> migrate only alerts created until <date> --include-alert-types <type>,<type>... migrate only alerts with this types --exclude-alert-types <type>,<type>... don't migrate alerts with this types --include-alert-sources <source>,<source>... migrate only alerts with this sources --exclude-alert-sources <source>,<source>... don't migrate alerts with this sources --max-audit-age <duration> migrate only audits whose age is less than <duration> --min-audit-age <duration> migrate only audits whose age is greater than <duration> --audit-from-date <date> migrate only audits created from <date> --audit-until-date <date> migrate only audits created until <date> --include-audit-actions <value> migration only audits with this action (Update, Creation, Delete) --exclude-audit-actions <value> don't migration audits with this action (Update, Creation, Delete) --include-audit-objectTypes <value> migration only audits with this objectType (case, case_artifact, case_task, ...) --exclude-audit-objectTypes <value> don't migration audits with this objectType (case, case_artifact, case_task, ...) Accepted date formats are \"yyyyMMdd[HH[mm[ss]]]\" and \"MMdd\" The Format for duration is: <length> <unit>. Accepted units are: DAY: d, day HOUR: h, hr, hour MINUTE: m, min, minute SECOND: s, sec, second MILLISECOND: ms, milli, millisecond Most of these options are filters you can apply to the program. For example, you could decide to import only some of the Cases/Alerts from your old instance: Import Cases/Alerts not older than X days/hours, Import Cases/Alerts with the ID number, Import part of Audit trail ... Taking the assumption that you are migrating a database hosted in a remote server, with TheHive 3, a basic command line to migrate data to a new instance will be like: /opt/thehive/bin/migrate \\ --output /etc/thehive/application.conf \\ --main-organisation myOrganisation \\ --es-uri http://ELASTICSEARCH_IP_ADDRESS:9200 \\ --es-index the_hive with: Option Description --output specifies the configuration file of TheHive 4.0 (the one that has previously been configured with at least, the database and file storage ) --main-organisation specifies the Organisation named myOrganisation to create during the migration. The tool will then create Users, Cases and Alerts from TheHive3 under that organisation; --es-uri specifies the URL of the Elasticsearch server. If using authentication on Elasticsearch, --input option with a configuration file for TheHive3 is required --es-index specifies the index used in Elasticsearch. Warning The migration process can be very long, from several hours to several days, depending on the volume of data to migrate. We highly recommand to not start the application during the migration. Using authentication on Cassandra # if you are using a dedicated account on Cassandra to access TheHive 4 data, this user must have permissions to create keyspaces on the database: GRANT CREATE on ALL KEYSPACES to username ; Migration logs # The migration tool generates some logs during the process. By default, every 10 sec. a log is generated with information regarding the situation of the migration: [info] o.t.t.m.Migrate - [Migrate cases and alerts] CaseTemplate/Task:32 Organisation:1/1 Case/Task:160/201 Case:31/52 Job:103/138 ObservableType:3/17 Alert:25/235 Audit:3207/2986 CaseTemplate:6/6 Alert/Observable:700(52ms) Case/Observable:1325/1665 User:9/9 CustomField:13/13 Case/Task/Log:20/27 Warning Numbers of Observables, Cases and others are estimations and not a definite value as computing these number can be very tedious. Files from MISP imported with TheHive 2.13 and earlier It is important to notice that migrating Cases/Alerts containing MISP event that were imported with TheHive 2.13 ( Sept 2017 ) or older, will cause observable files not being imported in TheHive 4. Indeed, until this version, TheHive referenced the file to the AttributeId in MISP and was not automatically downloaded. It then could generate a log like this: [warn] o.t.t.m.t.Input - Pre 2.13 file observables are ignored in MISP alert ffa3a8503ab0cd4f99fc6937a8e9b827 Starting TheHive 4 # Once the migration process is sucessfully completed, TheHive4 can be started. Warning During the first start data are indexed and service is not available ; this can take some time. Do not stop or restart the service at this time.","title":"Migration from TheHive 3.x"},{"location":"thehive/operations/migration/#migration-to-thehive-4","text":"TheHive 4.x is delivered with a tool to migrate your data from TheHive 3.x. stored in Elasticsearch.","title":"Migration to TheHive 4"},{"location":"thehive/operations/migration/#supported-versions","text":"The tool is provided with TheHive 4.x. Depending on the target version, the migration tool supports specific versions. Here is a summary: Migrating from Possible target version TheHive 3.4.x TheHive 4.0.x TheHive 3.5.x TheHive 4.1.x Using TheHive 3.4.x You can only migrate to TheHive 4.0.x. After that, an update to TheHive 4.1.x will be possible. Technically, many reasons explain this limitation: A new database format has been introduced with TheHive 3.4.0, Elasticsearch 6.x came with changes in data structure (mostly related to the definition of document relations) So, if you want to migrate your data from TheHive 3 to TheHive 4.0, you are invited to update your current instance to TheHive 3.4.0+ before. Using TheHive 3.5.x You can only process to the migration to TheHive 4.1.x. Using older versions You need to update your database at least to TheHive 3.4.0.","title":"Supported versions"},{"location":"thehive/operations/migration/#how-it-works","text":"All packages of TheHive4 distributed come with the migration program which can be used to import data from TheHive 3.4.0+. By default, it is installed in /opt/thehive/bin/migrate .","title":"How it works"},{"location":"thehive/operations/migration/#pre-requisite","text":"In order to migrate the data: TheHive 4 must be installed on the system running the migration tool; TheHive4 must be configured ; in particular database and file storage ; The service thehive must be stopped ( service thehive stop ) . This tools must also have access to Elasticsearch database (http://ES:9200) used by TheHive 3, and the configuration file of TheHive 3.x instance.","title":"Pre-requisite"},{"location":"thehive/operations/migration/#configuration-of-thehive-4","text":"Warning In TheHive4, users are identified by their email addresses. Thus, a domain will be appended to usernames in order to migrate users from TheHive 3. TheHive 4.0 comes with a default domain named thehive.local . Starting the migration without explicitely specifying a domain name will result in migrating all users with a username formatted like user@thehive.local . Change the default domain name used to import existing users in the configuration file of TheHive4 ( /etc/thehive/application.conf ) ; add or update the setting named auth.defaultUserDomain : auth.defaultUserDomain : \"mydomain.com\" This way, the domain mydomain.com will be appended to user accounts imported from TheHive 3.4+ ( user@mydomain.com ).","title":"Configuration of TheHive 4"},{"location":"thehive/operations/migration/#run-the-migration","text":"Prepare, install and configure your new instance of TheHive 4.x by following the associated guides . Once TheHive4 configuration file ( /etc/thehive/application.conf ) is correctly filled you can run migration tool. The program comes with a large set of options: # /opt/thehive/bin/migrate --help TheHive migration tool 4.0.4-1 Usage: migrate [options] -v, --version -h, --help -c, --config <file> global configuration file -i, --input <file> TheHive3 configuration file -o, --output <file> TheHive4 configuration file -d, --drop-database Drop TheHive4 database before migration -m, --main-organisation <organisation> -u, --es-uri http://ip1:port,ip2:port TheHive3 ElasticSearch URI -i, --es-index <index> TheHive3 ElasticSearch index name -a, --es-keepalive <duration> TheHive3 ElasticSearch keepalive -p, --es-pagesize <value> TheHive3 ElasticSearch page size --max-case-age <duration> migrate only cases whose age is less than <duration> --min-case-age <duration> migrate only cases whose age is greater than <duration> --case-from-date <date> migrate only cases created from <date> --case-until-date <date> migrate only cases created until <date> --case-from-number <number> migrate only cases from this case number --case-until-number <number> migrate only cases until this case number --max-alert-age <duration> migrate only alerts whose age is less than <duration> --min-alert-age <duration> migrate only alerts whose age is greater than <duration> --alert-from-date <date> migrate only alerts created from <date> --alert-until-date <date> migrate only alerts created until <date> --include-alert-types <type>,<type>... migrate only alerts with this types --exclude-alert-types <type>,<type>... don't migrate alerts with this types --include-alert-sources <source>,<source>... migrate only alerts with this sources --exclude-alert-sources <source>,<source>... don't migrate alerts with this sources --max-audit-age <duration> migrate only audits whose age is less than <duration> --min-audit-age <duration> migrate only audits whose age is greater than <duration> --audit-from-date <date> migrate only audits created from <date> --audit-until-date <date> migrate only audits created until <date> --include-audit-actions <value> migration only audits with this action (Update, Creation, Delete) --exclude-audit-actions <value> don't migration audits with this action (Update, Creation, Delete) --include-audit-objectTypes <value> migration only audits with this objectType (case, case_artifact, case_task, ...) --exclude-audit-objectTypes <value> don't migration audits with this objectType (case, case_artifact, case_task, ...) Accepted date formats are \"yyyyMMdd[HH[mm[ss]]]\" and \"MMdd\" The Format for duration is: <length> <unit>. Accepted units are: DAY: d, day HOUR: h, hr, hour MINUTE: m, min, minute SECOND: s, sec, second MILLISECOND: ms, milli, millisecond Most of these options are filters you can apply to the program. For example, you could decide to import only some of the Cases/Alerts from your old instance: Import Cases/Alerts not older than X days/hours, Import Cases/Alerts with the ID number, Import part of Audit trail ... Taking the assumption that you are migrating a database hosted in a remote server, with TheHive 3, a basic command line to migrate data to a new instance will be like: /opt/thehive/bin/migrate \\ --output /etc/thehive/application.conf \\ --main-organisation myOrganisation \\ --es-uri http://ELASTICSEARCH_IP_ADDRESS:9200 \\ --es-index the_hive with: Option Description --output specifies the configuration file of TheHive 4.0 (the one that has previously been configured with at least, the database and file storage ) --main-organisation specifies the Organisation named myOrganisation to create during the migration. The tool will then create Users, Cases and Alerts from TheHive3 under that organisation; --es-uri specifies the URL of the Elasticsearch server. If using authentication on Elasticsearch, --input option with a configuration file for TheHive3 is required --es-index specifies the index used in Elasticsearch. Warning The migration process can be very long, from several hours to several days, depending on the volume of data to migrate. We highly recommand to not start the application during the migration.","title":"Run the migration"},{"location":"thehive/operations/migration/#using-authentication-on-cassandra","text":"if you are using a dedicated account on Cassandra to access TheHive 4 data, this user must have permissions to create keyspaces on the database: GRANT CREATE on ALL KEYSPACES to username ;","title":"Using authentication on Cassandra"},{"location":"thehive/operations/migration/#migration-logs","text":"The migration tool generates some logs during the process. By default, every 10 sec. a log is generated with information regarding the situation of the migration: [info] o.t.t.m.Migrate - [Migrate cases and alerts] CaseTemplate/Task:32 Organisation:1/1 Case/Task:160/201 Case:31/52 Job:103/138 ObservableType:3/17 Alert:25/235 Audit:3207/2986 CaseTemplate:6/6 Alert/Observable:700(52ms) Case/Observable:1325/1665 User:9/9 CustomField:13/13 Case/Task/Log:20/27 Warning Numbers of Observables, Cases and others are estimations and not a definite value as computing these number can be very tedious. Files from MISP imported with TheHive 2.13 and earlier It is important to notice that migrating Cases/Alerts containing MISP event that were imported with TheHive 2.13 ( Sept 2017 ) or older, will cause observable files not being imported in TheHive 4. Indeed, until this version, TheHive referenced the file to the AttributeId in MISP and was not automatically downloaded. It then could generate a log like this: [warn] o.t.t.m.t.Input - Pre 2.13 file observables are ignored in MISP alert ffa3a8503ab0cd4f99fc6937a8e9b827","title":"Migration logs"},{"location":"thehive/operations/migration/#starting-thehive-4","text":"Once the migration process is sucessfully completed, TheHive4 can be started. Warning During the first start data are indexed and service is not available ; this can take some time. Do not stop or restart the service at this time.","title":"Starting TheHive 4"},{"location":"thehive/operations/troubleshooting/","text":"Troubleshooting # For some issues, we need extra information in logs to troubleshoot and understand to root causes. To gather and share this, please read carefully and follow these steps. Warning ENABLING TRACE LOGS HAS SIGNIFICANT IMPACT ON PERFORMANCES. DO NOT ENABLE IT ON PRODUCTION SERVERS. Stop TheHive service and ensure it is stopped # service thehive stop Ensure the service is stopped with the following command: service thehive status Renew application.log file # in /var/log/thehive move the file application.log to application.log.bak mv /var/log/thehive/application.log /var/log/thehive/application.log.bak Update log configuration # Edit the file /etc/thehive/logback.xml . Look for the line containing <logger name=\"org.thp\" level=\"INFO\"/> and update it to have following lines: [..] <logger name= \"org.thp\" level= \"TRACE\" /> [..] Save the file. Restart the service # service thehive start A new log file /var/log/thehive/application.log should be created and filed with a huge amount of logs. Wait for the issue to appear and/or the application stop. Save the logs # Copy the log file in a safe place. cp /var/log/thehive/application.log /root Share it with us # Create an issue on Github and please share context and symptoms with the log file. Please add information regarding: Context: instance (single node/cluster, backend type, index engine) System: Operating System, amount of RAM, #CPU for each server/node Symptoms: what you did, how you you come to this situation, what happened The log file with traces Revert # To get back a to normal log configuration, stop thehive, update logback.xml file with the previous configuration, and restart the application.","title":"Troubleshooting"},{"location":"thehive/operations/troubleshooting/#troubleshooting","text":"For some issues, we need extra information in logs to troubleshoot and understand to root causes. To gather and share this, please read carefully and follow these steps. Warning ENABLING TRACE LOGS HAS SIGNIFICANT IMPACT ON PERFORMANCES. DO NOT ENABLE IT ON PRODUCTION SERVERS.","title":"Troubleshooting"},{"location":"thehive/operations/troubleshooting/#stop-thehive-service-and-ensure-it-is-stopped","text":"service thehive stop Ensure the service is stopped with the following command: service thehive status","title":"Stop TheHive service and ensure it is stopped"},{"location":"thehive/operations/troubleshooting/#renew-applicationlog-file","text":"in /var/log/thehive move the file application.log to application.log.bak mv /var/log/thehive/application.log /var/log/thehive/application.log.bak","title":"Renew application.log file"},{"location":"thehive/operations/troubleshooting/#update-log-configuration","text":"Edit the file /etc/thehive/logback.xml . Look for the line containing <logger name=\"org.thp\" level=\"INFO\"/> and update it to have following lines: [..] <logger name= \"org.thp\" level= \"TRACE\" /> [..] Save the file.","title":"Update log configuration"},{"location":"thehive/operations/troubleshooting/#restart-the-service","text":"service thehive start A new log file /var/log/thehive/application.log should be created and filed with a huge amount of logs. Wait for the issue to appear and/or the application stop.","title":"Restart the service"},{"location":"thehive/operations/troubleshooting/#save-the-logs","text":"Copy the log file in a safe place. cp /var/log/thehive/application.log /root","title":"Save the logs"},{"location":"thehive/operations/troubleshooting/#share-it-with-us","text":"Create an issue on Github and please share context and symptoms with the log file. Please add information regarding: Context: instance (single node/cluster, backend type, index engine) System: Operating System, amount of RAM, #CPU for each server/node Symptoms: what you did, how you you come to this situation, what happened The log file with traces","title":"Share it with us"},{"location":"thehive/operations/troubleshooting/#revert","text":"To get back a to normal log configuration, stop thehive, update logback.xml file with the previous configuration, and restart the application.","title":"Revert"},{"location":"thehive/operations/update/","text":"Update guides # Update from TheHive 4.0.x to TheHive 4.1.0 # TheHive 4.1.0 comes with an updated application stack, with new components dedicated to performance improvement. TheHive 4.1.0 requires the usage of a dedicated index engine to manages indexed data. As a result, the minimum configuration required has been updated: If you are a new user of TheHive, follow the installation and configuration guide . If you are an existing user of TheHive 4.0.x, an index engine should be configured alongside the database. And wether you are using a standalone server or a cluster, the solution to implement and the configuration to update are different. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch . Updating a standalone server # In this case, a Lucene can be used. TheHive 4.1.0 comes with its Lucene engine. The configuration of TheHive, can be updated like this: Create a dedicated folder for indexes (for example /opt/thp/thehive/index ). This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Add the index configuration in the db.janusgraph part: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Restart TheHive service thehive restart Once TheHive configuration is updated and restarted, new indexes are created during the start-up phase. Warning The start-up phase of TheHive and the indexes creation can take a certain amount if time. This phase will be quicker once indexes exist. Updating a cluster # In this case, a Elasticsearch should be used, as all nodes should have access to the same index. Once your Elasticsearch instance is up and running, The configuration of TheHive can be updated like this: Here is an example of configuration, use your IP address/hostnames. ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive } } } } In this configuration, all TheHive nodes should have the same configuration. Restart all nodes of the cluster. Info the cluster makes it work out ; one of the nodes manage the indexing process while others are waiting for it to be ready. More information # More information about the configuration of database and indexes can be found in the dedicated configuration guide","title":"Howto update"},{"location":"thehive/operations/update/#update-guides","text":"","title":"Update guides"},{"location":"thehive/operations/update/#update-from-thehive-40x-to-thehive-410","text":"TheHive 4.1.0 comes with an updated application stack, with new components dedicated to performance improvement. TheHive 4.1.0 requires the usage of a dedicated index engine to manages indexed data. As a result, the minimum configuration required has been updated: If you are a new user of TheHive, follow the installation and configuration guide . If you are an existing user of TheHive 4.0.x, an index engine should be configured alongside the database. And wether you are using a standalone server or a cluster, the solution to implement and the configuration to update are different. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch .","title":"Update from TheHive 4.0.x to TheHive 4.1.0"},{"location":"thehive/operations/update/#updating-a-standalone-server","text":"In this case, a Lucene can be used. TheHive 4.1.0 comes with its Lucene engine. The configuration of TheHive, can be updated like this: Create a dedicated folder for indexes (for example /opt/thp/thehive/index ). This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Add the index configuration in the db.janusgraph part: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Restart TheHive service thehive restart Once TheHive configuration is updated and restarted, new indexes are created during the start-up phase. Warning The start-up phase of TheHive and the indexes creation can take a certain amount if time. This phase will be quicker once indexes exist.","title":"Updating a standalone server"},{"location":"thehive/operations/update/#updating-a-cluster","text":"In this case, a Elasticsearch should be used, as all nodes should have access to the same index. Once your Elasticsearch instance is up and running, The configuration of TheHive can be updated like this: Here is an example of configuration, use your IP address/hostnames. ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive } } } } In this configuration, all TheHive nodes should have the same configuration. Restart all nodes of the cluster. Info the cluster makes it work out ; one of the nodes manage the indexing process while others are waiting for it to be ready.","title":"Updating a cluster"},{"location":"thehive/operations/update/#more-information","text":"More information about the configuration of database and indexes can be found in the dedicated configuration guide","title":"More information"},{"location":"thehive/user-guides/","text":"User guides # Get a Quick start with TheHive or follow the guides bellow for more details: For administrators # Administrators as users defined in the admin organisation, created by default in TheHive. Administators have the responsibility of managing the platform by defining organisations and all the platform data available for to all the organisations. Manage organisations Manage users Manage profiles, roles and permissions Manage Custom fields Manage Observable types Manage Analyzers templates Manage Tags & taxonomies Manage Tactics, Techniques & Procedures Platform Status For organisation managers # Organisation managers are users belonging to any organisation other than admin and having one of the following permissions to manage users, case template, custom tags and UI configuration. TheHive comes with a default role for organisation managers, called org-admin . Organisations, users and sharing Manage users Manage Case templates Manage UI configurations For Analysts # Analysts are user belonging to any organisation other than admin without organisation management permissions. User settings Create and manage Alerts Create and manage Cases Use Custom fields Use tags Use TTPs Run Responders Run Analyzers Share a Case Merge a Case Export a Case Create Dashboards","title":"User guides"},{"location":"thehive/user-guides/#user-guides","text":"Get a Quick start with TheHive or follow the guides bellow for more details:","title":"User guides"},{"location":"thehive/user-guides/#for-administrators","text":"Administrators as users defined in the admin organisation, created by default in TheHive. Administators have the responsibility of managing the platform by defining organisations and all the platform data available for to all the organisations. Manage organisations Manage users Manage profiles, roles and permissions Manage Custom fields Manage Observable types Manage Analyzers templates Manage Tags & taxonomies Manage Tactics, Techniques & Procedures Platform Status","title":"For administrators"},{"location":"thehive/user-guides/#for-organisation-managers","text":"Organisation managers are users belonging to any organisation other than admin and having one of the following permissions to manage users, case template, custom tags and UI configuration. TheHive comes with a default role for organisation managers, called org-admin . Organisations, users and sharing Manage users Manage Case templates Manage UI configurations","title":"For organisation managers"},{"location":"thehive/user-guides/#for-analysts","text":"Analysts are user belonging to any organisation other than admin without organisation management permissions. User settings Create and manage Alerts Create and manage Cases Use Custom fields Use tags Use TTPs Run Responders Run Analyzers Share a Case Merge a Case Export a Case Create Dashboards","title":"For Analysts"},{"location":"thehive/user-guides/quick-start/","text":"Quick start with TheHive # TL;DR # Default administrator account: admin@thehive.local / secret Login with default account Create an organisaton Create a user account Before starting # Starting from TheHive 4.0-RC1, an email address is requested, and is mandatory to register a new user, and to log in the application. Intialize TheHive 4 # This version of TheHive comes with a big improvement: Multi-tenancy support Fine grained permissions Customized user profiles (a set of permissions) After TheHive installation, a default organisation called \"admin\" is created and contains the initial default super administrator user, having a profile called \"admin\" . We will discuss the user profiles later, but note that the \"admin\" user has all the administration permissions like: create organisation define profiles define observable types define custom fields Members of \" admin \" organisation are dedicated to user accounts in charge of administrating the solution. The initial user has the following credentials: login: admin@thehive.local password: secret This default group cannot create and own Cases or any other related objects like Tasks or Observables . First login # When TheHive starts the first time, you need to login using the credential of the \"admin\" user indicated above ( admin@thehive.local / secret ), and you will be redirected to the administration home page: List of organisations . Note that this organisation cannot be deleted . Possible operations for the \"admin\" users (members of the \"admin\" organisation) are accessible from the \"Admin\" menu located on the header bar: admin Organisation cannot manage Cases. so let's create an organisation and its users. Create an organisation # The initial action that a super admin have to make is to create the organisations (tenants) that will use TheHive to deal with incident response. From the \"List of organisations\" page, hit the \"New Organisation\" button to open the organisation dialog. The organisation name is required and must be unique. Hit \"Save\" to confirm. Create a user # Once you have created an organisation you can open its details page by clicking \"Configure\". This organisation details page, for users with \"admin\" profile allows managing organisation users only. You can see on this page: The details of the organisation: name, description, the user that created it A tab to manage users: Create them Edit their password, api key Edit their profile Reset their 2FA settings Lock and delete them To create a user, just click the \"Create new user\" button, that opens the user creation dialog. Note: The \"Profile\" field will be populated by the profiles that can be assigned to organisation users only. (Administration profiles will not be listed). The first user you must create for each organisation, should be a user with \"org-admin\" profile. That profile allows all the operations within an organisation. A user with \"org-admin\" profile will be able to connect and configure its organisation by at least: Creating other users Creating case templates Once you have created the users, you can set their passwords (they will be able to change them from their own account page). To do that, click on the \"New password\" button on the corresponding user's row and then hit ENTER or click the green check button: Login as org-admin user # Once the user is created, (s)he can connect to TheHive and start using it based on the profile. Note that users with \"org-admin\" profile have an \"Organisation\" menu in the right side corner of the header bar given access to the organisation configuration page with and additionnal tab for case template management. Now that the organisation and users are created, let's define custom fields and then use them to define case templates.","title":"Quick start with TheHive"},{"location":"thehive/user-guides/quick-start/#quick-start-with-thehive","text":"","title":"Quick start with TheHive"},{"location":"thehive/user-guides/quick-start/#tldr","text":"Default administrator account: admin@thehive.local / secret Login with default account Create an organisaton Create a user account","title":"TL;DR"},{"location":"thehive/user-guides/quick-start/#before-starting","text":"Starting from TheHive 4.0-RC1, an email address is requested, and is mandatory to register a new user, and to log in the application.","title":"Before starting"},{"location":"thehive/user-guides/quick-start/#intialize-thehive-4","text":"This version of TheHive comes with a big improvement: Multi-tenancy support Fine grained permissions Customized user profiles (a set of permissions) After TheHive installation, a default organisation called \"admin\" is created and contains the initial default super administrator user, having a profile called \"admin\" . We will discuss the user profiles later, but note that the \"admin\" user has all the administration permissions like: create organisation define profiles define observable types define custom fields Members of \" admin \" organisation are dedicated to user accounts in charge of administrating the solution. The initial user has the following credentials: login: admin@thehive.local password: secret This default group cannot create and own Cases or any other related objects like Tasks or Observables .","title":"Intialize TheHive 4"},{"location":"thehive/user-guides/quick-start/#first-login","text":"When TheHive starts the first time, you need to login using the credential of the \"admin\" user indicated above ( admin@thehive.local / secret ), and you will be redirected to the administration home page: List of organisations . Note that this organisation cannot be deleted . Possible operations for the \"admin\" users (members of the \"admin\" organisation) are accessible from the \"Admin\" menu located on the header bar: admin Organisation cannot manage Cases. so let's create an organisation and its users.","title":"First login"},{"location":"thehive/user-guides/quick-start/#create-an-organisation","text":"The initial action that a super admin have to make is to create the organisations (tenants) that will use TheHive to deal with incident response. From the \"List of organisations\" page, hit the \"New Organisation\" button to open the organisation dialog. The organisation name is required and must be unique. Hit \"Save\" to confirm.","title":"Create an organisation"},{"location":"thehive/user-guides/quick-start/#create-a-user","text":"Once you have created an organisation you can open its details page by clicking \"Configure\". This organisation details page, for users with \"admin\" profile allows managing organisation users only. You can see on this page: The details of the organisation: name, description, the user that created it A tab to manage users: Create them Edit their password, api key Edit their profile Reset their 2FA settings Lock and delete them To create a user, just click the \"Create new user\" button, that opens the user creation dialog. Note: The \"Profile\" field will be populated by the profiles that can be assigned to organisation users only. (Administration profiles will not be listed). The first user you must create for each organisation, should be a user with \"org-admin\" profile. That profile allows all the operations within an organisation. A user with \"org-admin\" profile will be able to connect and configure its organisation by at least: Creating other users Creating case templates Once you have created the users, you can set their passwords (they will be able to change them from their own account page). To do that, click on the \"New password\" button on the corresponding user's row and then hit ENTER or click the green check button:","title":"Create a user"},{"location":"thehive/user-guides/quick-start/#login-as-org-admin-user","text":"Once the user is created, (s)he can connect to TheHive and start using it based on the profile. Note that users with \"org-admin\" profile have an \"Organisation\" menu in the right side corner of the header bar given access to the organisation configuration page with and additionnal tab for case template management. Now that the organisation and users are created, let's define custom fields and then use them to define case templates.","title":"Login as org-admin user"},{"location":"thehive/user-guides/administrators/analyzer-templates/","text":"Manage analyzer template # Before TheHive4, we used to call them Report templates and we allowed two types of templates: Short reports: used to customise the display of analysis report summary Long reports: used to customise the rendering of the raw report of a given analyzer report Starting from TheHive4, short reports have been removed, and TheHive will display the analysis summary the same way for all analyzers: display a tag using taxonomies and level color. List analyzer templates # The management page is accessible from the header menu through the Admin > Analyzer templates menu and required a use with the manageAnalyzerTemplate permission (refer to Profiles and permissions ). Note that analyzer templates are global and common to all the organisations. Analyzer templates are still customisable via the UI and can also be imported. Import analyzer templates # TheHive Project provides a set of analyzer templates (we use the same report-templates.zip archive for backward compatibility reasons). The template archive is available at https://download.thehive-project.org/report-templates.zip . To import the zip file, click on the Import templates , this opens the import dialog. Drop the zip files or click to select it from your storage and finally click Yes, import template archive .","title":"Manage analyzer template"},{"location":"thehive/user-guides/administrators/analyzer-templates/#manage-analyzer-template","text":"Before TheHive4, we used to call them Report templates and we allowed two types of templates: Short reports: used to customise the display of analysis report summary Long reports: used to customise the rendering of the raw report of a given analyzer report Starting from TheHive4, short reports have been removed, and TheHive will display the analysis summary the same way for all analyzers: display a tag using taxonomies and level color.","title":"Manage analyzer template"},{"location":"thehive/user-guides/administrators/analyzer-templates/#list-analyzer-templates","text":"The management page is accessible from the header menu through the Admin > Analyzer templates menu and required a use with the manageAnalyzerTemplate permission (refer to Profiles and permissions ). Note that analyzer templates are global and common to all the organisations. Analyzer templates are still customisable via the UI and can also be imported.","title":"List analyzer templates"},{"location":"thehive/user-guides/administrators/analyzer-templates/#import-analyzer-templates","text":"TheHive Project provides a set of analyzer templates (we use the same report-templates.zip archive for backward compatibility reasons). The template archive is available at https://download.thehive-project.org/report-templates.zip . To import the zip file, click on the Import templates , this opens the import dialog. Drop the zip files or click to select it from your storage and finally click Yes, import template archive .","title":"Import analyzer templates"},{"location":"thehive/user-guides/administrators/custom-fields/","text":"Manage custom fields # In TheHive 4, Metrics have been removed. Why? Because metrics are simply, numeric custom fields. To manage Custom fields you need to login as an \"admin\" user (Member of the \"admin\" organisation) that has a profile including the manageCustomField permission (refer to Profiles and permissions for detailed information). The default \"admin\" user has that permission. \u26a0\ufe0f Note Custom fields are global to all the organisation. When installing TheHive, the list of custom fields is initially empty, administrators have to populate it. To create a custom field, click on the \"Add custom field\" button that opens a dialog: You need to set: a display name a name (automatically pre-filled by the UI based on the display name) a description a type: on of string , intger , booleen , date and float (new type added by TheHive 4) possible values (not available for date and boolean fields) wether the field is mandatory or not (will be prompted when you close a Case without setting its value) Once the custom field is created, you can edit its details or delete it: Only unused custom fields can be removed:","title":"Manage custom fields"},{"location":"thehive/user-guides/administrators/custom-fields/#manage-custom-fields","text":"In TheHive 4, Metrics have been removed. Why? Because metrics are simply, numeric custom fields. To manage Custom fields you need to login as an \"admin\" user (Member of the \"admin\" organisation) that has a profile including the manageCustomField permission (refer to Profiles and permissions for detailed information). The default \"admin\" user has that permission. \u26a0\ufe0f Note Custom fields are global to all the organisation. When installing TheHive, the list of custom fields is initially empty, administrators have to populate it. To create a custom field, click on the \"Add custom field\" button that opens a dialog: You need to set: a display name a name (automatically pre-filled by the UI based on the display name) a description a type: on of string , intger , booleen , date and float (new type added by TheHive 4) possible values (not available for date and boolean fields) wether the field is mandatory or not (will be prompted when you close a Case without setting its value) Once the custom field is created, you can edit its details or delete it: Only unused custom fields can be removed:","title":"Manage custom fields"},{"location":"thehive/user-guides/administrators/observable-types/","text":"Manage observable types # In TheHive4, we have big plans for observable types, since we plan to support observable templates insteand of a simple string value. But this feature is planned for the future. In TheHive 4.0 observable datatype are common to all the organisation, and manageable by administrators (members of the \"admin\" organisation). The management page is accessible from the header menu through the Admin > Observable types menu and required a use with the manageObservableTemplate permission (refer to Profiles and permissions ).","title":"Manage observable types"},{"location":"thehive/user-guides/administrators/observable-types/#manage-observable-types","text":"In TheHive4, we have big plans for observable types, since we plan to support observable templates insteand of a simple string value. But this feature is planned for the future. In TheHive 4.0 observable datatype are common to all the organisation, and manageable by administrators (members of the \"admin\" organisation). The management page is accessible from the header menu through the Admin > Observable types menu and required a use with the manageObservableTemplate permission (refer to Profiles and permissions ).","title":"Manage observable types"},{"location":"thehive/user-guides/administrators/organisations/","text":"","title":"Organisations"},{"location":"thehive/user-guides/administrators/plateform-status/","text":"Plateform status # With the database update and the new indexing engine, a health status page has been introduced. This page not only displays health status for indexes, but also for global data in the database. Indeed, when an element is created, no duplicate should exist, a control is then processed. This is sumarised in the Data health status part. Accessing to this page requires to be admin of the plateform, or at least, have managePlateform permission. Health status page # This plage can be accessed in the Admin organisation view. Open the Admin menu and click on Plateform status Note When opening the page, the indexes status can take a while. with: List of indexes and their health status. Database objects number and Index objects number should be equal for a good health status When the status is Error , proceed to reindex List of data types health status in the database and their duplicate state Process for a duplicate check on a specific data type if status is XXX warning XXX Quand un \u00e9l\u00e9ment est cr\u00e9\u00e9 et qu'il ne doit pas y avoir de doublon (caseNumber, alert type+source+sourceRef, customField, ...), un contr\u00f4le est r\u00e9alis\u00e9 (duplicationCheck). Le r\u00e9sultat de ces contr\u00f4les sont dans la cl\u00e9 duplicateStats avec les champs : - last pour le r\u00e9sultat du dernier check (avec le nombre de doublon et la dur\u00e9e du check en milliseconds), - lastDate pour la date du dernier check - global est l'aggr\u00e9gation de tous les checks depuis le lancement de TH (avec le nombre d'iterations) Les checks sont d\u00e9clench\u00e9s de fa\u00e7on \u00e0 limiter le nombre d'iterations quand on fait plusieurs ajouts dans un courte p\u00e9riode. needCheck indique qu'un check est en attente et duplicateTimer que je check est programm\u00e9. En plus des contr\u00f4les de doublon, il y a une multitude d'autres contr\u00f4les (un share doit \u00eatre attach\u00e9 \u00e0 une seule orga et un seule case, une alerte ne peut \u00eatre import\u00e9 qu'une fois, ...). Ces checks sont r\u00e9alis\u00e9s toutes les 6 heures par d\u00e9faut. On retrouve le r\u00e9sultat de ces checks dans globalStats avec la m\u00eame logique last, lastDate et global. Par contre, les cl\u00e9s ne sont pas duplicate mais un ensemble de valeurs propres \u00e0 chaque type de contr\u00f4le (orphan, extraOrganisation, nonExistentOrganisation, missingOrganisation, ...). Chaque valeur me permettent d'identifier la situation recontr\u00e9e.","title":"Plateform status"},{"location":"thehive/user-guides/administrators/plateform-status/#plateform-status","text":"With the database update and the new indexing engine, a health status page has been introduced. This page not only displays health status for indexes, but also for global data in the database. Indeed, when an element is created, no duplicate should exist, a control is then processed. This is sumarised in the Data health status part. Accessing to this page requires to be admin of the plateform, or at least, have managePlateform permission.","title":"Plateform status"},{"location":"thehive/user-guides/administrators/plateform-status/#health-status-page","text":"This plage can be accessed in the Admin organisation view. Open the Admin menu and click on Plateform status Note When opening the page, the indexes status can take a while. with: List of indexes and their health status. Database objects number and Index objects number should be equal for a good health status When the status is Error , proceed to reindex List of data types health status in the database and their duplicate state Process for a duplicate check on a specific data type if status is XXX warning XXX Quand un \u00e9l\u00e9ment est cr\u00e9\u00e9 et qu'il ne doit pas y avoir de doublon (caseNumber, alert type+source+sourceRef, customField, ...), un contr\u00f4le est r\u00e9alis\u00e9 (duplicationCheck). Le r\u00e9sultat de ces contr\u00f4les sont dans la cl\u00e9 duplicateStats avec les champs : - last pour le r\u00e9sultat du dernier check (avec le nombre de doublon et la dur\u00e9e du check en milliseconds), - lastDate pour la date du dernier check - global est l'aggr\u00e9gation de tous les checks depuis le lancement de TH (avec le nombre d'iterations) Les checks sont d\u00e9clench\u00e9s de fa\u00e7on \u00e0 limiter le nombre d'iterations quand on fait plusieurs ajouts dans un courte p\u00e9riode. needCheck indique qu'un check est en attente et duplicateTimer que je check est programm\u00e9. En plus des contr\u00f4les de doublon, il y a une multitude d'autres contr\u00f4les (un share doit \u00eatre attach\u00e9 \u00e0 une seule orga et un seule case, une alerte ne peut \u00eatre import\u00e9 qu'une fois, ...). Ces checks sont r\u00e9alis\u00e9s toutes les 6 heures par d\u00e9faut. On retrouve le r\u00e9sultat de ces checks dans globalStats avec la m\u00eame logique last, lastDate et global. Par contre, les cl\u00e9s ne sont pas duplicate mais un ensemble de valeurs propres \u00e0 chaque type de contr\u00f4le (orphan, extraOrganisation, nonExistentOrganisation, missingOrganisation, ...). Chaque valeur me permettent d'identifier la situation recontr\u00e9e.","title":"Health status page"},{"location":"thehive/user-guides/administrators/profiles/","text":"User Profiles management # User profiles is a new concept introduced by TheHive4 and coming from the support of role based access control aka RBAC. In TheHive4, users will be assigned a Profile , within an Organisation . A Profile is composed by a set of predefined permissions. Permissions # A Profile is a set of permissions attached to a User and an Organisation . It defines what the user can do on an object hold by the organisation. TheHive has a finite list of permissions: manageOrganisation (1) : the user can create , update an organisation manageConfig (1): the user can update configuration manageProfile (1): the user can create , update and delete profiles manageTag (1): the user can create , update and delete tags manageCustomField (1): the user can create , update and delete custom fields manageCase : the user can create , update and delete cases manageObservable : the user can create , update and delete observables manageAlert : the user can create , update and import alerts manageUser : the user can create , update and delete users manageCaseTemplate : the user can create , update and delete case template manageTask : the user can create , update and delete tasks manageShare : the user can share case, task and observable with other organisation manageAnalyse (2): the user can execute analyse manageAction (2): the user can execute actions manageAnalyzerTemplate (2): the user can create , update and delete analyzer template (previously named report template) (1) Organisations, configuration, profiles and tags are global objects. The related permissions are effective only on \u201cadmin\u201d organisation. (2) Actions, analysis and template is available only if Cortex connector is enabled Note Read information doesn\u2019t require specific permission. By default, users in an organisation can see all data shared with that organisation (cf. shares, discussed in Organisations,Users and sharing ). Profiles # We distinguish two types of profiles: Administration Profiles Organisation Profiles The management page is accessible from the header menu through the Admin > Profiles menu and required a use with the manageProfile permission (refer to the section above). TheHive comes with default profiles but they can be updated and removed (if not used). New profiles can be created. Once the New Profile button is clicked, a dialog is opened asking for the profile type, a name for the profile and a selection of permissions. Multiple selection can be made using CTRL+click. If it is used, a profile can\u2019t be remove but can be updated. Default profiles are: admin : can manage all global objects and users. Can\u2019t create case. analyst : can manage cases and other related objects (observables, tasks, \u2026), including shring them org-admin : all permissions except those related to global objects read-only : no permission","title":"User Profiles management"},{"location":"thehive/user-guides/administrators/profiles/#user-profiles-management","text":"User profiles is a new concept introduced by TheHive4 and coming from the support of role based access control aka RBAC. In TheHive4, users will be assigned a Profile , within an Organisation . A Profile is composed by a set of predefined permissions.","title":"User Profiles management"},{"location":"thehive/user-guides/administrators/profiles/#permissions","text":"A Profile is a set of permissions attached to a User and an Organisation . It defines what the user can do on an object hold by the organisation. TheHive has a finite list of permissions: manageOrganisation (1) : the user can create , update an organisation manageConfig (1): the user can update configuration manageProfile (1): the user can create , update and delete profiles manageTag (1): the user can create , update and delete tags manageCustomField (1): the user can create , update and delete custom fields manageCase : the user can create , update and delete cases manageObservable : the user can create , update and delete observables manageAlert : the user can create , update and import alerts manageUser : the user can create , update and delete users manageCaseTemplate : the user can create , update and delete case template manageTask : the user can create , update and delete tasks manageShare : the user can share case, task and observable with other organisation manageAnalyse (2): the user can execute analyse manageAction (2): the user can execute actions manageAnalyzerTemplate (2): the user can create , update and delete analyzer template (previously named report template) (1) Organisations, configuration, profiles and tags are global objects. The related permissions are effective only on \u201cadmin\u201d organisation. (2) Actions, analysis and template is available only if Cortex connector is enabled Note Read information doesn\u2019t require specific permission. By default, users in an organisation can see all data shared with that organisation (cf. shares, discussed in Organisations,Users and sharing ).","title":"Permissions"},{"location":"thehive/user-guides/administrators/profiles/#profiles","text":"We distinguish two types of profiles: Administration Profiles Organisation Profiles The management page is accessible from the header menu through the Admin > Profiles menu and required a use with the manageProfile permission (refer to the section above). TheHive comes with default profiles but they can be updated and removed (if not used). New profiles can be created. Once the New Profile button is clicked, a dialog is opened asking for the profile type, a name for the profile and a selection of permissions. Multiple selection can be made using CTRL+click. If it is used, a profile can\u2019t be remove but can be updated. Default profiles are: admin : can manage all global objects and users. Can\u2019t create case. analyst : can manage cases and other related objects (observables, tasks, \u2026), including shring them org-admin : all permissions except those related to global objects read-only : no permission","title":"Profiles"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/","text":"Tactics, Techniques & Procedures # TheHive 4.1.0+ is required to use TTPs Starting with version 4.1.0, TheHive allows to bind Cases to TTPs (Tactics, Techniques & Procedures) . The MITRE ATT&CK framework has been chosen to define these TTPs. Import MITRE ATT&CK patterns # To access and import MITRE ATT&CK patterns definition, beeing admin or at least have the role managePattern is required. In the admin organisation, open the ATT&CK Patterns menu Click on Import MITRE ATT&CK Patterns and select the appropriate file Ensure patterns are imported Tip A direct link to the current zip archive of MITRE ATT&CK patterns let you download it quickly from the official github page. Use MITRE ATT&CK # Refer to this page to learn how to add TTPs ( Tactics, Techniques and Procedures ) to a Case.","title":"Tactics, Techniques & Procedures"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/#tactics-techniques-procedures","text":"TheHive 4.1.0+ is required to use TTPs Starting with version 4.1.0, TheHive allows to bind Cases to TTPs (Tactics, Techniques & Procedures) . The MITRE ATT&CK framework has been chosen to define these TTPs.","title":"Tactics, Techniques &amp; Procedures"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/#import-mitre-attck-patterns","text":"To access and import MITRE ATT&CK patterns definition, beeing admin or at least have the role managePattern is required. In the admin organisation, open the ATT&CK Patterns menu Click on Import MITRE ATT&CK Patterns and select the appropriate file Ensure patterns are imported Tip A direct link to the current zip archive of MITRE ATT&CK patterns let you download it quickly from the official github page.","title":"Import MITRE ATT&amp;CK patterns"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/#use-mitre-attck","text":"Refer to this page to learn how to add TTPs ( Tactics, Techniques and Procedures ) to a Case.","title":"Use MITRE ATT&amp;CK"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/","text":"Taxonomies and Tags # TheHive 4.1.0+ is required to use Taxonomies TheHive 4.1.0 introduces the support of Taxonomies as it is defined and published by MISP . These set of classification libraries can be used in THeHive to tag Cases , Observables and Alerts . Tip Not only MISP-Taxonomies are supported by TheHive, but you can also build your own by: Following the IETF draft https://tools.ietf.org/id/draft-dulaunoy-misp-taxonomy-format-07.html Draw inspiration from an existing definition file :-) By default, TheHive does not contain any taxonomy. Import taxonomies # To access and import taxonomies, beeing admin or at least have the role manageTaxonomy is required. In the admin organisation, open the Taxonomies menu Click on Import taxonomies and select the file containing the libraries Tip A direct link to the current zip archive of MISP-Taxonomies let you download it quickly. Enable interesting taxonomies # Select the libraries you would like your user be able to use in Case or Observables , and enable it . Your browser does not support the video tag. Warning Enabling a taxonomy means all users of all Organisations can use one or more included tags in a Case or Observable . Tags from taxonomies versus free text tags # In the UI, users can add free text tags, and also choose to add a tag from a library in a dedicated view. Free text tags are managed at the Organisation level by users with orgadmin profile, or at least manageTag permission. Refer to appropriate pages to learn about how to manage custom tags , and how to use tags in TheHive. Info If a tag is imported with an Alert or created with the API, TheHive tries to dissect it as a machinetag . It tries to identify a namespace, a predicate and an optional value. If successful, and if an associated taxonomy exists and is enabled , the tag is linked to the library ; if not, it is considered as a free text tag.","title":"Taxonomies and Tags"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#taxonomies-and-tags","text":"TheHive 4.1.0+ is required to use Taxonomies TheHive 4.1.0 introduces the support of Taxonomies as it is defined and published by MISP . These set of classification libraries can be used in THeHive to tag Cases , Observables and Alerts . Tip Not only MISP-Taxonomies are supported by TheHive, but you can also build your own by: Following the IETF draft https://tools.ietf.org/id/draft-dulaunoy-misp-taxonomy-format-07.html Draw inspiration from an existing definition file :-) By default, TheHive does not contain any taxonomy.","title":"Taxonomies and Tags"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#import-taxonomies","text":"To access and import taxonomies, beeing admin or at least have the role manageTaxonomy is required. In the admin organisation, open the Taxonomies menu Click on Import taxonomies and select the file containing the libraries Tip A direct link to the current zip archive of MISP-Taxonomies let you download it quickly.","title":"Import taxonomies"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#enable-interesting-taxonomies","text":"Select the libraries you would like your user be able to use in Case or Observables , and enable it . Your browser does not support the video tag. Warning Enabling a taxonomy means all users of all Organisations can use one or more included tags in a Case or Observable .","title":"Enable interesting taxonomies"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#tags-from-taxonomies-versus-free-text-tags","text":"In the UI, users can add free text tags, and also choose to add a tag from a library in a dedicated view. Free text tags are managed at the Organisation level by users with orgadmin profile, or at least manageTag permission. Refer to appropriate pages to learn about how to manage custom tags , and how to use tags in TheHive. Info If a tag is imported with an Alert or created with the API, TheHive tries to dissect it as a machinetag . It tries to identify a namespace, a predicate and an optional value. If successful, and if an associated taxonomy exists and is enabled , the tag is linked to the library ; if not, it is considered as a free text tag.","title":"Tags from taxonomies versus free text tags"},{"location":"thehive/user-guides/analysts/close-case/","text":"","title":"Close case"},{"location":"thehive/user-guides/analysts/create-alerts/","text":"","title":"Create alerts"},{"location":"thehive/user-guides/analysts/create-case/","text":"","title":"Create case"},{"location":"thehive/user-guides/analysts/create-observables%20/","text":"","title":"Create observables "},{"location":"thehive/user-guides/analysts/create-tasks/","text":"","title":"Create tasks"},{"location":"thehive/user-guides/analysts/export/","text":"","title":"Export"},{"location":"thehive/user-guides/analysts/run-analyzers/","text":"","title":"Run analyzers"},{"location":"thehive/user-guides/analysts/run-responders/","text":"","title":"Run responders"},{"location":"thehive/user-guides/analysts/sharing/","text":"","title":"Sharing"},{"location":"thehive/user-guides/analysts/ttps/","text":"Tactics, Techniques and Procedures #","title":"Tactics, Techniques and Procedures"},{"location":"thehive/user-guides/analysts/ttps/#tactics-techniques-and-procedures","text":"","title":"Tactics, Techniques and Procedures"},{"location":"thehive/user-guides/analysts/user-settings/","text":"User settings configuration # Every TheHive user, has a set of settings that can be updated through the Settings menu located on the right hand side of the navigation bar This page allows the following operations: User settings configuration Update basic Info Update password Configure MFA Update basic Info # This section gives the user the ability to update the his/her name and upload an avatar image Update password # This section is hidden by default, the user needs to enable it, set the current password and the new one twice. Clicking Save button to submit the form Configure MFA # This section allows a user to enable 2FA authentication using a TOTP application (Google Authenticator, Authy, Microsoft Authenticator, 1password etc.) to scan the QR code or the code underneath it. The 2FA will generate A TOTP that the user should supply in the MFA Code area. If it is valid, 2FA will be activated. A Disable button allows the user to deactivate the 2FA settings. A user with 2FA activated, will be prompted to provide a TOTP during login process.","title":"User settings configuration"},{"location":"thehive/user-guides/analysts/user-settings/#user-settings-configuration","text":"Every TheHive user, has a set of settings that can be updated through the Settings menu located on the right hand side of the navigation bar This page allows the following operations: User settings configuration Update basic Info Update password Configure MFA","title":"User settings configuration"},{"location":"thehive/user-guides/analysts/user-settings/#update-basic-info","text":"This section gives the user the ability to update the his/her name and upload an avatar image","title":"Update basic Info"},{"location":"thehive/user-guides/analysts/user-settings/#update-password","text":"This section is hidden by default, the user needs to enable it, set the current password and the new one twice. Clicking Save button to submit the form","title":"Update password"},{"location":"thehive/user-guides/analysts/user-settings/#configure-mfa","text":"This section allows a user to enable 2FA authentication using a TOTP application (Google Authenticator, Authy, Microsoft Authenticator, 1password etc.) to scan the QR code or the code underneath it. The 2FA will generate A TOTP that the user should supply in the MFA Code area. If it is valid, 2FA will be activated. A Disable button allows the user to deactivate the 2FA settings. A user with 2FA activated, will be prompted to provide a TOTP during login process.","title":"Configure MFA"},{"location":"thehive/user-guides/organisation-managers/case-templates/","text":"","title":"Case templates"},{"location":"thehive/user-guides/organisation-managers/custom-tags/","text":"","title":"Custom tags"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/","text":"Organisations, Users and sharing # User role, profile and permission # User # In TheHive, a user is a member of one or more organisations. One user has a profile for each organisation and can have different profiles for different organisations. For example: \u201c analyst \u201d in \u201c organisationA \u201d; and \u201c admin \u201d in \u201c organisationB \u201d; and \u201c read-only \u201d in \u201c organisationC \u201d. Organisations and sharing # TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other, and can't share with any. To do so, an organisation must be \"linked\" with another one. Only super administrators or users with manageOrganisation permissions can give the ability of a organisation to see an other one. This ability named \u201c link \u201d is unidirectional. Link with other organisations # To share a case with another organisation, a user must be able to see it: its organisation must be \"linked\" with the targeted organisation. Share and effective permissions # When a user creates a case, the case is linked to the user\u2019s organisation with the profile \u201corg-admin\u201d. It means that there is no restriction, the effective permissions are the permissions the user has in his organisation. If he decides to share that case with another organisation, he must choose the profile applied on that share. To exerce a action on a case, the related permission must be present in the user profile and in the case share. When you share a case, you can share its tasks or observables but it is not mandatory. Tasks (and observables) can be unitary shared. They can be shared only with organisations for which case is already shared. A case can be shared only once for a given organisation. Thus a case an its tasks/observables are shared with the same permissions for the same organisation.","title":"Organisations, Users and sharing"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#organisations-users-and-sharing","text":"","title":"Organisations, Users and sharing"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#user-role-profile-and-permission","text":"","title":"User role, profile and permission"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#user","text":"In TheHive, a user is a member of one or more organisations. One user has a profile for each organisation and can have different profiles for different organisations. For example: \u201c analyst \u201d in \u201c organisationA \u201d; and \u201c admin \u201d in \u201c organisationB \u201d; and \u201c read-only \u201d in \u201c organisationC \u201d.","title":"User"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#organisations-and-sharing","text":"TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other, and can't share with any. To do so, an organisation must be \"linked\" with another one. Only super administrators or users with manageOrganisation permissions can give the ability of a organisation to see an other one. This ability named \u201c link \u201d is unidirectional.","title":"Organisations and sharing"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#link-with-other-organisations","text":"To share a case with another organisation, a user must be able to see it: its organisation must be \"linked\" with the targeted organisation.","title":"Link with other organisations"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#share-and-effective-permissions","text":"When a user creates a case, the case is linked to the user\u2019s organisation with the profile \u201corg-admin\u201d. It means that there is no restriction, the effective permissions are the permissions the user has in his organisation. If he decides to share that case with another organisation, he must choose the profile applied on that share. To exerce a action on a case, the related permission must be present in the user profile and in the case share. When you share a case, you can share its tasks or observables but it is not mandatory. Tasks (and observables) can be unitary shared. They can be shared only with organisations for which case is already shared. A case can be shared only once for a given organisation. Thus a case an its tasks/observables are shared with the same permissions for the same organisation.","title":"Share and effective permissions"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/","text":"","title":"Ui configuration"},{"location":"thehive/user-guides/organisation-managers/users-management/","text":"","title":"Users management"}]}