{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TheHive Project # This is the official documentation website of TheHive Project. TheHive 4 # TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. Sources: https://github.com/TheHive-Project/TheHive Documentation: https://docs.thehive-project.org/docs/thehive/ TheHive4py # TheHive4py is a Python API client for TheHive. Sources: https://github.com/TheHive-Project/TheHive4py Documentation: https://thehive-project.github.io/TheHive4py/ Cortex # Cortex is a powerful observable analysis and active response engine. Sources: https://github.com/TheHive-Project/Cortex Documentation: https://github.com/TheHive-Project/CortexDocs Cortex Neurons # Cortex neurons is the repository of the reviewed Analyzers and Responders, contributed by the community. Sources: https://github.com/TheHive-Project/Cortex-Analyzers/ Documentation: https://thehive-project.github.io/Cortex-Analyzers/ Cortex4py # Cortex4py is a Python API client for Cortex. Sources: https://github.com/TheHive-Project/Cortex4py Documentation: https://github.com/TheHive-Project/Cortex4py Cortexutils # Cortexutils is a Python library containing a set of classes that aims to make users write Cortex analyzers and responders easier. Sources: https://github.com/TheHive-Project/Cortexutils Documentation: https://github.com/TheHive-Project/Cortexutils Docker-templates # This repository is hosting docker configurations for TheHive, Cortex and 3rd party tools integrations. Sources: https://github.com/TheHive-Project/Docker-Templates Awesome # This repository aims at reference and centralise a curated list of awesome things related to TheHive & Cortex. Website: https://github.com/TheHive-Project/awesome TODO # How to update to TheHive 4.1.0 User Guides","title":"Home"},{"location":"#thehive-project","text":"This is the official documentation website of TheHive Project.","title":"TheHive Project"},{"location":"#thehive-4","text":"TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. Sources: https://github.com/TheHive-Project/TheHive Documentation: https://docs.thehive-project.org/docs/thehive/","title":"TheHive 4"},{"location":"#thehive4py","text":"TheHive4py is a Python API client for TheHive. Sources: https://github.com/TheHive-Project/TheHive4py Documentation: https://thehive-project.github.io/TheHive4py/","title":"TheHive4py"},{"location":"#cortex","text":"Cortex is a powerful observable analysis and active response engine. Sources: https://github.com/TheHive-Project/Cortex Documentation: https://github.com/TheHive-Project/CortexDocs","title":"Cortex"},{"location":"#cortex-neurons","text":"Cortex neurons is the repository of the reviewed Analyzers and Responders, contributed by the community. Sources: https://github.com/TheHive-Project/Cortex-Analyzers/ Documentation: https://thehive-project.github.io/Cortex-Analyzers/","title":"Cortex Neurons"},{"location":"#cortex4py","text":"Cortex4py is a Python API client for Cortex. Sources: https://github.com/TheHive-Project/Cortex4py Documentation: https://github.com/TheHive-Project/Cortex4py","title":"Cortex4py"},{"location":"#cortexutils","text":"Cortexutils is a Python library containing a set of classes that aims to make users write Cortex analyzers and responders easier. Sources: https://github.com/TheHive-Project/Cortexutils Documentation: https://github.com/TheHive-Project/Cortexutils","title":"Cortexutils"},{"location":"#docker-templates","text":"This repository is hosting docker configurations for TheHive, Cortex and 3rd party tools integrations. Sources: https://github.com/TheHive-Project/Docker-Templates","title":"Docker-templates"},{"location":"#awesome","text":"This repository aims at reference and centralise a curated list of awesome things related to TheHive & Cortex. Website: https://github.com/TheHive-Project/awesome","title":"Awesome"},{"location":"#todo","text":"How to update to TheHive 4.1.0 User Guides","title":"TODO"},{"location":"cortex/","text":"Cortex : Installation, operation and user guides Source Code : https://github.com/thehive-project/Cortex/ Website : https://www.strangebee.com Cortex # Cortex solves two common problems frequently encountered by SOCs, CSIRTs and security researchers in the course of threat intelligence, digital forensics and incident response: How to analyze observables they have collected, at scale, by querying a single tool instead of several? How to actively respond to threats and interact with the constituency and other teams? Thanks to its many analyzers and to its RESTful API, Cortex makes observable analysis a breeze, particularly if called from TheHive , the highly popular, Security Incident Response Platform (SIRP). TheHive can also leverage Cortex responders to perform specific actions on alerts, cases, tasks and observables collected in the course of the investigation: send an email to the constituents, block an IP address at the proxy level, notify team members that an alert needs to be taken care of urgently and much more. Many features are included with Cortex: Manage multiple organizations (i.e multi-tenancy) Manage users per organizations and roles Specify per-org analyzer & responder configuration Define rate limits: avoid consuming all your quotas at once Cache: an analysis is not re-executed for the same observable if a given analyzer is called on that observable several times within a specific timespan (10 minutes by default, can be adjusted for each analyzer). Installation and configuration guides # This documentation contains step-by-step installation instructions for Cortex for different operating systems as well as corresponding binary archives. All aspects of the configuration are aslo detailled in a dedicated section. s User guides # The first connection to the application requires several actions. Cortex supports differents roles for users. Refer to User roles for more details. License # Cortex is an open source and free software released under the AGPL (Affero General Public License). We, StrangeBee , are committed to ensure that Cortex will remain a free and open source project on the long-run. Updates and community discussions # Information, news and updates are regularly posted on several communication channels: TheHive Project Twitter account TheHive Project blog TheHive Project Discord Users forum on Google Groups . Request an access: using a Gmail address or without it . Contributing # We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing. Community support # Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Discord to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Note If you have problems with Cortex4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository . Professional support # Since 2018, Cortex is fully developped and maintained by StrangeBee . Should you need specific assistance, be aware that StrangeBee also provides professional services and support.","title":"Home"},{"location":"cortex/#cortex","text":"Cortex solves two common problems frequently encountered by SOCs, CSIRTs and security researchers in the course of threat intelligence, digital forensics and incident response: How to analyze observables they have collected, at scale, by querying a single tool instead of several? How to actively respond to threats and interact with the constituency and other teams? Thanks to its many analyzers and to its RESTful API, Cortex makes observable analysis a breeze, particularly if called from TheHive , the highly popular, Security Incident Response Platform (SIRP). TheHive can also leverage Cortex responders to perform specific actions on alerts, cases, tasks and observables collected in the course of the investigation: send an email to the constituents, block an IP address at the proxy level, notify team members that an alert needs to be taken care of urgently and much more. Many features are included with Cortex: Manage multiple organizations (i.e multi-tenancy) Manage users per organizations and roles Specify per-org analyzer & responder configuration Define rate limits: avoid consuming all your quotas at once Cache: an analysis is not re-executed for the same observable if a given analyzer is called on that observable several times within a specific timespan (10 minutes by default, can be adjusted for each analyzer).","title":"Cortex"},{"location":"cortex/#installation-and-configuration-guides","text":"This documentation contains step-by-step installation instructions for Cortex for different operating systems as well as corresponding binary archives. All aspects of the configuration are aslo detailled in a dedicated section. s","title":"Installation and configuration guides"},{"location":"cortex/#user-guides","text":"The first connection to the application requires several actions. Cortex supports differents roles for users. Refer to User roles for more details.","title":"User guides"},{"location":"cortex/#license","text":"Cortex is an open source and free software released under the AGPL (Affero General Public License). We, StrangeBee , are committed to ensure that Cortex will remain a free and open source project on the long-run.","title":"License"},{"location":"cortex/#updates-and-community-discussions","text":"Information, news and updates are regularly posted on several communication channels: TheHive Project Twitter account TheHive Project blog TheHive Project Discord Users forum on Google Groups . Request an access: using a Gmail address or without it .","title":"Updates and community discussions"},{"location":"cortex/#contributing","text":"We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing.","title":"Contributing"},{"location":"cortex/#community-support","text":"Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Discord to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Note If you have problems with Cortex4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository .","title":"Community support"},{"location":"cortex/#professional-support","text":"Since 2018, Cortex is fully developped and maintained by StrangeBee . Should you need specific assistance, be aware that StrangeBee also provides professional services and support.","title":"Professional support"},{"location":"cortex/code-of-conduct/","text":"Contributor Covenant Code of Conduct # Our Pledge # In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards # Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities # Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior, in compliance with the licensing terms applying to the Project developments. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. However, these actions shall respect the licensing terms of the Project Developments that will always supersede such Code of Conduct. Scope # This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement # Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at support@thehive-project.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution # This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4 This version includes a clarification to ensure that the code of conduct is in compliance with the free software licensing terms of the project.","title":"Contributor Covenant Code of Conduct"},{"location":"cortex/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"cortex/code-of-conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"cortex/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"cortex/code-of-conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior, in compliance with the licensing terms applying to the Project developments. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. However, these actions shall respect the licensing terms of the Project Developments that will always supersede such Code of Conduct.","title":"Our Responsibilities"},{"location":"cortex/code-of-conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"cortex/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at support@thehive-project.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"cortex/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4 This version includes a clarification to ensure that the code of conduct is in compliance with the free software licensing terms of the project.","title":"Attribution"},{"location":"cortex/api/api-guide/","text":"API Guide # This guide applies only to Cortex 2 and newer. It is not applicable to Cortex 1. Table of Contents # Introduction Request & Response Formats Authentication Organization APIs Organization Model List Create Update Delete Obtain Details List Users List Enabled Analyzers User APIs User Model List All List Users within an Organization Search Create Update Get Details Set a Password Change a password Set and Renew an API Key Get an API Key Revoke an API Key Job APIs Job Model List and Search Get Details Get Details and Report Wait and Get Job Report Get Artifacts Delete Analyzer APIs Analyzer Model Enable List and Search Get Details Get By Type Update Run Disable Miscellaneous APIs Paging and Sorting Introduction # Cortex 2 offers a REST API that can be leveraged by various applications and programs to interact with it. The following guide describe the Cortex 2 API to allow developers to interface the powerful observable analysis engine with other SIRPs (Security Incident Response Platforms) besides TheHive, TIPs (Threat Intelligence Platforms), SIEMs or scripts. Please note that the Web UI of Cortex 2 exclusively leverage the REST API to interact with the back-end. Note : You can use Cortex4py , the Python library we provide, to facilitate interaction with the REST API of Cortex. You need Cortex4py 2.0.0 or later as earlier versions are not compatible with Cortex 2. All the exposed APIs share the same request & response formats and authentication strategies as described below. There are also some transverse parameters supported by several calls, in addition to utility APIs . If you want to create an analyzer, please read the How to Write and Submit an Analyzer guide. Request & Response Formats # Cortex accepts several parameter formats within a HTTP request. They can be used indifferently. Input data can be: A query string A URL-encoded form A multi-part JSON Hence, the requests shown below are equivalent. Query String # curl -XPOST 'https://CORTEX_APP_URL:9001/api/login?user=me&password=secret' URL-encoded Form # curl -XPOST 'https://CORTEX_APP_URL:9001/api/login' -d user = me -d password = secret JSON # curl -XPOST https://CORTEX_APP_URL:9001/api/login -H 'Content-Type: application/json' -d '{ \"user\": \"me\", \"password\": \"secret\" }' Multi-part # curl -XPOST https://CORTEX_APP_URL:9001/api/login -F '_json=<-;type=application/json' << _EOF_ { \"user\": \"me\", \"password\": \"secret\" } _EOF_ Response Format # For each request submitted, Cortex will respond back with JSON data. For example, if the authentication request is successful, Cortex should return the following output: { \"id\" : \"me\" , \"name\" : \"me\" , \"roles\" :[ \"read\" , \"analyze\" , \"orgadmin\" ]} If not, Cortex should return an authentication error: { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } Authentication # Most API calls require authentication. Credentials can be provided using a session cookie , an API key or directly using HTTP basic authentication (if this method is specifically enabled). Session cookies are better suited for browser authentication. Hence, we recommend authenticating with API keys when calling the Cortex APIs. Generating API Keys with an orgAdmin Account # API keys can be generated using the Web UI. To do so, connect using an orgAdmin account then click on Organization and then on the Create API Key button in the row corresponding to the user you intend to use for API authentication. Once the API key has been created, click on Reveal to display the API key then click on the copy to clipboard button if you wish to copy the key to your system's clipboard. If the user is not yet created, start by clicking on Add user to create it then follow the steps mentioned above. Generating API Keys with a superAdmin Account # You can use a superAdmin account to achieve the same result as described above. Once authenticated, click on Users then on the Create API Key button in the row corresponding to the user you intend to use for API authentication. Please make sure the user is in the right organization by thoroughly reading its name, which is shown below the user name. Once the API key has been created, click on Reveal to display the API key then click on the copy to clipboard button if you wish to copy the key to your system's clipboard. Authenticating with an API Key # Once you have generated an API key you can use it, for example, to list the Cortex jobs thanks to the following curl command: # Using API key curl -H 'Authorization: Bearer **API_KEY**' https://CORTEX_APP_URL:9001/api/job As you can see in the example above, we instructed curl to add the Authorization header to the request. The value of the header is Bearer: **API_KEY** . So if your API key is GPX20GUAQWwpqnhA6JpOwNGPMfWuxsX3 , the curl command above would look like the following: # Using API key curl -H 'Authorization: Bearer GPX20GUAQWwpqnhA6JpOwNGPMfWuxsX3' https://CORTEX_APP_URL:9001/api/job Using Basic Authentication # Cortex also supports basic authentication but it is disabled by default for security reasons. If you absolutely need to use it , you can enable it by adding auth.method.basic=true to the configuration file ( /etc/cortex/application.conf by default). Once you do, restart the Cortex service. You can then, for example, list the Cortex jobs using the following curl command: # Using basic authentication curl -u mylogin:mypassword https://CORTEX_APP_URL:9001/api/job Organization APIs # Cortex offers a set of APIs to create, update and list organizations. Organization Model # An organization (org) is defined by the following attributes: Attribute Description Type id Copy of the org's name (see next row) readonly name Name readonly status Status ( Active or Locked ) writable description Description writable createdAt Creation date computed createdBy User who created the org computed updatedAt Last update computed updatedBy User who last updated the org computed Please note that id and name are essentially the same. Also, createdAt and updatedAt are in epoch . List # It is possible to list all the organizations using the following API call, which requires the API key associated with a superAdmin account: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization' You can also search/filter organizations using the following query: curl -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/_search' -d '{ \"query\": {\"status\": \"Active\"} }' Both APIs supports the range and sort query parameters described in paging and sorting details . Create # It is possible to create an organization using the following API call, which requires the API key associated with a superAdmin account: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization' -d '{ \"name\": \"demo\", \"description\": \"Demo organization\", \"status\": \"Active\" }' Update # You can update an organization's description and status ( Active or Locked ) using the following API call. This requires the API key associated with a superAdmin account: curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID' -d '{ \"description\": \"New Demo organization\", }' or curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID' -d '{ \"status\": \"Active\", }' Delete # Deleting an organization just marks it as Locked and doesn't remove the associated data from the DB. To \"delete\" an organization, you can use the API call shown below. It requires the API key associated with a superAdmin account. curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID' Obtain Details # This API call returns the details of an organization as described in the Organization model section. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID' Let's assume that the organization we are seeking to obtain details about is called demo . The curl command would be: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/demo' and it should return: { \"id\" : \"demo\" , \"name\" : \"demo\" , \"status\" : \"Active\" , \"description\" : \"Demo organization\" , \"createdAt\" : 1520258040437 , \"createdBy\" : \"superadmin\" , \"updatedBy\" : \"superadmin\" , \"updatedAt\" : 1522077420693 } List Users # As mentioned above, you can use the API to return the list of all the users declared withing an organization. For that purpose, use the API call shown below with the API key of an orgAdmin or superAdmin account. It supports the range and sort query parameters declared in paging and sorting details . curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID/user' and should return a list of users . If one wants to filter/search for some users (active ones for example), there is a search API to use as below: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID/user/_search' -d '{ \"query\": {} }' It also supports the range and sort query parameters declared in paging and sorting details . List Enabled Analyzers # To list the analyzers that have been enabled within an organization, use the following API call with the API key of an orgAdmin user: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer' It should return a list of Analyzers . Please note that this API call does not display analyzers that are disabled. It supports the range and sort query parameters declared in paging and sorting details . User APIs # The following section describes the APIs that allow creating, updating and listing users within an organization. User Model # A user is defined by the following attributes: Attribute Description Type id ID/login readonly name Name writable roles Roles. Possible values are: read , read,analyze , read,analyze,orgadmin and superadmin writable status Status ( Active or Locked ) writable organization organization to which the user belongs (set upon account creation) readonly createdAt Creation date computed createdBy User who created the account computed updatedAt Last update date computed updatedBy User who last updated the account computed hasKey true when the user has an API key computed hasPassword true if the user has a password computed List All # This API call allows a superAdmin to list and search all the users of all defined organizations: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user' This call supports the range and sort query parameters declared in paging and sorting details . List Users within an Organization # This call is described in organization APIs . Search # This API call allows a superAdmin to perform search on the user accounts created in a Cortex instance: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/_search' -d '{ \"query\": {} }' This call supports the range and sort query parameters declared in paging and sorting details Create # This API calls allows you to programmatically create user creation. If the call is made by a superAdmin user, the request must specify the organization to which the user belong in the organization field. If the call is made by an orgAdmin user, the value of organization field must be the same as the user who makes the call: orgAdmin users are allowed to create users only in their organization. curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user' -d '{ \"name\": \"Demo org Admin\", \"roles\": [ \"read\", \"analyze\", \"orgadmin\" ], \"organization\": \"demo\", \"login\": \"demo\" }' If successful, the call returns a JSON object representing the created user as described above . { \"id\" : \"demo\" , \"organization\" : \"demo\" , \"name\" : \"Demo org Admin\" , \"roles\" : [ \"read\" , \"analyze\" , \"orgadmin\" ], \"status\" : \"Ok\" , \"createdAt\" : 1526050123286 , \"createdBy\" : \"superadmin\" , \"hasKey\" : false , \"hasPassword\" : false } Update # This API call allows updating the writable attributed of a user account. It's available to users with superAdmin or orgAdmin roles. Any user can also use it to update their own information (but obviously not their roles). curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN' -d '{ \"name\": \"John Doe\", \"roles\": [ \"read\", \"analyze\" ], \"status\": \"Locked\" }' It returns a JSON object representing the updated user as described above . Get Details # This call returns the user details. It's available to users with superAdmin roles and to users in the same organization. Every user can also use it to read their own details. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN' It returns a JSON object representing the user as described previously . Set a Password # This call sets the user's password. It's available to users with superAdmin or orgAdmin roles. Please note that the request needs to be made using HTTPS with a valid certificate on the server's end to prevent credential sniffing or other PITM (Person-In-The-Middle) attacks. curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/password/set' -d '{ \"password\": \"SOMEPASSWORD\" }' If successful, the call returns 204 (success / no content). Change a password # This call allows a given user to change only their own existing password. It is available to all users including superAdmin and orgAdmin ones. Please note that if a superAdmin or an orgAdmin needs to update the password of another user, they must use the /password/set call described in the previous subsection. curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/password/change' -d '{ \"currentPassword\": \"password\", \"password\": \"new-password\" }' If successful, the call returns 204 (success / no content). Set and Renew an API Key # This calls allows setting and renewing the API key of a user. It's available to users with superAdmin or orgAdmin roles. Any user can also use it to renew their own API key. Again, the request needs to be made using HTTPS with a valid certificate on the server's end to prevent credential sniffing or other PITM (Person-In-The-Middle) attacks. You know the drill ;-) curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/key/renew' If successful, it returns the generated API key in a text/plain response. Get an API Key # This calls allows getting a user's API key. It's available to users with superAdmin or orgAdmin roles. Any user can also use it to obtain their own API key. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/key' If successful, the generated API key is returned in text/plain response Revoke an API Key # This calls allow revoking a user's API key. This calls allow revoking a user's API key. curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/key' A successful request returns nothing (HTTP 200 OK). Job APIs # The following section describes the APIs that allow manipulating jobs. Jobs are basically submissions made to analyzers and the resulting reports. Job Model # A job is defined by the following attributes: Attribute Description Type id Job ID computed organization The organization to which the job belongs readonly analyzerDefinitionId Analyzer definition name readonly analyzerId Instance ID of the analyzer to which the job is associated readonly organization Organization to which the user belongs (set upon account creation) readonly analyzerName Name of the analyzer to which the job is associated readonly dataType the datatype of the analyzed observable readonly status Status of the job ( Waiting , InProgress , Success , Failure , Deleted ) computed data Value of the analyzed observable (does not apply to file observables) readonly attachment JSON object representing file observables (does not apply to non- file observables). It defines the name , hashes , size , contentType and id of the file observable readonly parameters JSON object of key/value pairs set during job creation readonly message A free text field to set additional text/context for a job readonly tlp The TLP of the analyzed observable readonly startDate Start date computed endDate End date computed createdAt Creation date. Please note that a job can be requested but not immediately honored. The actual time at which it is started is the value of startDate computed createdBy User who created the job computed updatedAt Last update date (only Cortex updates a job when it finishes) computed updatedBy User who submitted the job and which identity is used by Cortex to update the job once it is finished computed List and Search # This call allows a user with read , analyze or orgAdmin role to list and search all the analysis jobs made by their organization. If you want to list all the jobs: curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/_search?range=all' If you want to list 10 jobs: curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/_search' If you want to list 100 jobs: curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/_search?range=0-100' If you want to search jobs according to various criteria: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/job/_search' -d '{ \"query\": { \"_and\": [ {\"status\": \"Success\"}, {\"dataType\": \"ip\"} ] } }' This call supports the range and sort query parameters declared in paging and sorting details Get Details # This call allows a user with read , analyze or orgAdmin role to get the details of a job. It does not fetch the job report. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID' It returns a JSON response with the following structure: { \"id\" : \"AWNei4vH3rJ8unegCPB9\" , \"analyzerDefinitionId\" : \"Abuse_Finder_2_0\" , \"analyzerId\" : \"220483fde9608c580fb6a2508ff3d2d3\" , \"analyzerName\" : \"Abuse_Finder_2_0\" , \"status\" : \"Success\" , \"data\" : \"8.8.8.8\" , \"parameters\" : \"{}\" , \"tlp\" : 0 , \"message\" : \"\" , \"dataType\" : \"ip\" , \"organization\" : \"demo\" , \"startDate\" : 1526299593923 , \"endDate\" : 1526299597064 , \"date\" : 1526299593633 , \"createdAt\" : 1526299593633 , \"createdBy\" : \"demo\" , \"updatedAt\" : 1526299597066 , \"updatedBy\" : \"demo\" } Get Details and Report # This call allows a user with read , analyze or orgAdmin role to get the details of a job including its report. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID/report' It returns a JSON response with the structure below. If the job is not yet completed, the report field contains a string representing the job status: { \"id\" : \"AWNei4vH3rJ8unegCPB9\" , \"analyzerDefinitionId\" : \"Abuse_Finder_2_0\" , \"analyzerId\" : \"220483fde9608c580fb6a2508ff3d2d3\" , \"analyzerName\" : \"Abuse_Finder_2_0\" , \"status\" : \"Success\" , \"data\" : \"8.8.8.8\" , \"parameters\" : \"{}\" , \"tlp\" : 0 , \"message\" : \"\" , \"dataType\" : \"ip\" , \"organization\" : \"demo\" , \"startDate\" : 1526299593923 , \"endDate\" : 1526299597064 , \"date\" : 1526299593633 , \"createdAt\" : 1526299593633 , \"createdBy\" : \"demo\" , \"updatedAt\" : 1526299597066 , \"updatedBy\" : \"demo\" , \"report\" : { \"summary\" : { \"taxonomies\" : [ { \"predicate\" : \"Address\" , \"namespace\" : \"Abuse_Finder\" , \"value\" : \"network-abuse@google.com\" , \"level\" : \"info\" } ] }, \"full\" : { \"abuse_finder\" : { \"raw\" : \"...\" , \"abuse\" : [ \"network-abuse@google.com\" ], \"names\" : [ \"Google LLC\" , \"Level 3 Parent, LLC\" ], \"value\" : \"8.8.8.8\" } }, \"success\" : true , \"artifacts\" : [] } } Wait and Get Job Report # This call is similar the one described above but allows the user to provide a timeout to wait for the report in case it is not available at the time the query was made: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID/waitreport?atMost=1minute' The atMost is a duration using the format Xhour , Xminute or Xsecond . Get Artifacts # This call allows a user with read , analyze or orgAdmin role to get the extracted artifacts from a job if such extraction has been enabled in the corresponding analyzer configuration. Please note that extraction is imperfect and you might have inconsistent or incorrect data. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID/artifacts' It returns a JSON array with the following structure: [ { \"dataType\" : \"ip\" , \"createdBy\" : \"demo\" , \"data\" : \"8.8.8.8\" , \"tlp\" : 0 , \"createdAt\" : 1525432900553 , \"id\" : \"AWMq4tvLjidKq_asiwcl\" } ] Delete # This API allows a user with analyze or orgAdmin role to delete a job: curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID' This marks the job as Deleted . However the job's data is not removed from the database. Analyzer APIs # The following section describes the APIs that allow manipulating analyzers. Analyzer Model # An analyzer is defined by the following attributes: Attribute Description Type id Analyzer ID once enabled within an organization readonly analyzerDefinitionId Analyzer definition name readonly name Name of the analyzer readonly version Version of the analyzer readonly description Description of the analyzer readonly author Author of the analyzer readonly url URL where the analyzer has been published readonly license License of the analyzer readonly dataTypeList Allowed datatypes readonly baseConfig Base configuration name. This identifies the shared set of configuration with all the analyzer's flavors readonly jobCache Report cache timeout in minutes, visible for orgAdmin users only writable rate Numeric amount of analyzer calls authorized for the specified rateUnit , visible for orgAdmin users only writable rateUnit Period of availability of the rate limite: Day or Month , visible for orgAdmin users only writable configuration A JSON object where key/value pairs represent the config names, and their values. It includes the default properties proxy_http , proxy_https , auto_extract_artifacts , check_tlp , and max_tlp , visible for orgAdmin users only writable createdBy User who enabled the analyzer computed updatedAt Last update date computed updatedBy User who last updated the analyzer computed Enable # This call allows a user with an orgAdmin role to enable an analyzer. curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/analyzer/:analyzerId' -d '{ \"name\": \"Censys_1_0\", \"configuration\": { \"uid\": \"XXXX\", \"key\": \"XXXXXXXXXXXXXXXXXXXX\", \"proxy_http\": \"http://proxy:9999\", \"proxy_https\": \"http://proxy:9999\", \"auto_extract_artifacts\": false, \"check_tlp\": true, \"max_tlp\": 2 }, \"rate\": 1000, \"rateUnit\": \"Day\", \"jobCache\": 5 }' List and Search # These calls allow a user with a analyze or orgAdmin role to list and search all the enabled analyzers within the organization. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer' or curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/_search' -d '{ \"query\": {} }' Both calls supports the range and sort query parameters declared in paging and sorting details , and both return a JSON array of analyzer objects as described in Analyzer Model section . If called by a user with only an nalyzer role, the configuration attribute is not included on the JSON objects. Get Details # This call allows a user with a analyze or orgAdmin role to get an analyzer's details. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID' It returns a analyzer JSON object as described in Analyzer Model section . If called by a user with only an nalyzer role, the configuration attribute is not included on the JSON objects. Get By Type # This call is mostly used by TheHive and allows to quickly get the list of analyzers that can run on the given datatype. It requires an analyze or orgAdmin role. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer/type/DATA_TYPE' It returns a JSON array of analyzer objects as described in Analyzer Model section without the configuration attribute, which could contain sensitive data. Update # This call allows an orgAdmin user to update the name , configuration and jobCache of an enabled analyzer. curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID' -d '{ \"configuration\": { \"key\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\", \"polling_interval\": 60, \"proxy_http\": \"http://localhost:8080\", \"proxy_https\": \"http://localhost:8080\", \"auto_extract_artifacts\": true, \"check_tlp\": true, \"max_tlp\": 1 }, \"name\": \"Shodan_Host_1_0\", \"rate\": 1000, \"rateUnit\": \"Day\", \"jobCache\": null }' It returns a JSON object describing the analyzer as defined in Analyzer Model section . Run # This API allows a user with a analyze or orgAdmin role to run analyzers on observables of different datatypes. For file observables, the API call must be made as described below: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID/run' \\ -F 'attachment=@/path/to/observable-file' \\ -F '_json=<-;type=application/json' << _EOF_ { \"dataType\":\"file\", \"tlp\":0 } _EOF_ for all the other types of observerables, the request is: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID/run' -d '{ \"data\":\"8.8.8.8\", \"dataType\":\"ip\", \"tlp\":0, \"message\": \"A message that can be accessed from the analyzer\", \"parameters\": { \"key1\": \"value1\", \"key2\": \"value2\" } }' This call will fetch a similar job from the cache, and if it finds one, it returns it from the cache, based on the duration defined in jobCache attribute of the analyzer. To force bypassing the cache, one can add the following query parameter: force=1 curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID/run?force=1' -d '{ \"data\":\"8.8.8.8\", \"dataType\":\"ip\", \"tlp\":0, \"message\": \"A message that can be accessed from the analyzer\", \"parameters\": { \"key1\": \"value1\", \"key2\": \"value2\" } }' Disable # This API allows an orgAdmin to disable an existing analyzer in their organization and delete the corresponding configuration. curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID' Miscellaneous APIs # Paging and Sorting # All the search API calls allow sorting and paging parameters, in addition to a query in the request's body. These calls usually have URLs ending with the _search keyword but that's not always the case. The followings are query parameters: range : all or x-y where x and y are numbers (ex: 0-10). sort : you can provide multiple sort criteria such as: -createdAt or +status . Example: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'http://CORTEX_APP_URL:9001/api/organization/ORG_ID/user?range=0-10&sort=-createdAt&sort=+status' -d '{ \"query\": {} }'","title":"API Guide"},{"location":"cortex/api/api-guide/#api-guide","text":"This guide applies only to Cortex 2 and newer. It is not applicable to Cortex 1.","title":"API Guide"},{"location":"cortex/api/api-guide/#table-of-contents","text":"Introduction Request & Response Formats Authentication Organization APIs Organization Model List Create Update Delete Obtain Details List Users List Enabled Analyzers User APIs User Model List All List Users within an Organization Search Create Update Get Details Set a Password Change a password Set and Renew an API Key Get an API Key Revoke an API Key Job APIs Job Model List and Search Get Details Get Details and Report Wait and Get Job Report Get Artifacts Delete Analyzer APIs Analyzer Model Enable List and Search Get Details Get By Type Update Run Disable Miscellaneous APIs Paging and Sorting","title":"Table of Contents"},{"location":"cortex/api/api-guide/#introduction","text":"Cortex 2 offers a REST API that can be leveraged by various applications and programs to interact with it. The following guide describe the Cortex 2 API to allow developers to interface the powerful observable analysis engine with other SIRPs (Security Incident Response Platforms) besides TheHive, TIPs (Threat Intelligence Platforms), SIEMs or scripts. Please note that the Web UI of Cortex 2 exclusively leverage the REST API to interact with the back-end. Note : You can use Cortex4py , the Python library we provide, to facilitate interaction with the REST API of Cortex. You need Cortex4py 2.0.0 or later as earlier versions are not compatible with Cortex 2. All the exposed APIs share the same request & response formats and authentication strategies as described below. There are also some transverse parameters supported by several calls, in addition to utility APIs . If you want to create an analyzer, please read the How to Write and Submit an Analyzer guide.","title":"Introduction"},{"location":"cortex/api/api-guide/#request-response-formats","text":"Cortex accepts several parameter formats within a HTTP request. They can be used indifferently. Input data can be: A query string A URL-encoded form A multi-part JSON Hence, the requests shown below are equivalent.","title":"Request &amp; Response Formats"},{"location":"cortex/api/api-guide/#query-string","text":"curl -XPOST 'https://CORTEX_APP_URL:9001/api/login?user=me&password=secret'","title":"Query String"},{"location":"cortex/api/api-guide/#url-encoded-form","text":"curl -XPOST 'https://CORTEX_APP_URL:9001/api/login' -d user = me -d password = secret","title":"URL-encoded Form"},{"location":"cortex/api/api-guide/#json","text":"curl -XPOST https://CORTEX_APP_URL:9001/api/login -H 'Content-Type: application/json' -d '{ \"user\": \"me\", \"password\": \"secret\" }'","title":"JSON"},{"location":"cortex/api/api-guide/#multi-part","text":"curl -XPOST https://CORTEX_APP_URL:9001/api/login -F '_json=<-;type=application/json' << _EOF_ { \"user\": \"me\", \"password\": \"secret\" } _EOF_","title":"Multi-part"},{"location":"cortex/api/api-guide/#response-format","text":"For each request submitted, Cortex will respond back with JSON data. For example, if the authentication request is successful, Cortex should return the following output: { \"id\" : \"me\" , \"name\" : \"me\" , \"roles\" :[ \"read\" , \"analyze\" , \"orgadmin\" ]} If not, Cortex should return an authentication error: { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response Format"},{"location":"cortex/api/api-guide/#authentication","text":"Most API calls require authentication. Credentials can be provided using a session cookie , an API key or directly using HTTP basic authentication (if this method is specifically enabled). Session cookies are better suited for browser authentication. Hence, we recommend authenticating with API keys when calling the Cortex APIs.","title":"Authentication"},{"location":"cortex/api/api-guide/#generating-api-keys-with-an-orgadmin-account","text":"API keys can be generated using the Web UI. To do so, connect using an orgAdmin account then click on Organization and then on the Create API Key button in the row corresponding to the user you intend to use for API authentication. Once the API key has been created, click on Reveal to display the API key then click on the copy to clipboard button if you wish to copy the key to your system's clipboard. If the user is not yet created, start by clicking on Add user to create it then follow the steps mentioned above.","title":"Generating API Keys with an orgAdmin Account"},{"location":"cortex/api/api-guide/#generating-api-keys-with-a-superadmin-account","text":"You can use a superAdmin account to achieve the same result as described above. Once authenticated, click on Users then on the Create API Key button in the row corresponding to the user you intend to use for API authentication. Please make sure the user is in the right organization by thoroughly reading its name, which is shown below the user name. Once the API key has been created, click on Reveal to display the API key then click on the copy to clipboard button if you wish to copy the key to your system's clipboard.","title":"Generating API Keys with a superAdmin Account"},{"location":"cortex/api/api-guide/#authenticating-with-an-api-key","text":"Once you have generated an API key you can use it, for example, to list the Cortex jobs thanks to the following curl command: # Using API key curl -H 'Authorization: Bearer **API_KEY**' https://CORTEX_APP_URL:9001/api/job As you can see in the example above, we instructed curl to add the Authorization header to the request. The value of the header is Bearer: **API_KEY** . So if your API key is GPX20GUAQWwpqnhA6JpOwNGPMfWuxsX3 , the curl command above would look like the following: # Using API key curl -H 'Authorization: Bearer GPX20GUAQWwpqnhA6JpOwNGPMfWuxsX3' https://CORTEX_APP_URL:9001/api/job","title":"Authenticating with an API Key"},{"location":"cortex/api/api-guide/#using-basic-authentication","text":"Cortex also supports basic authentication but it is disabled by default for security reasons. If you absolutely need to use it , you can enable it by adding auth.method.basic=true to the configuration file ( /etc/cortex/application.conf by default). Once you do, restart the Cortex service. You can then, for example, list the Cortex jobs using the following curl command: # Using basic authentication curl -u mylogin:mypassword https://CORTEX_APP_URL:9001/api/job","title":"Using Basic Authentication"},{"location":"cortex/api/api-guide/#organization-apis","text":"Cortex offers a set of APIs to create, update and list organizations.","title":"Organization APIs"},{"location":"cortex/api/api-guide/#organization-model","text":"An organization (org) is defined by the following attributes: Attribute Description Type id Copy of the org's name (see next row) readonly name Name readonly status Status ( Active or Locked ) writable description Description writable createdAt Creation date computed createdBy User who created the org computed updatedAt Last update computed updatedBy User who last updated the org computed Please note that id and name are essentially the same. Also, createdAt and updatedAt are in epoch .","title":"Organization Model"},{"location":"cortex/api/api-guide/#list","text":"It is possible to list all the organizations using the following API call, which requires the API key associated with a superAdmin account: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization' You can also search/filter organizations using the following query: curl -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/_search' -d '{ \"query\": {\"status\": \"Active\"} }' Both APIs supports the range and sort query parameters described in paging and sorting details .","title":"List"},{"location":"cortex/api/api-guide/#create","text":"It is possible to create an organization using the following API call, which requires the API key associated with a superAdmin account: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization' -d '{ \"name\": \"demo\", \"description\": \"Demo organization\", \"status\": \"Active\" }'","title":"Create"},{"location":"cortex/api/api-guide/#update","text":"You can update an organization's description and status ( Active or Locked ) using the following API call. This requires the API key associated with a superAdmin account: curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID' -d '{ \"description\": \"New Demo organization\", }' or curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID' -d '{ \"status\": \"Active\", }'","title":"Update"},{"location":"cortex/api/api-guide/#delete","text":"Deleting an organization just marks it as Locked and doesn't remove the associated data from the DB. To \"delete\" an organization, you can use the API call shown below. It requires the API key associated with a superAdmin account. curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID'","title":"Delete"},{"location":"cortex/api/api-guide/#obtain-details","text":"This API call returns the details of an organization as described in the Organization model section. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID' Let's assume that the organization we are seeking to obtain details about is called demo . The curl command would be: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/demo' and it should return: { \"id\" : \"demo\" , \"name\" : \"demo\" , \"status\" : \"Active\" , \"description\" : \"Demo organization\" , \"createdAt\" : 1520258040437 , \"createdBy\" : \"superadmin\" , \"updatedBy\" : \"superadmin\" , \"updatedAt\" : 1522077420693 }","title":"Obtain Details"},{"location":"cortex/api/api-guide/#list-users","text":"As mentioned above, you can use the API to return the list of all the users declared withing an organization. For that purpose, use the API call shown below with the API key of an orgAdmin or superAdmin account. It supports the range and sort query parameters declared in paging and sorting details . curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID/user' and should return a list of users . If one wants to filter/search for some users (active ones for example), there is a search API to use as below: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID/user/_search' -d '{ \"query\": {} }' It also supports the range and sort query parameters declared in paging and sorting details .","title":"List Users"},{"location":"cortex/api/api-guide/#list-enabled-analyzers","text":"To list the analyzers that have been enabled within an organization, use the following API call with the API key of an orgAdmin user: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer' It should return a list of Analyzers . Please note that this API call does not display analyzers that are disabled. It supports the range and sort query parameters declared in paging and sorting details .","title":"List Enabled Analyzers"},{"location":"cortex/api/api-guide/#user-apis","text":"The following section describes the APIs that allow creating, updating and listing users within an organization.","title":"User APIs"},{"location":"cortex/api/api-guide/#user-model","text":"A user is defined by the following attributes: Attribute Description Type id ID/login readonly name Name writable roles Roles. Possible values are: read , read,analyze , read,analyze,orgadmin and superadmin writable status Status ( Active or Locked ) writable organization organization to which the user belongs (set upon account creation) readonly createdAt Creation date computed createdBy User who created the account computed updatedAt Last update date computed updatedBy User who last updated the account computed hasKey true when the user has an API key computed hasPassword true if the user has a password computed","title":"User Model"},{"location":"cortex/api/api-guide/#list-all","text":"This API call allows a superAdmin to list and search all the users of all defined organizations: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user' This call supports the range and sort query parameters declared in paging and sorting details .","title":"List All"},{"location":"cortex/api/api-guide/#list-users-within-an-organization","text":"This call is described in organization APIs .","title":"List Users within an Organization"},{"location":"cortex/api/api-guide/#search","text":"This API call allows a superAdmin to perform search on the user accounts created in a Cortex instance: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/_search' -d '{ \"query\": {} }' This call supports the range and sort query parameters declared in paging and sorting details","title":"Search"},{"location":"cortex/api/api-guide/#create_1","text":"This API calls allows you to programmatically create user creation. If the call is made by a superAdmin user, the request must specify the organization to which the user belong in the organization field. If the call is made by an orgAdmin user, the value of organization field must be the same as the user who makes the call: orgAdmin users are allowed to create users only in their organization. curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user' -d '{ \"name\": \"Demo org Admin\", \"roles\": [ \"read\", \"analyze\", \"orgadmin\" ], \"organization\": \"demo\", \"login\": \"demo\" }' If successful, the call returns a JSON object representing the created user as described above . { \"id\" : \"demo\" , \"organization\" : \"demo\" , \"name\" : \"Demo org Admin\" , \"roles\" : [ \"read\" , \"analyze\" , \"orgadmin\" ], \"status\" : \"Ok\" , \"createdAt\" : 1526050123286 , \"createdBy\" : \"superadmin\" , \"hasKey\" : false , \"hasPassword\" : false }","title":"Create"},{"location":"cortex/api/api-guide/#update_1","text":"This API call allows updating the writable attributed of a user account. It's available to users with superAdmin or orgAdmin roles. Any user can also use it to update their own information (but obviously not their roles). curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN' -d '{ \"name\": \"John Doe\", \"roles\": [ \"read\", \"analyze\" ], \"status\": \"Locked\" }' It returns a JSON object representing the updated user as described above .","title":"Update"},{"location":"cortex/api/api-guide/#get-details","text":"This call returns the user details. It's available to users with superAdmin roles and to users in the same organization. Every user can also use it to read their own details. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN' It returns a JSON object representing the user as described previously .","title":"Get Details"},{"location":"cortex/api/api-guide/#set-a-password","text":"This call sets the user's password. It's available to users with superAdmin or orgAdmin roles. Please note that the request needs to be made using HTTPS with a valid certificate on the server's end to prevent credential sniffing or other PITM (Person-In-The-Middle) attacks. curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/password/set' -d '{ \"password\": \"SOMEPASSWORD\" }' If successful, the call returns 204 (success / no content).","title":"Set a Password"},{"location":"cortex/api/api-guide/#change-a-password","text":"This call allows a given user to change only their own existing password. It is available to all users including superAdmin and orgAdmin ones. Please note that if a superAdmin or an orgAdmin needs to update the password of another user, they must use the /password/set call described in the previous subsection. curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/password/change' -d '{ \"currentPassword\": \"password\", \"password\": \"new-password\" }' If successful, the call returns 204 (success / no content).","title":"Change a password"},{"location":"cortex/api/api-guide/#set-and-renew-an-api-key","text":"This calls allows setting and renewing the API key of a user. It's available to users with superAdmin or orgAdmin roles. Any user can also use it to renew their own API key. Again, the request needs to be made using HTTPS with a valid certificate on the server's end to prevent credential sniffing or other PITM (Person-In-The-Middle) attacks. You know the drill ;-) curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/key/renew' If successful, it returns the generated API key in a text/plain response.","title":"Set and Renew an API Key"},{"location":"cortex/api/api-guide/#get-an-api-key","text":"This calls allows getting a user's API key. It's available to users with superAdmin or orgAdmin roles. Any user can also use it to obtain their own API key. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/key' If successful, the generated API key is returned in text/plain response","title":"Get an API Key"},{"location":"cortex/api/api-guide/#revoke-an-api-key","text":"This calls allow revoking a user's API key. This calls allow revoking a user's API key. curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/key' A successful request returns nothing (HTTP 200 OK).","title":"Revoke an API Key"},{"location":"cortex/api/api-guide/#job-apis","text":"The following section describes the APIs that allow manipulating jobs. Jobs are basically submissions made to analyzers and the resulting reports.","title":"Job APIs"},{"location":"cortex/api/api-guide/#job-model","text":"A job is defined by the following attributes: Attribute Description Type id Job ID computed organization The organization to which the job belongs readonly analyzerDefinitionId Analyzer definition name readonly analyzerId Instance ID of the analyzer to which the job is associated readonly organization Organization to which the user belongs (set upon account creation) readonly analyzerName Name of the analyzer to which the job is associated readonly dataType the datatype of the analyzed observable readonly status Status of the job ( Waiting , InProgress , Success , Failure , Deleted ) computed data Value of the analyzed observable (does not apply to file observables) readonly attachment JSON object representing file observables (does not apply to non- file observables). It defines the name , hashes , size , contentType and id of the file observable readonly parameters JSON object of key/value pairs set during job creation readonly message A free text field to set additional text/context for a job readonly tlp The TLP of the analyzed observable readonly startDate Start date computed endDate End date computed createdAt Creation date. Please note that a job can be requested but not immediately honored. The actual time at which it is started is the value of startDate computed createdBy User who created the job computed updatedAt Last update date (only Cortex updates a job when it finishes) computed updatedBy User who submitted the job and which identity is used by Cortex to update the job once it is finished computed","title":"Job Model"},{"location":"cortex/api/api-guide/#list-and-search","text":"This call allows a user with read , analyze or orgAdmin role to list and search all the analysis jobs made by their organization. If you want to list all the jobs: curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/_search?range=all' If you want to list 10 jobs: curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/_search' If you want to list 100 jobs: curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/_search?range=0-100' If you want to search jobs according to various criteria: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/job/_search' -d '{ \"query\": { \"_and\": [ {\"status\": \"Success\"}, {\"dataType\": \"ip\"} ] } }' This call supports the range and sort query parameters declared in paging and sorting details","title":"List and Search"},{"location":"cortex/api/api-guide/#get-details_1","text":"This call allows a user with read , analyze or orgAdmin role to get the details of a job. It does not fetch the job report. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID' It returns a JSON response with the following structure: { \"id\" : \"AWNei4vH3rJ8unegCPB9\" , \"analyzerDefinitionId\" : \"Abuse_Finder_2_0\" , \"analyzerId\" : \"220483fde9608c580fb6a2508ff3d2d3\" , \"analyzerName\" : \"Abuse_Finder_2_0\" , \"status\" : \"Success\" , \"data\" : \"8.8.8.8\" , \"parameters\" : \"{}\" , \"tlp\" : 0 , \"message\" : \"\" , \"dataType\" : \"ip\" , \"organization\" : \"demo\" , \"startDate\" : 1526299593923 , \"endDate\" : 1526299597064 , \"date\" : 1526299593633 , \"createdAt\" : 1526299593633 , \"createdBy\" : \"demo\" , \"updatedAt\" : 1526299597066 , \"updatedBy\" : \"demo\" }","title":"Get Details"},{"location":"cortex/api/api-guide/#get-details-and-report","text":"This call allows a user with read , analyze or orgAdmin role to get the details of a job including its report. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID/report' It returns a JSON response with the structure below. If the job is not yet completed, the report field contains a string representing the job status: { \"id\" : \"AWNei4vH3rJ8unegCPB9\" , \"analyzerDefinitionId\" : \"Abuse_Finder_2_0\" , \"analyzerId\" : \"220483fde9608c580fb6a2508ff3d2d3\" , \"analyzerName\" : \"Abuse_Finder_2_0\" , \"status\" : \"Success\" , \"data\" : \"8.8.8.8\" , \"parameters\" : \"{}\" , \"tlp\" : 0 , \"message\" : \"\" , \"dataType\" : \"ip\" , \"organization\" : \"demo\" , \"startDate\" : 1526299593923 , \"endDate\" : 1526299597064 , \"date\" : 1526299593633 , \"createdAt\" : 1526299593633 , \"createdBy\" : \"demo\" , \"updatedAt\" : 1526299597066 , \"updatedBy\" : \"demo\" , \"report\" : { \"summary\" : { \"taxonomies\" : [ { \"predicate\" : \"Address\" , \"namespace\" : \"Abuse_Finder\" , \"value\" : \"network-abuse@google.com\" , \"level\" : \"info\" } ] }, \"full\" : { \"abuse_finder\" : { \"raw\" : \"...\" , \"abuse\" : [ \"network-abuse@google.com\" ], \"names\" : [ \"Google LLC\" , \"Level 3 Parent, LLC\" ], \"value\" : \"8.8.8.8\" } }, \"success\" : true , \"artifacts\" : [] } }","title":"Get Details and Report"},{"location":"cortex/api/api-guide/#wait-and-get-job-report","text":"This call is similar the one described above but allows the user to provide a timeout to wait for the report in case it is not available at the time the query was made: curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID/waitreport?atMost=1minute' The atMost is a duration using the format Xhour , Xminute or Xsecond .","title":"Wait and Get Job Report"},{"location":"cortex/api/api-guide/#get-artifacts","text":"This call allows a user with read , analyze or orgAdmin role to get the extracted artifacts from a job if such extraction has been enabled in the corresponding analyzer configuration. Please note that extraction is imperfect and you might have inconsistent or incorrect data. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID/artifacts' It returns a JSON array with the following structure: [ { \"dataType\" : \"ip\" , \"createdBy\" : \"demo\" , \"data\" : \"8.8.8.8\" , \"tlp\" : 0 , \"createdAt\" : 1525432900553 , \"id\" : \"AWMq4tvLjidKq_asiwcl\" } ]","title":"Get Artifacts"},{"location":"cortex/api/api-guide/#delete_1","text":"This API allows a user with analyze or orgAdmin role to delete a job: curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID' This marks the job as Deleted . However the job's data is not removed from the database.","title":"Delete"},{"location":"cortex/api/api-guide/#analyzer-apis","text":"The following section describes the APIs that allow manipulating analyzers.","title":"Analyzer APIs"},{"location":"cortex/api/api-guide/#analyzer-model","text":"An analyzer is defined by the following attributes: Attribute Description Type id Analyzer ID once enabled within an organization readonly analyzerDefinitionId Analyzer definition name readonly name Name of the analyzer readonly version Version of the analyzer readonly description Description of the analyzer readonly author Author of the analyzer readonly url URL where the analyzer has been published readonly license License of the analyzer readonly dataTypeList Allowed datatypes readonly baseConfig Base configuration name. This identifies the shared set of configuration with all the analyzer's flavors readonly jobCache Report cache timeout in minutes, visible for orgAdmin users only writable rate Numeric amount of analyzer calls authorized for the specified rateUnit , visible for orgAdmin users only writable rateUnit Period of availability of the rate limite: Day or Month , visible for orgAdmin users only writable configuration A JSON object where key/value pairs represent the config names, and their values. It includes the default properties proxy_http , proxy_https , auto_extract_artifacts , check_tlp , and max_tlp , visible for orgAdmin users only writable createdBy User who enabled the analyzer computed updatedAt Last update date computed updatedBy User who last updated the analyzer computed","title":"Analyzer Model"},{"location":"cortex/api/api-guide/#enable","text":"This call allows a user with an orgAdmin role to enable an analyzer. curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/analyzer/:analyzerId' -d '{ \"name\": \"Censys_1_0\", \"configuration\": { \"uid\": \"XXXX\", \"key\": \"XXXXXXXXXXXXXXXXXXXX\", \"proxy_http\": \"http://proxy:9999\", \"proxy_https\": \"http://proxy:9999\", \"auto_extract_artifacts\": false, \"check_tlp\": true, \"max_tlp\": 2 }, \"rate\": 1000, \"rateUnit\": \"Day\", \"jobCache\": 5 }'","title":"Enable"},{"location":"cortex/api/api-guide/#list-and-search_1","text":"These calls allow a user with a analyze or orgAdmin role to list and search all the enabled analyzers within the organization. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer' or curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/_search' -d '{ \"query\": {} }' Both calls supports the range and sort query parameters declared in paging and sorting details , and both return a JSON array of analyzer objects as described in Analyzer Model section . If called by a user with only an nalyzer role, the configuration attribute is not included on the JSON objects.","title":"List and Search"},{"location":"cortex/api/api-guide/#get-details_2","text":"This call allows a user with a analyze or orgAdmin role to get an analyzer's details. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID' It returns a analyzer JSON object as described in Analyzer Model section . If called by a user with only an nalyzer role, the configuration attribute is not included on the JSON objects.","title":"Get Details"},{"location":"cortex/api/api-guide/#get-by-type","text":"This call is mostly used by TheHive and allows to quickly get the list of analyzers that can run on the given datatype. It requires an analyze or orgAdmin role. curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer/type/DATA_TYPE' It returns a JSON array of analyzer objects as described in Analyzer Model section without the configuration attribute, which could contain sensitive data.","title":"Get By Type"},{"location":"cortex/api/api-guide/#update_2","text":"This call allows an orgAdmin user to update the name , configuration and jobCache of an enabled analyzer. curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID' -d '{ \"configuration\": { \"key\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\", \"polling_interval\": 60, \"proxy_http\": \"http://localhost:8080\", \"proxy_https\": \"http://localhost:8080\", \"auto_extract_artifacts\": true, \"check_tlp\": true, \"max_tlp\": 1 }, \"name\": \"Shodan_Host_1_0\", \"rate\": 1000, \"rateUnit\": \"Day\", \"jobCache\": null }' It returns a JSON object describing the analyzer as defined in Analyzer Model section .","title":"Update"},{"location":"cortex/api/api-guide/#run","text":"This API allows a user with a analyze or orgAdmin role to run analyzers on observables of different datatypes. For file observables, the API call must be made as described below: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID/run' \\ -F 'attachment=@/path/to/observable-file' \\ -F '_json=<-;type=application/json' << _EOF_ { \"dataType\":\"file\", \"tlp\":0 } _EOF_ for all the other types of observerables, the request is: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID/run' -d '{ \"data\":\"8.8.8.8\", \"dataType\":\"ip\", \"tlp\":0, \"message\": \"A message that can be accessed from the analyzer\", \"parameters\": { \"key1\": \"value1\", \"key2\": \"value2\" } }' This call will fetch a similar job from the cache, and if it finds one, it returns it from the cache, based on the duration defined in jobCache attribute of the analyzer. To force bypassing the cache, one can add the following query parameter: force=1 curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID/run?force=1' -d '{ \"data\":\"8.8.8.8\", \"dataType\":\"ip\", \"tlp\":0, \"message\": \"A message that can be accessed from the analyzer\", \"parameters\": { \"key1\": \"value1\", \"key2\": \"value2\" } }'","title":"Run"},{"location":"cortex/api/api-guide/#disable","text":"This API allows an orgAdmin to disable an existing analyzer in their organization and delete the corresponding configuration. curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID'","title":"Disable"},{"location":"cortex/api/api-guide/#miscellaneous-apis","text":"","title":"Miscellaneous APIs"},{"location":"cortex/api/api-guide/#paging-and-sorting","text":"All the search API calls allow sorting and paging parameters, in addition to a query in the request's body. These calls usually have URLs ending with the _search keyword but that's not always the case. The followings are query parameters: range : all or x-y where x and y are numbers (ex: 0-10). sort : you can provide multiple sort criteria such as: -createdAt or +status . Example: curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'http://CORTEX_APP_URL:9001/api/organization/ORG_ID/user?range=0-10&sort=-createdAt&sort=+status' -d '{ \"query\": {} }'","title":"Paging and Sorting"},{"location":"cortex/api/how-to-create-a-responder/","text":"How to Write and Submit a Responder # Table of Contents # Writing a Responder The Program Service Interaction Files (Flavors) Python Requirements Example: Mailer Responder Files Input Service Interaction Configuration Items Responder Configuration in the Global Configuration File Output The Cortexutils Python Library Submitting a Responder Check Existing Issues Open an Issue Review your Service Interaction File(s) Provide the List of Requirements Verify Execution Create a Pull Request Need Help? Writing a Responder # A responder is a program that takes JSON input and do an action and produces a basic result of that action. Responders are very similar to analyzers though they have different purposes. Responders are made of at least 2 types of files: The program itself One or several service interaction files or flavors A Python requirements file, which is only necessary if the responder is written in Python. The Program # The first type of files a responder is made of is the core program that performs actions. It can be written in any programming language that is supported by Linux. you can write your responders in Python, Ruby, Perl or even Scala. However, the very handy Cortexutils library described below is in Python. It greatly facilitates responder development and it also provides some methods to quickly format the output to make it compliant with the JSON schema expected by TheHive . Service Interaction Files (Flavors) # A responder must have at least one service interaction file. Such files contain key configuration information such as the responder's author information, the datatypes ( thehive:case , thehive:alert , ...) the responder accepts as input and to which it applies to if used from TheHive, the TLP and PAP (or Permissible Actions Protocol ) above which it will refuse to execute to protect against data leakage and to enforce sane OPSEC practices and so on. A responder can have two or more service interaction files to allow it to perform different actions. We speak then of flavors. For example, a Mailer responder can send message using several body templates. Python Requirements # If the responder is written in Python, a requirements.txt must be provided with the list of all the dependencies. Example: Mailer Responder Files # Below is a directory listing of the files corresponding to a Mailer responder. responders/Mailer | -- Mailer.json | -- requirements.txt ` -- mailer.py Input # The input of a responder can be any JSON data, even a simple string. The submitter must send data with the structure expected by the program. The acceptable datatypes described in the Service Interaction files indicate what kind of data is expected. For example, if the program requires a thehive:case (i.e. it applies at the case level in TheHive), input must comply with TheHive case. Below an example of thehive:case input. { \"data\" : { \"updatedAt\" : 1606230814019 , \"tlp\" : 2 , \"endDate\" : 1606230814019 , \"description\" : \"Case Description\" , \"tags\" : [ \"tag\" ], \"caseId\" : 157 , \"customFields\" : {}, \"pap\" : 2 , \"status\" : \"Open\" , \"resolutionStatus\" : \"Indeterminate\" , \"createdAt\" : 1606183201646 , \"createdBy\" : \"user\" , \"flag\" : false , \"severity\" : 2 , \"metrics\" : {}, \"owner\" : \"user\" , \"title\" : \"Title\" , \"updatedBy\" : \"user\" , \"startDate\" : 1606183201000 , \"impactStatus\" : \"NotApplicable\" , \"_type\" : \"case\" , \"_routing\" : \"iwD693UBlJefU8pMrqOq\" , \"_parent\" : null , \"_id\" : \"iwD693UBlJefU8pMrqOq\" , \"_seqNo\" : 4572 , \"_primaryTerm\" : 48 , \"id\" : \"iwD693UBlJefU8pMrqOq\" }, \"dataType\" : \"thehive:case\" , \"tlp\" : 2 , \"pap\" : 2 , \"message\" : \"\" , \"parameters\" : { \"user\" : \"user\" }, \"config\" : { \"proxy_https\" : null , \"cacerts\" : null , \"max_pap\" : 2 , \"jobTimeout\" : 30 , \"api_key\" : \"3bDBUb7EL409MHOmXBkqsysZ1vpTab1Q\" , \"check_tlp\" : true , \"proxy_http\" : null , \"max_tlp\" : 2 , \"url\" : \"configured_url\" , \"check_pap\" : true } } In the addition to the input ( data section) sent by the submitter, Cortex adds the config section which is the responder's specific configuration provided by an orgAdmin user when the responder is enabled in the Cortex UI. Example: Service Interaction File for the Mailer Responder # The <== sign and anything after it are comments that do no appear in the original file. { \"name\" : \"Mailer\" , \"version\" : \"1.0\" , \"author\" : \"CERT-BDF\" , \"url\" : \"https://github.com/TheHive-Project/Cortex-Analyzers\" , \"license\" : \"AGPL-V3\" , \"description\" : \"Send an email with information from a TheHive case or alert\" , \"dataTypeList\" : [ \"thehive:case\" , \"thehive:alert\" , \"thehive:case_task\" ], \"command\" : \"Mailer/mailer.py\" , \"baseConfig\" : \"Mailer\" , \"configurationItems\" : [ { \"name\" : \"from\" , \"description\" : \"email address from which the mail is send\" , \"type\" : \"string\" , <== de f i nes wha t ki n d o f da ta t ype t he co nf igura t io n i te m is (s tr i n g , nu mber) \"multi\" : false , <== se tt i n g mul t i t o true allows t o pass a lis t o f i te ms \"required\" : true }, { \"name\" : \"smtp_host\" , \"description\" : \"SMTP server used to send mail\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : true , \"defaultValue\" : \"localhost\" }, { \"name\" : \"smtp_port\" , \"description\" : \"SMTP server port\" , \"type\" : \"number\" , \"multi\" : false , \"required\" : true , \"defaultValue\" : 25 }, { \"name\" : \"smtp_user\" , \"description\" : \"SMTP server user\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : \"user\" }, { \"name\" : \"smtp_pwd\" , \"description\" : \"SMTP server password\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : \"pwd\" } ] } Service Interaction Configuration Items # name # Name of the specific service (or flavor) of the responder. If your responder has only one service interaction (i.e. performs only one action), it is the name of the responder's directory. If your responder performs several actions (i.e. comes in several flavors), you have to give a specific and meaningful name to each flavor. Each flavor's name appear in TheHive's responder list and in MISP when you use Cortex for attribute enrichment. version # The version of the responder. You must increase major version numbers when new features are added, modifications are made to take into account API changes, report output is modified or when report templates (more on this later) are updated. You must increase minor version numbers when bugs are fixed. author # You must provide your full name and/or your organization/team name when submitting a responder. Pseudos are not accepted. If you'd rather remain anonymous, please contact us at support@thehive-project.org prior to submitting your responder. url # The URL where the responder is stored. This should ideally be https://github.com/TheHive-Project/Cortex-Analyzers license # The license of the code. Ideally, we recommend using the AGPL-v3 license. Make sure your code's license is compatible with the license(s) of the various components and libraries you use if applicable. description # Description of the responder. Please be concise and clear. The description is shown in the Cortex UI and TheHive. dataTypeList # The list of TheHive datatypes supported by the responder. Currently TheHive accepts the following datatypes: thehive:case thehive:case_artifact (i.e. observable) thehive:alert thehive:case_task thehive:case_task_log (i.e. task log) baseConfig # Name used to group configuration items common to several responders. This prevent the user to enter the same API key for all responder flavors. The Cortex responder config page group configuration items by their baseConfig . config # Configuration dedicated to the responder's flavor. This is where we typically specify the TLP level of observables allowed to be analyzed with the check_tlp and max_tlp parameters. For example, if max_tlp is set to 2 (TLP:AMBER), TLP:RED observables cannot be analyzed. max_tlp # The TLP level above which the responder must not be executed. TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3 check_tlp # This is a boolean parameter. When true , max_tlp is checked. And if the input's TLP is above max_tlp , the responder is not executed. For consistency reasons, we do recommend setting both check_tlp and max_tlp even if check_tlp is set to false . max_pap # The PAP level above which the responder must not be executed. TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3 check_pap # This is a boolean parameter. When true , max_pap is checked. And if the input's PAP is above max_pap , the responder is not executed. For consistency reasons, we do recommend setting both check_pap and max_pap even if check_pap is set to false . command # The command used to run the responder. That's typically the full, absolute path to the main program file. configurationItems # The list of configurationItems is necessary in order to be able to set all configuration variables for responders directly in the Cortex 2 user interface. As in the VirusTotal example above can be seen, every item is a json object that defines: - name (string) - description (string) - type (string) - multi (boolean) - required (boolean) - defaultValue (according to type, optional) The multi parameter allows to pass a list as configuration variable instead of a single string or number. This is used e.g. in the MISP responder that queries multiple servers in one run and needs different parameters for that. Output # The output of a responder depends on the success or failure of its execution. If the responder fails to execute: { \"success\" : false , \"errorMessage\" : \"..\" } When success is set to false , it indicates that something went wrong during the execution. errorMessage is free text - typically the error output message. If the responder succeeds (i.e. it runs without any error): { \"success\" : true , \"full\" :{ \"message\" : \"..\" }, \"operations\" :[] } When success is set to true , it indicates that the responder ran successfully. full is the full report of the responder. It must contain at least a message. operations is a list what the submitter system should execute. As of version 3.1.0, TheHive accepts the following operations: AddTagToArtifact ( { \"type\": \"AddTagToArtifact\", \"tag\": \"tag to add\" } ): add a tag to the artifact related to the object AddTagToCase ( { \"type\": \"AddTagToCase\", \"tag\": \"tag to add\" } ): add a tag to the case related to the object MarkAlertAsRead : mark the alert related to the object as read AddCustomFields ( {\"name\": \"key\", \"value\": \"value\", \"tpe\": \"type\" ): add a custom field to the case related to the object The list of acceptable operations will increase in future releases of TheHive. The Cortexutils Python Library # So far, all the published responders have been written in Python. We provide a Python library called cortexutils to help developers easily write their programs. Note though that Python is not mandatory for responder coding and any language that runs on Linux can be used, though you won't have the benefits of the CortexUtils library. Cortexutils can be used with Python 2 and 3. Due to the end of life from Python2 it is strongly advised to work as much with Python3 as possible. To install it : pip install cortexutils or pip3 install cortexutils This library is already used by all the responders published in our Github repository . Feel free to start reading the code of some of them before writing your own. Submitting a Responder # We highly encourage you to share your responders with the community through our Github repository. To do so, we invite you to follow a few steps before submitting a pull request. Check Existing Issues # Start by checking if an issue already exists for the responder you'd like to write and contribute. Verify that nobody is working on it. If an issue exists and has the in progress , under review or pr-submitted label, it means somebody is already working on the code or has finished it. If you are short on ideas, check issues with a help wanted label . If one of those issues interest you, indicate that you are working on it. Open an Issue # If there's no issue open for the responder you'd like to contribute, open one . Indicate that you are working on it to avoid having someone start coding it. You have to create an issue for each responder you'd like to submit. Review your Service Interaction File(s) # Review your service interaction files. For example, let's check the Mailer JSON responder configuration file(s): { \"name\" : \"Mailer\" , \"version\" : \"1.0\" , \"author\" : \"CERT-BDF\" , \"url\" : \"https://github.com/TheHive-Project/Cortex-Analyzers\" , \"license\" : \"AGPL-V3\" , \"description\" : \"Send an email with information from a TheHive case or alert\" , \"dataTypeList\" : [ \"thehive:case\" , \"thehive:alert\" , \"thehive:case_task\" ], \"command\" : \"Mailer/mailer.py\" , \"baseConfig\" : \"Mailer\" , \"configurationItems\" : [ { \"name\" : \"from\" , \"description\" : \"email address from which the mail is send\" , \"type\" : \"string\" , <== de f i nes wha t ki n d o f da ta t ype t he co nf igura t io n i te m is (s tr i n g , nu mber) \"multi\" : false , <== se tt i n g mul t i t o true allows t o pass a lis t o f i te ms \"required\" : true }, { \"name\" : \"smtp_host\" , \"description\" : \"SMTP server used to send mail\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : true , \"defaultValue\" : \"localhost\" }, { \"name\" : \"smtp_port\" , \"description\" : \"SMTP server port\" , \"type\" : \"number\" , \"multi\" : false , \"required\" : true , \"defaultValue\" : 25 }, { \"name\" : \"smtp_user\" , \"description\" : \"SMTP server user\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : \"user\" }, { \"name\" : \"smtp_pwd\" , \"description\" : \"SMTP server password\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : \"pwd\" } ] } Ensure that all information is correct and particularly the author and license parameters. Provide the List of Requirements # If your responder is written in Python, make sure to complete the requirements.txt file with the list of all the external libraries that are needed to run the responder correctly. Verify Execution # Use these three simple checks before submitting your responder: Ensure it works with the expected configuration, TLP, PAP or datatype. Ensure it works with missing configuration, PAP, datatype or TLP: your responder must generate an explicit error message. Create a Pull Request # Create one Pull Request per responder against the develop branch of the Cortex-Analyzers repository. Reference the issue you've created in your PR. We have to review your responders. Distinct PRs will allow us to review them more quickly and release them to the benefit of the whole community. Need Help? # Something does not work as expected? No worries, we got you covered. Please join our user forum , contact us on Gitter , or send us an email at support@thehive-project.org . We are here to help.","title":"How to Write and Submit a Responder"},{"location":"cortex/api/how-to-create-a-responder/#how-to-write-and-submit-a-responder","text":"","title":"How to Write and Submit a Responder"},{"location":"cortex/api/how-to-create-a-responder/#table-of-contents","text":"Writing a Responder The Program Service Interaction Files (Flavors) Python Requirements Example: Mailer Responder Files Input Service Interaction Configuration Items Responder Configuration in the Global Configuration File Output The Cortexutils Python Library Submitting a Responder Check Existing Issues Open an Issue Review your Service Interaction File(s) Provide the List of Requirements Verify Execution Create a Pull Request Need Help?","title":"Table of Contents"},{"location":"cortex/api/how-to-create-a-responder/#writing-a-responder","text":"A responder is a program that takes JSON input and do an action and produces a basic result of that action. Responders are very similar to analyzers though they have different purposes. Responders are made of at least 2 types of files: The program itself One or several service interaction files or flavors A Python requirements file, which is only necessary if the responder is written in Python.","title":"Writing a Responder"},{"location":"cortex/api/how-to-create-a-responder/#the-program","text":"The first type of files a responder is made of is the core program that performs actions. It can be written in any programming language that is supported by Linux. you can write your responders in Python, Ruby, Perl or even Scala. However, the very handy Cortexutils library described below is in Python. It greatly facilitates responder development and it also provides some methods to quickly format the output to make it compliant with the JSON schema expected by TheHive .","title":"The Program"},{"location":"cortex/api/how-to-create-a-responder/#service-interaction-files-flavors","text":"A responder must have at least one service interaction file. Such files contain key configuration information such as the responder's author information, the datatypes ( thehive:case , thehive:alert , ...) the responder accepts as input and to which it applies to if used from TheHive, the TLP and PAP (or Permissible Actions Protocol ) above which it will refuse to execute to protect against data leakage and to enforce sane OPSEC practices and so on. A responder can have two or more service interaction files to allow it to perform different actions. We speak then of flavors. For example, a Mailer responder can send message using several body templates.","title":"Service Interaction Files (Flavors)"},{"location":"cortex/api/how-to-create-a-responder/#python-requirements","text":"If the responder is written in Python, a requirements.txt must be provided with the list of all the dependencies.","title":"Python Requirements"},{"location":"cortex/api/how-to-create-a-responder/#example-mailer-responder-files","text":"Below is a directory listing of the files corresponding to a Mailer responder. responders/Mailer | -- Mailer.json | -- requirements.txt ` -- mailer.py","title":"Example: Mailer Responder Files"},{"location":"cortex/api/how-to-create-a-responder/#input","text":"The input of a responder can be any JSON data, even a simple string. The submitter must send data with the structure expected by the program. The acceptable datatypes described in the Service Interaction files indicate what kind of data is expected. For example, if the program requires a thehive:case (i.e. it applies at the case level in TheHive), input must comply with TheHive case. Below an example of thehive:case input. { \"data\" : { \"updatedAt\" : 1606230814019 , \"tlp\" : 2 , \"endDate\" : 1606230814019 , \"description\" : \"Case Description\" , \"tags\" : [ \"tag\" ], \"caseId\" : 157 , \"customFields\" : {}, \"pap\" : 2 , \"status\" : \"Open\" , \"resolutionStatus\" : \"Indeterminate\" , \"createdAt\" : 1606183201646 , \"createdBy\" : \"user\" , \"flag\" : false , \"severity\" : 2 , \"metrics\" : {}, \"owner\" : \"user\" , \"title\" : \"Title\" , \"updatedBy\" : \"user\" , \"startDate\" : 1606183201000 , \"impactStatus\" : \"NotApplicable\" , \"_type\" : \"case\" , \"_routing\" : \"iwD693UBlJefU8pMrqOq\" , \"_parent\" : null , \"_id\" : \"iwD693UBlJefU8pMrqOq\" , \"_seqNo\" : 4572 , \"_primaryTerm\" : 48 , \"id\" : \"iwD693UBlJefU8pMrqOq\" }, \"dataType\" : \"thehive:case\" , \"tlp\" : 2 , \"pap\" : 2 , \"message\" : \"\" , \"parameters\" : { \"user\" : \"user\" }, \"config\" : { \"proxy_https\" : null , \"cacerts\" : null , \"max_pap\" : 2 , \"jobTimeout\" : 30 , \"api_key\" : \"3bDBUb7EL409MHOmXBkqsysZ1vpTab1Q\" , \"check_tlp\" : true , \"proxy_http\" : null , \"max_tlp\" : 2 , \"url\" : \"configured_url\" , \"check_pap\" : true } } In the addition to the input ( data section) sent by the submitter, Cortex adds the config section which is the responder's specific configuration provided by an orgAdmin user when the responder is enabled in the Cortex UI.","title":"Input"},{"location":"cortex/api/how-to-create-a-responder/#example-service-interaction-file-for-the-mailer-responder","text":"The <== sign and anything after it are comments that do no appear in the original file. { \"name\" : \"Mailer\" , \"version\" : \"1.0\" , \"author\" : \"CERT-BDF\" , \"url\" : \"https://github.com/TheHive-Project/Cortex-Analyzers\" , \"license\" : \"AGPL-V3\" , \"description\" : \"Send an email with information from a TheHive case or alert\" , \"dataTypeList\" : [ \"thehive:case\" , \"thehive:alert\" , \"thehive:case_task\" ], \"command\" : \"Mailer/mailer.py\" , \"baseConfig\" : \"Mailer\" , \"configurationItems\" : [ { \"name\" : \"from\" , \"description\" : \"email address from which the mail is send\" , \"type\" : \"string\" , <== de f i nes wha t ki n d o f da ta t ype t he co nf igura t io n i te m is (s tr i n g , nu mber) \"multi\" : false , <== se tt i n g mul t i t o true allows t o pass a lis t o f i te ms \"required\" : true }, { \"name\" : \"smtp_host\" , \"description\" : \"SMTP server used to send mail\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : true , \"defaultValue\" : \"localhost\" }, { \"name\" : \"smtp_port\" , \"description\" : \"SMTP server port\" , \"type\" : \"number\" , \"multi\" : false , \"required\" : true , \"defaultValue\" : 25 }, { \"name\" : \"smtp_user\" , \"description\" : \"SMTP server user\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : \"user\" }, { \"name\" : \"smtp_pwd\" , \"description\" : \"SMTP server password\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : \"pwd\" } ] }","title":"Example: Service Interaction File for the Mailer Responder"},{"location":"cortex/api/how-to-create-a-responder/#service-interaction-configuration-items","text":"","title":"Service Interaction Configuration Items"},{"location":"cortex/api/how-to-create-a-responder/#name","text":"Name of the specific service (or flavor) of the responder. If your responder has only one service interaction (i.e. performs only one action), it is the name of the responder's directory. If your responder performs several actions (i.e. comes in several flavors), you have to give a specific and meaningful name to each flavor. Each flavor's name appear in TheHive's responder list and in MISP when you use Cortex for attribute enrichment.","title":"name"},{"location":"cortex/api/how-to-create-a-responder/#version","text":"The version of the responder. You must increase major version numbers when new features are added, modifications are made to take into account API changes, report output is modified or when report templates (more on this later) are updated. You must increase minor version numbers when bugs are fixed.","title":"version"},{"location":"cortex/api/how-to-create-a-responder/#author","text":"You must provide your full name and/or your organization/team name when submitting a responder. Pseudos are not accepted. If you'd rather remain anonymous, please contact us at support@thehive-project.org prior to submitting your responder.","title":"author"},{"location":"cortex/api/how-to-create-a-responder/#url","text":"The URL where the responder is stored. This should ideally be https://github.com/TheHive-Project/Cortex-Analyzers","title":"url"},{"location":"cortex/api/how-to-create-a-responder/#license","text":"The license of the code. Ideally, we recommend using the AGPL-v3 license. Make sure your code's license is compatible with the license(s) of the various components and libraries you use if applicable.","title":"license"},{"location":"cortex/api/how-to-create-a-responder/#description","text":"Description of the responder. Please be concise and clear. The description is shown in the Cortex UI and TheHive.","title":"description"},{"location":"cortex/api/how-to-create-a-responder/#datatypelist","text":"The list of TheHive datatypes supported by the responder. Currently TheHive accepts the following datatypes: thehive:case thehive:case_artifact (i.e. observable) thehive:alert thehive:case_task thehive:case_task_log (i.e. task log)","title":"dataTypeList"},{"location":"cortex/api/how-to-create-a-responder/#baseconfig","text":"Name used to group configuration items common to several responders. This prevent the user to enter the same API key for all responder flavors. The Cortex responder config page group configuration items by their baseConfig .","title":"baseConfig"},{"location":"cortex/api/how-to-create-a-responder/#config","text":"Configuration dedicated to the responder's flavor. This is where we typically specify the TLP level of observables allowed to be analyzed with the check_tlp and max_tlp parameters. For example, if max_tlp is set to 2 (TLP:AMBER), TLP:RED observables cannot be analyzed.","title":"config"},{"location":"cortex/api/how-to-create-a-responder/#max_tlp","text":"The TLP level above which the responder must not be executed. TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3","title":"max_tlp"},{"location":"cortex/api/how-to-create-a-responder/#check_tlp","text":"This is a boolean parameter. When true , max_tlp is checked. And if the input's TLP is above max_tlp , the responder is not executed. For consistency reasons, we do recommend setting both check_tlp and max_tlp even if check_tlp is set to false .","title":"check_tlp"},{"location":"cortex/api/how-to-create-a-responder/#max_pap","text":"The PAP level above which the responder must not be executed. TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3","title":"max_pap"},{"location":"cortex/api/how-to-create-a-responder/#check_pap","text":"This is a boolean parameter. When true , max_pap is checked. And if the input's PAP is above max_pap , the responder is not executed. For consistency reasons, we do recommend setting both check_pap and max_pap even if check_pap is set to false .","title":"check_pap"},{"location":"cortex/api/how-to-create-a-responder/#command","text":"The command used to run the responder. That's typically the full, absolute path to the main program file.","title":"command"},{"location":"cortex/api/how-to-create-a-responder/#configurationitems","text":"The list of configurationItems is necessary in order to be able to set all configuration variables for responders directly in the Cortex 2 user interface. As in the VirusTotal example above can be seen, every item is a json object that defines: - name (string) - description (string) - type (string) - multi (boolean) - required (boolean) - defaultValue (according to type, optional) The multi parameter allows to pass a list as configuration variable instead of a single string or number. This is used e.g. in the MISP responder that queries multiple servers in one run and needs different parameters for that.","title":"configurationItems"},{"location":"cortex/api/how-to-create-a-responder/#output","text":"The output of a responder depends on the success or failure of its execution. If the responder fails to execute: { \"success\" : false , \"errorMessage\" : \"..\" } When success is set to false , it indicates that something went wrong during the execution. errorMessage is free text - typically the error output message. If the responder succeeds (i.e. it runs without any error): { \"success\" : true , \"full\" :{ \"message\" : \"..\" }, \"operations\" :[] } When success is set to true , it indicates that the responder ran successfully. full is the full report of the responder. It must contain at least a message. operations is a list what the submitter system should execute. As of version 3.1.0, TheHive accepts the following operations: AddTagToArtifact ( { \"type\": \"AddTagToArtifact\", \"tag\": \"tag to add\" } ): add a tag to the artifact related to the object AddTagToCase ( { \"type\": \"AddTagToCase\", \"tag\": \"tag to add\" } ): add a tag to the case related to the object MarkAlertAsRead : mark the alert related to the object as read AddCustomFields ( {\"name\": \"key\", \"value\": \"value\", \"tpe\": \"type\" ): add a custom field to the case related to the object The list of acceptable operations will increase in future releases of TheHive.","title":"Output"},{"location":"cortex/api/how-to-create-a-responder/#the-cortexutils-python-library","text":"So far, all the published responders have been written in Python. We provide a Python library called cortexutils to help developers easily write their programs. Note though that Python is not mandatory for responder coding and any language that runs on Linux can be used, though you won't have the benefits of the CortexUtils library. Cortexutils can be used with Python 2 and 3. Due to the end of life from Python2 it is strongly advised to work as much with Python3 as possible. To install it : pip install cortexutils or pip3 install cortexutils This library is already used by all the responders published in our Github repository . Feel free to start reading the code of some of them before writing your own.","title":"The Cortexutils Python Library"},{"location":"cortex/api/how-to-create-a-responder/#submitting-a-responder","text":"We highly encourage you to share your responders with the community through our Github repository. To do so, we invite you to follow a few steps before submitting a pull request.","title":"Submitting a Responder"},{"location":"cortex/api/how-to-create-a-responder/#check-existing-issues","text":"Start by checking if an issue already exists for the responder you'd like to write and contribute. Verify that nobody is working on it. If an issue exists and has the in progress , under review or pr-submitted label, it means somebody is already working on the code or has finished it. If you are short on ideas, check issues with a help wanted label . If one of those issues interest you, indicate that you are working on it.","title":"Check Existing Issues"},{"location":"cortex/api/how-to-create-a-responder/#open-an-issue","text":"If there's no issue open for the responder you'd like to contribute, open one . Indicate that you are working on it to avoid having someone start coding it. You have to create an issue for each responder you'd like to submit.","title":"Open an Issue"},{"location":"cortex/api/how-to-create-a-responder/#review-your-service-interaction-files","text":"Review your service interaction files. For example, let's check the Mailer JSON responder configuration file(s): { \"name\" : \"Mailer\" , \"version\" : \"1.0\" , \"author\" : \"CERT-BDF\" , \"url\" : \"https://github.com/TheHive-Project/Cortex-Analyzers\" , \"license\" : \"AGPL-V3\" , \"description\" : \"Send an email with information from a TheHive case or alert\" , \"dataTypeList\" : [ \"thehive:case\" , \"thehive:alert\" , \"thehive:case_task\" ], \"command\" : \"Mailer/mailer.py\" , \"baseConfig\" : \"Mailer\" , \"configurationItems\" : [ { \"name\" : \"from\" , \"description\" : \"email address from which the mail is send\" , \"type\" : \"string\" , <== de f i nes wha t ki n d o f da ta t ype t he co nf igura t io n i te m is (s tr i n g , nu mber) \"multi\" : false , <== se tt i n g mul t i t o true allows t o pass a lis t o f i te ms \"required\" : true }, { \"name\" : \"smtp_host\" , \"description\" : \"SMTP server used to send mail\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : true , \"defaultValue\" : \"localhost\" }, { \"name\" : \"smtp_port\" , \"description\" : \"SMTP server port\" , \"type\" : \"number\" , \"multi\" : false , \"required\" : true , \"defaultValue\" : 25 }, { \"name\" : \"smtp_user\" , \"description\" : \"SMTP server user\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : \"user\" }, { \"name\" : \"smtp_pwd\" , \"description\" : \"SMTP server password\" , \"type\" : \"string\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : \"pwd\" } ] } Ensure that all information is correct and particularly the author and license parameters.","title":"Review your Service Interaction File(s)"},{"location":"cortex/api/how-to-create-a-responder/#provide-the-list-of-requirements","text":"If your responder is written in Python, make sure to complete the requirements.txt file with the list of all the external libraries that are needed to run the responder correctly.","title":"Provide the List of Requirements"},{"location":"cortex/api/how-to-create-a-responder/#verify-execution","text":"Use these three simple checks before submitting your responder: Ensure it works with the expected configuration, TLP, PAP or datatype. Ensure it works with missing configuration, PAP, datatype or TLP: your responder must generate an explicit error message.","title":"Verify Execution"},{"location":"cortex/api/how-to-create-a-responder/#create-a-pull-request","text":"Create one Pull Request per responder against the develop branch of the Cortex-Analyzers repository. Reference the issue you've created in your PR. We have to review your responders. Distinct PRs will allow us to review them more quickly and release them to the benefit of the whole community.","title":"Create a Pull Request"},{"location":"cortex/api/how-to-create-a-responder/#need-help","text":"Something does not work as expected? No worries, we got you covered. Please join our user forum , contact us on Gitter , or send us an email at support@thehive-project.org . We are here to help.","title":"Need Help?"},{"location":"cortex/api/how-to-create-an-analyzer/","text":"How to Write and Submit an Analyzer # Table of Contents # Writing an Analyzer The Program Service Interaction Files (Flavors) Python Requirements Example: VirusTotal Analyzer Files Input Service Interaction Configuration Items Analyzer Configuration in the Global Configuration File Output The Cortexutils Python Library Report Templates Submitting an Analyzer Check Existing Issues Open an Issue Review your Service Interaction File(s) Provide the List of Requirements Check the Taxonomy Provide Global Configuration Parameters Verify Execution Create a Pull Request Need Help? Writing an Analyzer # An analyzer is a program that takes an observable and configuration information as raw input , analyze the observable and produces a result as raw output . It is made of at least 2 types of files: The program itself One or several service interaction files or flavors A Python requirements file, which is only necessary if the analyzer is written in Python. The Program # The first type of files an analyzer is made of is the core program that performs actions. It can be written in any programming language that is supported by Linux. While many analyzers are written in Python ( *.py files), you can write yours in Ruby, Perl or even Scala. However, the very handy Cortexutils library described below is in Python. It greatly facilitates analyzer development and it also provides some methods to quickly format the output to make it compliant with the JSON schema expected by TheHive . Service Interaction Files (Flavors) # An analyzer must have at least one service interaction file. Such files contain key configuration information such as the analyzer's author information, the datatypes (IP, URL, hash, domain...) the analyzer accepts as input, the TLP and PAP ( Permissible Actions Protocol ) above which it will refuse to execute to protect against data leakage and to enforce sane OPSEC practices and so on. An analyzer can have two or more service interaction files to allow it to perform different actions. We speak then of flavors. For example, a sandbox analyzer can analyze a file with or without an Internet connection. Another example could be an analyzer that can either send a file to VirusTotal for analysis or get the last report using its hash. Python Requirements # If the analyzer is written in Python, a requirements.txt must be provided with the list of all the dependencies. Example: VirusTotal Analyzer Files # Below is a directory listing of the files corresponding to the VirusTotal analyzer. You can see that the analyzer has two flavors: GetReport and Scan . analyzers/VirusTotal | -- VirusTotal_GetReport.json | -- VirusTotal_Scan.json | -- requirements.txt | -- virustotal.py ` -- virustotal_api.py Input # The input of an analyzer is a JSON structure with different pieces of information. For example, to use the VirusTotal analyzer's GetReport flavor in order to obtain the latest available report for hash d41d8cd98f00b204e9800998ecf8427e , you must submit input such as: { \"data\" : \"d41d8cd98f00b204e9800998ecf8427e\" , \"dataType\" : \"hash\" , \"tlp\" : 0 , \"config\" :{ \"key\" : \"1234567890abcdef\" , \"max_tlp\" : 3 , \"check_tlp\" : true , \"service\" : \"GetReport\" [ .. ] }, \"proxy\" :{ \"http\" : \"http://myproxy:8080\" , \"https\" : \"https://myproxy:8080\" } } data , dataType and tlp are the observable-related information generated by TheHive or any other program that is calling Cortex. config is the analyzer's specific configuration provided by an orgAdmin users when the analyzer is enabled in the Cortex UI. Let's take the GetReport flavor of the VirusTotal analyzer as an example again. Example: VirusTotal Get Report's Input # { \"data\" : \"d41d8cd98f00b204e9800998ecf8427e\" , \"dataType\" : \"hash\" , \"tlp\" : 0 , [ .. ] } Example: Service Interaction File for VirusTotal GetReport # The <== sign and anything after it are comments that do no appear in the original file. { \"name\" : \"VirusTotal_GetReport\" , \"version\" : \"3.0\" , \"author\" : \"CERT-BDF\" , \"url\" : \"https://github.com/TheHive-Project/Cortex-Analyzers\" , \"license\" : \"AGPL-V3\" , \"description\" : \"Get the latest VirusTotal report for a file, hash, domain or an IP address.\" , \"dataTypeList\" : [ \"file\" , \"hash\" , \"domain\" , \"ip\" ], \"command\" : \"VirusTotal/virustotal.py\" , <== Program t o ru n whe n i n voki n g t he a nal yzer \"baseConfig\" : \"VirusTotal\" , <== na me o f base co nf ig i n Cor te x a nal yzer co nf ig page \"config\" : { \"service\" : \"get\" }, \"configurationItems\" : [ <== lis t o f co nf igura t io n i te ms t he a nal yzer nee ds t o opera te (api key e t c.) { \"name\" : \"key\" , \"description\" : \"API key for Virustotal\" , \"type\" : \"string\" , <== de f i nes wha t ki n d o f da ta t ype t he co nf igura t io n i te m is (s tr i n g , nu mber) \"multi\" : false , <== se tt i n g mul t i t o true allows t o pass a lis t o f i te ms (e.g. MISP a nal yzer) \"required\" : true }, { \"name\" : \"polling_interval\" , \"description\" : \"Define time interval between two requests attempts for the report\" , \"type\" : \"number\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : 60 } ] } Service Interaction Configuration Items # name # Name of the specific service (or flavor) of the analyzer. If your analyzer has only one service interaction (i.e. performs only one action), it is the name of the analyzer's directory. If your analyzer performs several actions (i.e. comes in several flavors), you have to give a specific and meaningful name to each flavor. Each flavor's name appear in TheHive's analyzer list and in MISP when you use Cortex for attribute enrichment. version # The version of the analyzer. You must increase major version numbers when new features are added, modifications are made to take into account API changes, report output is modified or when report templates (more on this later) are updated. You must increase minor version numbers when bugs are fixed. The version number is also used in the folder name of the associated report templates ; e.g. VirusTotal_GetReport and 3.0 on the JSON file should correspond a folder named VirusTotal_GetReport_3_0 for report templates. Report templates are used by TheHive to display the analyzer's JSON output in an analyst-friendly fashion. author # You must provide your full name and/or your organization/team name when submitting an analyzer. Pseudos are not accepted. If you'd rather remain anonymous, please contact us at support@thehive-project.org prior to submitting your analyzer. url # The URL where the analyzer is stored. This should ideally be https://github.com/TheHive-Project/Cortex-Analyzers license # The license of the code. Ideally, we recommend using the AGPL-v3 license. Make sure your code's license is compatible with the license(s) of the various components and libraries you use if applicable. description # Description of the analyzer. Please be concise and clear. The description is shown in the Cortex UI, TheHive and MISP. dataTypeList # The list of TheHive datatypes supported by the analyzer. Currently TheHive accepts the following datatypes: domain file filename fqdn hash ip mail mail_subject other regexp registry uri_path url user-agent If you need additional datatypes for your analyzer, please let us know at support@thehive-project.org . baseConfig # Name used to group configuration items common to several analyzer. This prevent the user to enter the same API key for all analyzer flavors. The Cortex analyzer config page group configuration items by their baseConfig . config # Configuration dedicated to the analyzer's flavor. This is where we typically specify the TLP level of observables allowed to be analyzed with the check_tlp and max_tlp parameters. For example, if max_tlp is set to 2 (TLP:AMBER), TLP:RED observables cannot be analyzed. max_tlp # The TLP level above which the analyzer must not be executed. TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3 check_tlp # This is a boolean parameter. When true , max_tlp is checked. And if the input's TLP is above max_tlp , the analyzer is not executed. For consistency reasons, we do recommend setting both check_tlp and max_tlp even if check_tlp is set to false . max_pap # The PAP level above which the analyzer must not be executed. TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3 check_pap # This is a boolean parameter. When true , max_pap is checked. And if the input's PAP is above max_pap , the analyzer is not executed. For consistency reasons, we do recommend setting both check_pap and max_pap even if check_pap is set to false . command # The command used to run the analyzer. That's typically the full, absolute path to the main program file. configurationItems # The list of configurationItems is necessary in order to be able to set all configuration variables for analyzers directly in the Cortex 2 user interface. As in the VirusTotal example above can be seen, every item is a json object that defines: - name (string) - description (string) - type (string) - multi (boolean) - required (boolean) - defaultValue (according to type, optional) The multi parameter allows to pass a list as configuration variable instead of a single string or number. This is used e.g. in the MISP analyzer that queries multiple servers in one run and needs different parameters for that. Output # The output of an analyzer depends on the success or failure of its execution. If the analyzer fails to execute: { \"success\" : false , \"errorMessage\" : \"..\" } When success is set to false , it indicates that something went wrong during the execution. errorMessage is free text - typically the error output message. If the analyzer succeeds (i.e. it runs without any error): { \"success\" : true , \"artifacts\" :[ .. ], \"summary\" :{ \"taxonomies\" :[ .. ] }, \"full\" :{ .. } } When success is set to true , it indicates that the analyzer ran successfully. artifacts is a list of indicators extracted from the produced report. full is the full report of the analyzer. It is free form, as long as it is JSON formatted. summary is used in TheHive for short reports displayed in the observable list and in the detailed page of each observable. It contains a list of taxonomies. taxonomies : \"taxonomies\" :[ { \"namespace\" : \"NAME\" , \"predicate\" : \"PREDICATE\" , \"value\" : \"\\\"VALUE\\\"\" , \"level\" : \"info\" } ] namespace and predicate are free values but they should be as concise as possible. For example, the VirusTotal analyzer uses VT as a namespace and Score as a predicate. level intends to convey the maliciousness of the result: : info : the analyzer produced an information, and the short report is shown in blue color in TheHive. safe : the analyzer did not find anything suspicious or the analyzed observable is safe according to the analyzer. TheHive displays the short report in green color. suspicious : the analyzer found that the observable is either suspicious or warrants further investigation. The short report has an orange color in TheHive. malicious : the analyzer found that the observable is malicious. The short report is red colored in TheHive. For more information refer to our blog . The Cortexutils Python Library # So far, all the published analyzers have been written in Python. We released a special Python library called cortexutils to help developers easily write their programs. Note though that Python is not mandatory for analyzer coding and any language that runs on Linux can be used, though you won't have the benefits of the CortexUtils library. Cortexutils can be used with Python 2 and 3. To install it : pip install cortexutils or pip3 install cortexutils This library is already used by all the analyzers published in our Github repository . Feel free to start reading the code of some of them before writing your own. Report Templates # When using TheHive, analysts can submit an observable for analysis to one or several Cortex instances by a click of a button. Once finished, Cortex returns the result to TheHive. The TheHive displays that result using HTML templates for short and long reports. Cortex Result in TheHive # TheHive receives the Cortex result which is simply the JSON formatted analyzer output described above: The summary section is read to display short reports in the observables list and in the detailed observable page. This is stored in a dict object named content within TheHive. The full section is read to display long reports when clicking the short report in the observable list or when accessing a detailed observable page. In TheHive application, it is stored in a dict object named content . Displayed Information # When No Template is Imported # In the event that the analyzer report templates are not imported in TheHive (only administrators can do such an operation via the Admin > Report Templates menu): In the observable list, TheHive is able to display the analyzer summary results using a builtin style sheet associated with the previously described taxonomy. In the detailed observable page: the full result is displayed in raw format (the JSON output from Cortex) the summary result is not displayed . When Templates are Imported # If templates are imported into TheHive: Short reports are displayed in the observable list and in the detailed observable page. Long reports are displayed when clicking on the short reports or in the detailed observable page. Writing Templates # To display results nicely in TheHive, write two HTML templates: One for short reports One for long reports When TheHive users import them in the application, they will be definitely more efficient at reading the analyzer reports and do their job accordingly. If the analyzer is made of different flavors (i.e. has different service interaction files with a json extension), you should provide two HTML templates (short and long reports) for each flavor. For example, the VirusTotal analyzer comes in two flavors hence it has 4 HTML templates: thehive-templates/VirusTotal_GetReport_3_0 | -- long.html ` -- short.html thehive-templates/VirusTotal_Scan_3_0 | -- long.html ` -- short.html The folder's name is the concatenation of the name and the version values found in the service interaction files. TheHive uses Bootstrap and AngularJS so you can leverage them in your templates. Short Report Templates (short.html) # The short report uses taxonomies and is built into the analyzers by the summary() function. Report templates read it as shown in the example below: < span class = \"label\" ng-repeat = \"t in content.taxonomies\" ng-class = \"{'info': 'label-info', 'safe': 'label-success', 'suspicious': 'label-warning', 'malicious':'label-danger'}[t.level]\" > {{t.namespace}}:{{t.predicate}}={{t.value}} </ span > If you want to change or add the information displayed in the short report in the detailed observable page, you have to update the summary() function in the analyzer's program and edit short.html as well. Basically, copy the code in your short.html template and it will do the job. Long Report Templates (long.html) # Long report templates are more or less free form as long as it reads the content of the relevant section in the Cortex result ( full ). Feel free to check what has already been written for existing analyzers to write yours. A good start can be: <!-- Success --> < div class = \"panel panel-danger\" ng-if = \"success\" > < div class = \"panel-heading\" > ANALYZERNAME Report </ div > < div class = \"panel-body\" > [...] < = code here </ div > </ div > <!-- General error --> < div class = \"panel panel-danger\" ng-if = \"!success\" > < div class = \"panel-heading\" > < strong > {{(artifact.data || artifact.attachment.name) | fang}} </ strong > </ div > < div class = \"panel-body\" > < dl class = \"dl-horizontal\" ng-if = \"content.errorMessage\" > < dt >< i class = \"fa fa-warning\" ></ i > ANALYZERNAME: </ dt > < dd class = \"wrap\" > {{content.errorMessage}} </ dd > </ dl > </ div > </ div > Submitting an Analyzer # We highly encourage you to share your analyzers with the community through our Github repository. To do so, we invite you to follow a few steps before submitting a pull request. Check Existing Issues # Start by checking if an issue already exists for the analyzer you'd like to write and contribute. Verify that nobody is working on it. If an issue exists and has the in progress , under review or pr-submitted label, it means somebody is already working on the code or has finished it. If you are short on ideas, check issues with a help wanted label . If one of those issues interest you, indicate that you are working on it. Open an Issue # If there's no issue open for the analyzer you'd like to contribute, open one . Indicate that you are working on it to avoid having someone start coding it. You have to create an issue for each analyzer you'd like to submit. Review your Service Interaction File(s) # Review your service interaction files. For example, let's check the VirusTotal JSON analyzer configuration file(s): { \"name\" : \"VirusTotal_GetReport\" , \"version\" : \"3.0\" , \"author\" : \"CERT-BDF\" , \"url\" : \"https://github.com/TheHive-Project/Cortex-Analyzers\" , \"license\" : \"AGPL-V3\" , \"description\" : \"Get the latest VirusTotal report for a file, hash, domain or an IP address\" , \"dataTypeList\" : [ \"file\" , \"hash\" , \"domain\" , \"ip\" ], \"baseConfig\" : \"VirusTotal\" , \"config\" : { \"check_tlp\" : true , \"max_tlp\" : 3 , \"service\" : \"get\" }, \"command\" : \"VirusTotal/virustotal.py\" } Ensure that all information is correct and particularly the author and license parameters. Provide the List of Requirements # If your analyzer is written in Python, make sure to complete the requirements.txt file with the list of all the external libraries that are needed to run the analyzer correctly. Check the Taxonomy # We chose to use a formatted summary report to match a taxonomy as described above. If you want your analyzer reports in the observable lists, ensure that your summary matches this format. If your analyzer is written in Python and you are using our cortexutils library, you can use the summary() and build_taxonomy() functions. Provide Global Configuration Parameters # When submitting your analyzer, please provide the necessary global configuration in /etc/cortex/application.conf if needed. You can provide this information in a README file. Verify Execution # Use these three simple checks before submtting your analyzer: Ensure it works with the expected configuration, TLP or dataType. Ensure it works with missing configuration, dataType or TLP: your analyzer must generate an explicit error message. Ensure the long report template handles error messages correctly. Create a Pull Request # Create one Pull Request per analyzer against the develop branch of the Cortex-Analyzers repository. Reference the issue you've created in your PR. We have to review your analyzers. Distinct PRs will allow us to review them more quickly and release them to the benefit of the whole community. Need Help? # Something does not work as expected? No worries, we got you covered. Please join our user forum , contact us on Gitter , or send us an email at support@thehive-project.org . We are here to help.","title":"How to Write and Submit an Analyzer"},{"location":"cortex/api/how-to-create-an-analyzer/#how-to-write-and-submit-an-analyzer","text":"","title":"How to Write and Submit an Analyzer"},{"location":"cortex/api/how-to-create-an-analyzer/#table-of-contents","text":"Writing an Analyzer The Program Service Interaction Files (Flavors) Python Requirements Example: VirusTotal Analyzer Files Input Service Interaction Configuration Items Analyzer Configuration in the Global Configuration File Output The Cortexutils Python Library Report Templates Submitting an Analyzer Check Existing Issues Open an Issue Review your Service Interaction File(s) Provide the List of Requirements Check the Taxonomy Provide Global Configuration Parameters Verify Execution Create a Pull Request Need Help?","title":"Table of Contents"},{"location":"cortex/api/how-to-create-an-analyzer/#writing-an-analyzer","text":"An analyzer is a program that takes an observable and configuration information as raw input , analyze the observable and produces a result as raw output . It is made of at least 2 types of files: The program itself One or several service interaction files or flavors A Python requirements file, which is only necessary if the analyzer is written in Python.","title":"Writing an Analyzer"},{"location":"cortex/api/how-to-create-an-analyzer/#the-program","text":"The first type of files an analyzer is made of is the core program that performs actions. It can be written in any programming language that is supported by Linux. While many analyzers are written in Python ( *.py files), you can write yours in Ruby, Perl or even Scala. However, the very handy Cortexutils library described below is in Python. It greatly facilitates analyzer development and it also provides some methods to quickly format the output to make it compliant with the JSON schema expected by TheHive .","title":"The Program"},{"location":"cortex/api/how-to-create-an-analyzer/#service-interaction-files-flavors","text":"An analyzer must have at least one service interaction file. Such files contain key configuration information such as the analyzer's author information, the datatypes (IP, URL, hash, domain...) the analyzer accepts as input, the TLP and PAP ( Permissible Actions Protocol ) above which it will refuse to execute to protect against data leakage and to enforce sane OPSEC practices and so on. An analyzer can have two or more service interaction files to allow it to perform different actions. We speak then of flavors. For example, a sandbox analyzer can analyze a file with or without an Internet connection. Another example could be an analyzer that can either send a file to VirusTotal for analysis or get the last report using its hash.","title":"Service Interaction Files (Flavors)"},{"location":"cortex/api/how-to-create-an-analyzer/#python-requirements","text":"If the analyzer is written in Python, a requirements.txt must be provided with the list of all the dependencies.","title":"Python Requirements"},{"location":"cortex/api/how-to-create-an-analyzer/#example-virustotal-analyzer-files","text":"Below is a directory listing of the files corresponding to the VirusTotal analyzer. You can see that the analyzer has two flavors: GetReport and Scan . analyzers/VirusTotal | -- VirusTotal_GetReport.json | -- VirusTotal_Scan.json | -- requirements.txt | -- virustotal.py ` -- virustotal_api.py","title":"Example: VirusTotal Analyzer Files"},{"location":"cortex/api/how-to-create-an-analyzer/#input","text":"The input of an analyzer is a JSON structure with different pieces of information. For example, to use the VirusTotal analyzer's GetReport flavor in order to obtain the latest available report for hash d41d8cd98f00b204e9800998ecf8427e , you must submit input such as: { \"data\" : \"d41d8cd98f00b204e9800998ecf8427e\" , \"dataType\" : \"hash\" , \"tlp\" : 0 , \"config\" :{ \"key\" : \"1234567890abcdef\" , \"max_tlp\" : 3 , \"check_tlp\" : true , \"service\" : \"GetReport\" [ .. ] }, \"proxy\" :{ \"http\" : \"http://myproxy:8080\" , \"https\" : \"https://myproxy:8080\" } } data , dataType and tlp are the observable-related information generated by TheHive or any other program that is calling Cortex. config is the analyzer's specific configuration provided by an orgAdmin users when the analyzer is enabled in the Cortex UI. Let's take the GetReport flavor of the VirusTotal analyzer as an example again.","title":"Input"},{"location":"cortex/api/how-to-create-an-analyzer/#example-virustotal-get-reports-input","text":"{ \"data\" : \"d41d8cd98f00b204e9800998ecf8427e\" , \"dataType\" : \"hash\" , \"tlp\" : 0 , [ .. ] }","title":"Example: VirusTotal Get Report's Input"},{"location":"cortex/api/how-to-create-an-analyzer/#example-service-interaction-file-for-virustotal-getreport","text":"The <== sign and anything after it are comments that do no appear in the original file. { \"name\" : \"VirusTotal_GetReport\" , \"version\" : \"3.0\" , \"author\" : \"CERT-BDF\" , \"url\" : \"https://github.com/TheHive-Project/Cortex-Analyzers\" , \"license\" : \"AGPL-V3\" , \"description\" : \"Get the latest VirusTotal report for a file, hash, domain or an IP address.\" , \"dataTypeList\" : [ \"file\" , \"hash\" , \"domain\" , \"ip\" ], \"command\" : \"VirusTotal/virustotal.py\" , <== Program t o ru n whe n i n voki n g t he a nal yzer \"baseConfig\" : \"VirusTotal\" , <== na me o f base co nf ig i n Cor te x a nal yzer co nf ig page \"config\" : { \"service\" : \"get\" }, \"configurationItems\" : [ <== lis t o f co nf igura t io n i te ms t he a nal yzer nee ds t o opera te (api key e t c.) { \"name\" : \"key\" , \"description\" : \"API key for Virustotal\" , \"type\" : \"string\" , <== de f i nes wha t ki n d o f da ta t ype t he co nf igura t io n i te m is (s tr i n g , nu mber) \"multi\" : false , <== se tt i n g mul t i t o true allows t o pass a lis t o f i te ms (e.g. MISP a nal yzer) \"required\" : true }, { \"name\" : \"polling_interval\" , \"description\" : \"Define time interval between two requests attempts for the report\" , \"type\" : \"number\" , \"multi\" : false , \"required\" : false , \"defaultValue\" : 60 } ] }","title":"Example: Service Interaction File for VirusTotal GetReport"},{"location":"cortex/api/how-to-create-an-analyzer/#service-interaction-configuration-items","text":"","title":"Service Interaction Configuration Items"},{"location":"cortex/api/how-to-create-an-analyzer/#name","text":"Name of the specific service (or flavor) of the analyzer. If your analyzer has only one service interaction (i.e. performs only one action), it is the name of the analyzer's directory. If your analyzer performs several actions (i.e. comes in several flavors), you have to give a specific and meaningful name to each flavor. Each flavor's name appear in TheHive's analyzer list and in MISP when you use Cortex for attribute enrichment.","title":"name"},{"location":"cortex/api/how-to-create-an-analyzer/#version","text":"The version of the analyzer. You must increase major version numbers when new features are added, modifications are made to take into account API changes, report output is modified or when report templates (more on this later) are updated. You must increase minor version numbers when bugs are fixed. The version number is also used in the folder name of the associated report templates ; e.g. VirusTotal_GetReport and 3.0 on the JSON file should correspond a folder named VirusTotal_GetReport_3_0 for report templates. Report templates are used by TheHive to display the analyzer's JSON output in an analyst-friendly fashion.","title":"version"},{"location":"cortex/api/how-to-create-an-analyzer/#author","text":"You must provide your full name and/or your organization/team name when submitting an analyzer. Pseudos are not accepted. If you'd rather remain anonymous, please contact us at support@thehive-project.org prior to submitting your analyzer.","title":"author"},{"location":"cortex/api/how-to-create-an-analyzer/#url","text":"The URL where the analyzer is stored. This should ideally be https://github.com/TheHive-Project/Cortex-Analyzers","title":"url"},{"location":"cortex/api/how-to-create-an-analyzer/#license","text":"The license of the code. Ideally, we recommend using the AGPL-v3 license. Make sure your code's license is compatible with the license(s) of the various components and libraries you use if applicable.","title":"license"},{"location":"cortex/api/how-to-create-an-analyzer/#description","text":"Description of the analyzer. Please be concise and clear. The description is shown in the Cortex UI, TheHive and MISP.","title":"description"},{"location":"cortex/api/how-to-create-an-analyzer/#datatypelist","text":"The list of TheHive datatypes supported by the analyzer. Currently TheHive accepts the following datatypes: domain file filename fqdn hash ip mail mail_subject other regexp registry uri_path url user-agent If you need additional datatypes for your analyzer, please let us know at support@thehive-project.org .","title":"dataTypeList"},{"location":"cortex/api/how-to-create-an-analyzer/#baseconfig","text":"Name used to group configuration items common to several analyzer. This prevent the user to enter the same API key for all analyzer flavors. The Cortex analyzer config page group configuration items by their baseConfig .","title":"baseConfig"},{"location":"cortex/api/how-to-create-an-analyzer/#config","text":"Configuration dedicated to the analyzer's flavor. This is where we typically specify the TLP level of observables allowed to be analyzed with the check_tlp and max_tlp parameters. For example, if max_tlp is set to 2 (TLP:AMBER), TLP:RED observables cannot be analyzed.","title":"config"},{"location":"cortex/api/how-to-create-an-analyzer/#max_tlp","text":"The TLP level above which the analyzer must not be executed. TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3","title":"max_tlp"},{"location":"cortex/api/how-to-create-an-analyzer/#check_tlp","text":"This is a boolean parameter. When true , max_tlp is checked. And if the input's TLP is above max_tlp , the analyzer is not executed. For consistency reasons, we do recommend setting both check_tlp and max_tlp even if check_tlp is set to false .","title":"check_tlp"},{"location":"cortex/api/how-to-create-an-analyzer/#max_pap","text":"The PAP level above which the analyzer must not be executed. TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3","title":"max_pap"},{"location":"cortex/api/how-to-create-an-analyzer/#check_pap","text":"This is a boolean parameter. When true , max_pap is checked. And if the input's PAP is above max_pap , the analyzer is not executed. For consistency reasons, we do recommend setting both check_pap and max_pap even if check_pap is set to false .","title":"check_pap"},{"location":"cortex/api/how-to-create-an-analyzer/#command","text":"The command used to run the analyzer. That's typically the full, absolute path to the main program file.","title":"command"},{"location":"cortex/api/how-to-create-an-analyzer/#configurationitems","text":"The list of configurationItems is necessary in order to be able to set all configuration variables for analyzers directly in the Cortex 2 user interface. As in the VirusTotal example above can be seen, every item is a json object that defines: - name (string) - description (string) - type (string) - multi (boolean) - required (boolean) - defaultValue (according to type, optional) The multi parameter allows to pass a list as configuration variable instead of a single string or number. This is used e.g. in the MISP analyzer that queries multiple servers in one run and needs different parameters for that.","title":"configurationItems"},{"location":"cortex/api/how-to-create-an-analyzer/#output","text":"The output of an analyzer depends on the success or failure of its execution. If the analyzer fails to execute: { \"success\" : false , \"errorMessage\" : \"..\" } When success is set to false , it indicates that something went wrong during the execution. errorMessage is free text - typically the error output message. If the analyzer succeeds (i.e. it runs without any error): { \"success\" : true , \"artifacts\" :[ .. ], \"summary\" :{ \"taxonomies\" :[ .. ] }, \"full\" :{ .. } } When success is set to true , it indicates that the analyzer ran successfully. artifacts is a list of indicators extracted from the produced report. full is the full report of the analyzer. It is free form, as long as it is JSON formatted. summary is used in TheHive for short reports displayed in the observable list and in the detailed page of each observable. It contains a list of taxonomies. taxonomies : \"taxonomies\" :[ { \"namespace\" : \"NAME\" , \"predicate\" : \"PREDICATE\" , \"value\" : \"\\\"VALUE\\\"\" , \"level\" : \"info\" } ] namespace and predicate are free values but they should be as concise as possible. For example, the VirusTotal analyzer uses VT as a namespace and Score as a predicate. level intends to convey the maliciousness of the result: : info : the analyzer produced an information, and the short report is shown in blue color in TheHive. safe : the analyzer did not find anything suspicious or the analyzed observable is safe according to the analyzer. TheHive displays the short report in green color. suspicious : the analyzer found that the observable is either suspicious or warrants further investigation. The short report has an orange color in TheHive. malicious : the analyzer found that the observable is malicious. The short report is red colored in TheHive. For more information refer to our blog .","title":"Output"},{"location":"cortex/api/how-to-create-an-analyzer/#the-cortexutils-python-library","text":"So far, all the published analyzers have been written in Python. We released a special Python library called cortexutils to help developers easily write their programs. Note though that Python is not mandatory for analyzer coding and any language that runs on Linux can be used, though you won't have the benefits of the CortexUtils library. Cortexutils can be used with Python 2 and 3. To install it : pip install cortexutils or pip3 install cortexutils This library is already used by all the analyzers published in our Github repository . Feel free to start reading the code of some of them before writing your own.","title":"The Cortexutils Python Library"},{"location":"cortex/api/how-to-create-an-analyzer/#report-templates","text":"When using TheHive, analysts can submit an observable for analysis to one or several Cortex instances by a click of a button. Once finished, Cortex returns the result to TheHive. The TheHive displays that result using HTML templates for short and long reports.","title":"Report Templates"},{"location":"cortex/api/how-to-create-an-analyzer/#cortex-result-in-thehive","text":"TheHive receives the Cortex result which is simply the JSON formatted analyzer output described above: The summary section is read to display short reports in the observables list and in the detailed observable page. This is stored in a dict object named content within TheHive. The full section is read to display long reports when clicking the short report in the observable list or when accessing a detailed observable page. In TheHive application, it is stored in a dict object named content .","title":"Cortex Result in TheHive"},{"location":"cortex/api/how-to-create-an-analyzer/#displayed-information","text":"","title":"Displayed Information"},{"location":"cortex/api/how-to-create-an-analyzer/#when-no-template-is-imported","text":"In the event that the analyzer report templates are not imported in TheHive (only administrators can do such an operation via the Admin > Report Templates menu): In the observable list, TheHive is able to display the analyzer summary results using a builtin style sheet associated with the previously described taxonomy. In the detailed observable page: the full result is displayed in raw format (the JSON output from Cortex) the summary result is not displayed .","title":"When No Template is Imported"},{"location":"cortex/api/how-to-create-an-analyzer/#when-templates-are-imported","text":"If templates are imported into TheHive: Short reports are displayed in the observable list and in the detailed observable page. Long reports are displayed when clicking on the short reports or in the detailed observable page.","title":"When Templates are Imported"},{"location":"cortex/api/how-to-create-an-analyzer/#writing-templates","text":"To display results nicely in TheHive, write two HTML templates: One for short reports One for long reports When TheHive users import them in the application, they will be definitely more efficient at reading the analyzer reports and do their job accordingly. If the analyzer is made of different flavors (i.e. has different service interaction files with a json extension), you should provide two HTML templates (short and long reports) for each flavor. For example, the VirusTotal analyzer comes in two flavors hence it has 4 HTML templates: thehive-templates/VirusTotal_GetReport_3_0 | -- long.html ` -- short.html thehive-templates/VirusTotal_Scan_3_0 | -- long.html ` -- short.html The folder's name is the concatenation of the name and the version values found in the service interaction files. TheHive uses Bootstrap and AngularJS so you can leverage them in your templates.","title":"Writing Templates"},{"location":"cortex/api/how-to-create-an-analyzer/#short-report-templates-shorthtml","text":"The short report uses taxonomies and is built into the analyzers by the summary() function. Report templates read it as shown in the example below: < span class = \"label\" ng-repeat = \"t in content.taxonomies\" ng-class = \"{'info': 'label-info', 'safe': 'label-success', 'suspicious': 'label-warning', 'malicious':'label-danger'}[t.level]\" > {{t.namespace}}:{{t.predicate}}={{t.value}} </ span > If you want to change or add the information displayed in the short report in the detailed observable page, you have to update the summary() function in the analyzer's program and edit short.html as well. Basically, copy the code in your short.html template and it will do the job.","title":"Short Report Templates (short.html)"},{"location":"cortex/api/how-to-create-an-analyzer/#long-report-templates-longhtml","text":"Long report templates are more or less free form as long as it reads the content of the relevant section in the Cortex result ( full ). Feel free to check what has already been written for existing analyzers to write yours. A good start can be: <!-- Success --> < div class = \"panel panel-danger\" ng-if = \"success\" > < div class = \"panel-heading\" > ANALYZERNAME Report </ div > < div class = \"panel-body\" > [...] < = code here </ div > </ div > <!-- General error --> < div class = \"panel panel-danger\" ng-if = \"!success\" > < div class = \"panel-heading\" > < strong > {{(artifact.data || artifact.attachment.name) | fang}} </ strong > </ div > < div class = \"panel-body\" > < dl class = \"dl-horizontal\" ng-if = \"content.errorMessage\" > < dt >< i class = \"fa fa-warning\" ></ i > ANALYZERNAME: </ dt > < dd class = \"wrap\" > {{content.errorMessage}} </ dd > </ dl > </ div > </ div >","title":"Long Report Templates (long.html)"},{"location":"cortex/api/how-to-create-an-analyzer/#submitting-an-analyzer","text":"We highly encourage you to share your analyzers with the community through our Github repository. To do so, we invite you to follow a few steps before submitting a pull request.","title":"Submitting an Analyzer"},{"location":"cortex/api/how-to-create-an-analyzer/#check-existing-issues","text":"Start by checking if an issue already exists for the analyzer you'd like to write and contribute. Verify that nobody is working on it. If an issue exists and has the in progress , under review or pr-submitted label, it means somebody is already working on the code or has finished it. If you are short on ideas, check issues with a help wanted label . If one of those issues interest you, indicate that you are working on it.","title":"Check Existing Issues"},{"location":"cortex/api/how-to-create-an-analyzer/#open-an-issue","text":"If there's no issue open for the analyzer you'd like to contribute, open one . Indicate that you are working on it to avoid having someone start coding it. You have to create an issue for each analyzer you'd like to submit.","title":"Open an Issue"},{"location":"cortex/api/how-to-create-an-analyzer/#review-your-service-interaction-files","text":"Review your service interaction files. For example, let's check the VirusTotal JSON analyzer configuration file(s): { \"name\" : \"VirusTotal_GetReport\" , \"version\" : \"3.0\" , \"author\" : \"CERT-BDF\" , \"url\" : \"https://github.com/TheHive-Project/Cortex-Analyzers\" , \"license\" : \"AGPL-V3\" , \"description\" : \"Get the latest VirusTotal report for a file, hash, domain or an IP address\" , \"dataTypeList\" : [ \"file\" , \"hash\" , \"domain\" , \"ip\" ], \"baseConfig\" : \"VirusTotal\" , \"config\" : { \"check_tlp\" : true , \"max_tlp\" : 3 , \"service\" : \"get\" }, \"command\" : \"VirusTotal/virustotal.py\" } Ensure that all information is correct and particularly the author and license parameters.","title":"Review your Service Interaction File(s)"},{"location":"cortex/api/how-to-create-an-analyzer/#provide-the-list-of-requirements","text":"If your analyzer is written in Python, make sure to complete the requirements.txt file with the list of all the external libraries that are needed to run the analyzer correctly.","title":"Provide the List of Requirements"},{"location":"cortex/api/how-to-create-an-analyzer/#check-the-taxonomy","text":"We chose to use a formatted summary report to match a taxonomy as described above. If you want your analyzer reports in the observable lists, ensure that your summary matches this format. If your analyzer is written in Python and you are using our cortexutils library, you can use the summary() and build_taxonomy() functions.","title":"Check the Taxonomy"},{"location":"cortex/api/how-to-create-an-analyzer/#provide-global-configuration-parameters","text":"When submitting your analyzer, please provide the necessary global configuration in /etc/cortex/application.conf if needed. You can provide this information in a README file.","title":"Provide Global Configuration Parameters"},{"location":"cortex/api/how-to-create-an-analyzer/#verify-execution","text":"Use these three simple checks before submtting your analyzer: Ensure it works with the expected configuration, TLP or dataType. Ensure it works with missing configuration, dataType or TLP: your analyzer must generate an explicit error message. Ensure the long report template handles error messages correctly.","title":"Verify Execution"},{"location":"cortex/api/how-to-create-an-analyzer/#create-a-pull-request","text":"Create one Pull Request per analyzer against the develop branch of the Cortex-Analyzers repository. Reference the issue you've created in your PR. We have to review your analyzers. Distinct PRs will allow us to review them more quickly and release them to the benefit of the whole community.","title":"Create a Pull Request"},{"location":"cortex/api/how-to-create-an-analyzer/#need-help","text":"Something does not work as expected? No worries, we got you covered. Please join our user forum , contact us on Gitter , or send us an email at support@thehive-project.org . We are here to help.","title":"Need Help?"},{"location":"cortex/download/","text":"Download Cortex # Cortex is published and available as many binary packages formats: Debian / Ubuntu # Import the GPG key : curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - wget -qO- https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo gpg --dearmor -o /usr/share/keyrings/thehive-project-archive-keyring.gpg /etc/apt/source.list.d/thehive-project.list deb [signed-by=/usr/share/keyrings/thehive-project-archive-keyring.gpg] https://deb.thehive-project.org release main Red Hat Enterprise Linux / Fedora # Import the GPG key : sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY /etc/yum.repos.d/thehive-project.repo [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=https://rpm.thehive-project.org/release/noarch gpgcheck=1 ZIP archive # Download it at: https://download.thehive-project.org/cortex-latest.zip Docker # Docker images are published on Dockerhub here: https://hub.docker.com/r/thehiveproject/cortex Archives # There is no archive available for Cortex.","title":"Download Cortex"},{"location":"cortex/download/#download-cortex","text":"Cortex is published and available as many binary packages formats:","title":"Download Cortex"},{"location":"cortex/download/#debian-ubuntu","text":"Import the GPG key : curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - wget -qO- https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo gpg --dearmor -o /usr/share/keyrings/thehive-project-archive-keyring.gpg /etc/apt/source.list.d/thehive-project.list deb [signed-by=/usr/share/keyrings/thehive-project-archive-keyring.gpg] https://deb.thehive-project.org release main","title":" Debian /  Ubuntu"},{"location":"cortex/download/#red-hat-enterprise-linux-fedora","text":"Import the GPG key : sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY /etc/yum.repos.d/thehive-project.repo [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=https://rpm.thehive-project.org/release/noarch gpgcheck=1","title":" Red Hat Enterprise Linux /  Fedora"},{"location":"cortex/download/#zip-archive","text":"Download it at: https://download.thehive-project.org/cortex-latest.zip","title":" ZIP archive"},{"location":"cortex/download/#docker","text":"Docker images are published on Dockerhub here: https://hub.docker.com/r/thehiveproject/cortex","title":" Docker"},{"location":"cortex/download/#archives","text":"There is no archive available for Cortex.","title":"Archives"},{"location":"cortex/installation-and-configuration/","text":"Installation & configuration guides # Overview # Cortex relies on Elasticsearch to store its data. A basic setup to install Elasticsearch, then Cortex on a standalone and dedicated server (physical or virtual). Hardware requirements # Hardware requirements depends on the usage of the system. We recommend starting with dedicated resources: 8 vCPU 16 GB of RAM Operating systems # Cortex has been tested and is supported on the following operating systems: Ubuntu 20.04 LTS Debian 11 RHEL 8 Fedora 35 Installation Guide # Too much in a hurry to read ? If you are using one of the supported operating systems, use our all-in-one installation script : wget -q -O /tmp/install.sh https://archives.strangebee.com/scripts/install.sh ; sudo -v ; bash /tmp/install.sh This script helps with the installation process on a fresh and supported OS ; the program also run successfully if the conditions in terms of hardware requirements are met. Once executed, several options are available: Setup proxy settings ; will configure everything on the host to work with a HTTP proxy, and custom CA certificate. Install TheHive ; use this option to install TheHive 5 and its dependancies Install Cortex and all its dependencies to run Analyzers & Responders as Docker Iiages Install Cortex and all its dependencies to run Analyzers & Responders on the host (Debian and Ubuntu ONLY ) For each release, DEB, RPM and ZIP binary packages are built and provided. The following Guide let you prepare , install and configure Cortex and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages. Configuration Guides # The configuration of Cortex is in files stored in the /etc/cortex folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/cortex \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by Debian or RPM packages. This file should contain a secret that should be used by one instance. Various aspects can configured in the application.conf file: database Authentication Analyzers & Responders Analyzers & Responders # Before starting the installation of Cortex, this is important to know how Analyzers and Responders will be managed and run. 2 solutions are available to run them: Run locally # The programs are downloaded and installed on the system running Cortex. There are many disadvantages with this option: Some public Analyzers or Responders , or you own custom program might required specific applications installed on the system, All of the programs published are written in Python and come with dependancies. To run successfully, the dependancies of all programs should be installed on the same operating system ; so there is a high risk of incompatibilities (some program might require a specific version of a librarie with the latest is also required by another one) The goal of Analyzers is to extract or gather information or intelligence about observables ; and some of them might be malicious. Depending on the analysis, like a code analysis, you might want to ensure the Analyzer has not been compromised - and the host - by the observable itself You might want to ensure that when you run an Analyzer , there is no question about the integrity of its programs Updating them might be a pain regarding Operating System used and dependancies Run with Docker # Analyzers & Responders we publish also have their own Docker images . There are several benefits to use Docker images of Analyzers & Responders . No need to worry about applications required or libraries, it just work When requested, Cortex downloads the docker image of a program and instanciate a container running the program. When finished, the container is trashed and a new one is created the next time. No need to worry about the integrity of the program This is simple to use and maintain This is the recommended option. It requires installing Docker engine as well. This is not an exclusive choice, both solutions can be used by the same instance of Cortex.","title":"Installation & configuration guides"},{"location":"cortex/installation-and-configuration/#installation-configuration-guides","text":"","title":"Installation &amp; configuration guides"},{"location":"cortex/installation-and-configuration/#overview","text":"Cortex relies on Elasticsearch to store its data. A basic setup to install Elasticsearch, then Cortex on a standalone and dedicated server (physical or virtual).","title":"Overview"},{"location":"cortex/installation-and-configuration/#hardware-requirements","text":"Hardware requirements depends on the usage of the system. We recommend starting with dedicated resources: 8 vCPU 16 GB of RAM","title":"Hardware requirements"},{"location":"cortex/installation-and-configuration/#operating-systems","text":"Cortex has been tested and is supported on the following operating systems: Ubuntu 20.04 LTS Debian 11 RHEL 8 Fedora 35","title":"Operating systems"},{"location":"cortex/installation-and-configuration/#installation-guide","text":"Too much in a hurry to read ? If you are using one of the supported operating systems, use our all-in-one installation script : wget -q -O /tmp/install.sh https://archives.strangebee.com/scripts/install.sh ; sudo -v ; bash /tmp/install.sh This script helps with the installation process on a fresh and supported OS ; the program also run successfully if the conditions in terms of hardware requirements are met. Once executed, several options are available: Setup proxy settings ; will configure everything on the host to work with a HTTP proxy, and custom CA certificate. Install TheHive ; use this option to install TheHive 5 and its dependancies Install Cortex and all its dependencies to run Analyzers & Responders as Docker Iiages Install Cortex and all its dependencies to run Analyzers & Responders on the host (Debian and Ubuntu ONLY ) For each release, DEB, RPM and ZIP binary packages are built and provided. The following Guide let you prepare , install and configure Cortex and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages.","title":"Installation Guide"},{"location":"cortex/installation-and-configuration/#configuration-guides","text":"The configuration of Cortex is in files stored in the /etc/cortex folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/cortex \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by Debian or RPM packages. This file should contain a secret that should be used by one instance. Various aspects can configured in the application.conf file: database Authentication Analyzers & Responders","title":"Configuration Guides"},{"location":"cortex/installation-and-configuration/#analyzers-responders","text":"Before starting the installation of Cortex, this is important to know how Analyzers and Responders will be managed and run. 2 solutions are available to run them:","title":"Analyzers &amp; Responders"},{"location":"cortex/installation-and-configuration/#run-locally","text":"The programs are downloaded and installed on the system running Cortex. There are many disadvantages with this option: Some public Analyzers or Responders , or you own custom program might required specific applications installed on the system, All of the programs published are written in Python and come with dependancies. To run successfully, the dependancies of all programs should be installed on the same operating system ; so there is a high risk of incompatibilities (some program might require a specific version of a librarie with the latest is also required by another one) The goal of Analyzers is to extract or gather information or intelligence about observables ; and some of them might be malicious. Depending on the analysis, like a code analysis, you might want to ensure the Analyzer has not been compromised - and the host - by the observable itself You might want to ensure that when you run an Analyzer , there is no question about the integrity of its programs Updating them might be a pain regarding Operating System used and dependancies","title":"Run locally"},{"location":"cortex/installation-and-configuration/#run-with-docker","text":"Analyzers & Responders we publish also have their own Docker images . There are several benefits to use Docker images of Analyzers & Responders . No need to worry about applications required or libraries, it just work When requested, Cortex downloads the docker image of a program and instanciate a container running the program. When finished, the container is trashed and a new one is created the next time. No need to worry about the integrity of the program This is simple to use and maintain This is the recommended option. It requires installing Docker engine as well. This is not an exclusive choice, both solutions can be used by the same instance of Cortex.","title":"Run with Docker"},{"location":"cortex/installation-and-configuration/advanced-configuration/","text":"Advanced configuration # Cache # Performance # In order to increase Cortex performance, a cache is configured to prevent repetitive database solicitation. Cache retention time can be configured for users and organizations (default is 5 minutes). If a user is updated, the cache is automatically invalidated. Analyzer Results # Analyzer results (job reports) can also be cached. If an analyzer is executed against the same observable, the previous report can be returned without re-executing the analyzer. The cache is used only if the second job occurs within cache.job (the default is 10 minutes). cache { job = 10 minutes user = 5 minutes organization = 5 minutes } Notes The global cache.job value can be overridden for each analyzer in the analyzer configuration Web dialog it is possible to bypass the cache altogether (for example to get extra fresh results) through the API as explained in the API Guide or by setting the cache to Custom in the Cortex UI for each analyzer and specifying 0 as the number of minutes. Streaming (a.k.a The Flow) # The user interface is automatically updated when data is changed in the back-end. To do this, the back-end sends events to all the connected front-ends. The mechanism used to notify the front-end is called long polling and its settings are: refresh : when there is no notification, close the connection after this duration (the default is 1 minute). cache : before polling a session must be created, in order to make sure no event is lost between two polls. If there is no poll during the cache setting, the session is destroyed (the default is 15 minutes). nextItemMaxWait , globalMaxWait : when an event occurs, it is not immediately sent to the front-ends. The back-end waits nextItemMaxWait and up to globalMaxWait in case another event can be included in the notification. This mechanism saves many HTTP requests. The default values are: # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s } Entity Size Limit # The Play framework used by Cortex sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. This could be too small in some cases so you may want to change it with the following settings in the application.conf file: # Max textual content length play.http.parser.maxMemoryBuffer=1M # Max file size play.http.parser.maxDiskBuffer=1G Note if you are using a NGINX reverse proxy in front of Cortex, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size as defined in Cortex application.conf file.","title":"Advanced configuration"},{"location":"cortex/installation-and-configuration/advanced-configuration/#advanced-configuration","text":"","title":"Advanced configuration"},{"location":"cortex/installation-and-configuration/advanced-configuration/#cache","text":"","title":"Cache"},{"location":"cortex/installation-and-configuration/advanced-configuration/#performance","text":"In order to increase Cortex performance, a cache is configured to prevent repetitive database solicitation. Cache retention time can be configured for users and organizations (default is 5 minutes). If a user is updated, the cache is automatically invalidated.","title":"Performance"},{"location":"cortex/installation-and-configuration/advanced-configuration/#analyzer-results","text":"Analyzer results (job reports) can also be cached. If an analyzer is executed against the same observable, the previous report can be returned without re-executing the analyzer. The cache is used only if the second job occurs within cache.job (the default is 10 minutes). cache { job = 10 minutes user = 5 minutes organization = 5 minutes } Notes The global cache.job value can be overridden for each analyzer in the analyzer configuration Web dialog it is possible to bypass the cache altogether (for example to get extra fresh results) through the API as explained in the API Guide or by setting the cache to Custom in the Cortex UI for each analyzer and specifying 0 as the number of minutes.","title":"Analyzer Results"},{"location":"cortex/installation-and-configuration/advanced-configuration/#streaming-aka-the-flow","text":"The user interface is automatically updated when data is changed in the back-end. To do this, the back-end sends events to all the connected front-ends. The mechanism used to notify the front-end is called long polling and its settings are: refresh : when there is no notification, close the connection after this duration (the default is 1 minute). cache : before polling a session must be created, in order to make sure no event is lost between two polls. If there is no poll during the cache setting, the session is destroyed (the default is 15 minutes). nextItemMaxWait , globalMaxWait : when an event occurs, it is not immediately sent to the front-ends. The back-end waits nextItemMaxWait and up to globalMaxWait in case another event can be included in the notification. This mechanism saves many HTTP requests. The default values are: # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s }","title":"Streaming (a.k.a The Flow)"},{"location":"cortex/installation-and-configuration/advanced-configuration/#entity-size-limit","text":"The Play framework used by Cortex sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. This could be too small in some cases so you may want to change it with the following settings in the application.conf file: # Max textual content length play.http.parser.maxMemoryBuffer=1M # Max file size play.http.parser.maxDiskBuffer=1G Note if you are using a NGINX reverse proxy in front of Cortex, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size as defined in Cortex application.conf file.","title":"Entity Size Limit"},{"location":"cortex/installation-and-configuration/analyzers-responders/","text":"Analyzers & Responders # Run with Docker # Ensure Cortex is authorized to run use Docker To run docker images of Analyzers & Responders, Cortex should have permissions to use docker. sudo usermod -G docker cortex Configure Cortex # To run Analyzers&Responders with Docker images, Cortex should be able have access to Internet: To download public catalogs from download.thehive-project.org To download Docker images from hub.docker.com ( https://hub.docker.com/search?q=cortexneurons ). /etc/cortex/application.conf [ .. ] analyzer { # Directory that holds analyzers urls = [ \"https://download.thehive-project.org/analyzers.json\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } responder { # Directory that holds responders urls = [ \"https://download.thehive-project.org/responders.json\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } [..] Store & run programs on the host # Additionnal packages # Some system packages are required to run Analyzers&Responders programs successfully: Debian sudo apt install -y --no-install-recommends python3-pip python3-dev ssdeep libfuzzy-dev libfuzzy2 libimage-exiftool-perl libmagic1 build-essential git libssl-dev You may need to install Python's setuptools and update pip/pip3: sudo pip3 install -U pip setuptools Clone the repository # Once finished, clone the Cortex-analyzers repository in the directory of your choosing: cd /opt git clone https://github.com/TheHive-Project/Cortex-Analyzers chown -R cortex:cortex /opt/Cortex-Analyzers Install dependencies # Each analyzer comes with its own, pip compatible requirements.txt file. You can install all requirements with the following commands: cd /opt for I in $(find Cortex-Analyzers -name 'requirements.txt'); do sudo -H pip3 install -r $I || true; done Configure Cortex # Next, you'll need to tell Cortex where to find the analyzers. Analyzers may be in different directories as shown in this dummy example of the Cortex configuration file ( application.conf ): /etc/cortex/application.conf [ .. ] analyzer { # Directory that holds analyzers urls = [ \"/opt/Cortex-Analyzers/responders\", ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } responder { # Directory that holds responders urls = [ \"/opt/Cortex-Analyzers/responders\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } [..] Run you own Analyzers & Responders # Either you run them from the host or with Docker images, you can also run your own custom Analyzers and Responders. Dedicated folder # Create a dedicated folder to host your programs: cd /opt mkdir -p Custom-Analyzers/ { analyzers,responder } chown -R cortex:cortex /opt/Cortex-Analyzers Update Cortex configuration # Update analyzer.urls and responders.urls accordingly. /etc/cortex/application.conf [ .. ] analyzer { # Directory that holds analyzers urls = [ \"https://download.thehive-project.org/analyzers.json\", \"/opt/Custom-Analyzers/analyzers\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } responder { # Directory that holds responders urls = [ \"https://download.thehive-project.org/responders.json\", \"/opt/Custom-Analyzers/responders\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } [..] Then restart Cortex for the changes to take effect. How to develop your own Analyzers or Responders ? Have a look at the dedicated documentation: https://thehive-project.github.io/Cortex-Analyzers/dev_guides/how-to-create-an-analyzer/","title":"Configure Analyzers & Responders"},{"location":"cortex/installation-and-configuration/analyzers-responders/#analyzers-responders","text":"","title":"Analyzers &amp; Responders"},{"location":"cortex/installation-and-configuration/analyzers-responders/#run-with-docker","text":"Ensure Cortex is authorized to run use Docker To run docker images of Analyzers & Responders, Cortex should have permissions to use docker. sudo usermod -G docker cortex","title":"Run with Docker"},{"location":"cortex/installation-and-configuration/analyzers-responders/#configure-cortex","text":"To run Analyzers&Responders with Docker images, Cortex should be able have access to Internet: To download public catalogs from download.thehive-project.org To download Docker images from hub.docker.com ( https://hub.docker.com/search?q=cortexneurons ). /etc/cortex/application.conf [ .. ] analyzer { # Directory that holds analyzers urls = [ \"https://download.thehive-project.org/analyzers.json\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } responder { # Directory that holds responders urls = [ \"https://download.thehive-project.org/responders.json\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } [..]","title":"Configure Cortex"},{"location":"cortex/installation-and-configuration/analyzers-responders/#store-run-programs-on-the-host","text":"","title":"Store &amp; run programs on the host"},{"location":"cortex/installation-and-configuration/analyzers-responders/#additionnal-packages","text":"Some system packages are required to run Analyzers&Responders programs successfully: Debian sudo apt install -y --no-install-recommends python3-pip python3-dev ssdeep libfuzzy-dev libfuzzy2 libimage-exiftool-perl libmagic1 build-essential git libssl-dev You may need to install Python's setuptools and update pip/pip3: sudo pip3 install -U pip setuptools","title":"Additionnal packages"},{"location":"cortex/installation-and-configuration/analyzers-responders/#clone-the-repository","text":"Once finished, clone the Cortex-analyzers repository in the directory of your choosing: cd /opt git clone https://github.com/TheHive-Project/Cortex-Analyzers chown -R cortex:cortex /opt/Cortex-Analyzers","title":"Clone the repository"},{"location":"cortex/installation-and-configuration/analyzers-responders/#install-dependencies","text":"Each analyzer comes with its own, pip compatible requirements.txt file. You can install all requirements with the following commands: cd /opt for I in $(find Cortex-Analyzers -name 'requirements.txt'); do sudo -H pip3 install -r $I || true; done","title":"Install dependencies"},{"location":"cortex/installation-and-configuration/analyzers-responders/#configure-cortex_1","text":"Next, you'll need to tell Cortex where to find the analyzers. Analyzers may be in different directories as shown in this dummy example of the Cortex configuration file ( application.conf ): /etc/cortex/application.conf [ .. ] analyzer { # Directory that holds analyzers urls = [ \"/opt/Cortex-Analyzers/responders\", ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } responder { # Directory that holds responders urls = [ \"/opt/Cortex-Analyzers/responders\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } [..]","title":"Configure Cortex"},{"location":"cortex/installation-and-configuration/analyzers-responders/#run-you-own-analyzers-responders","text":"Either you run them from the host or with Docker images, you can also run your own custom Analyzers and Responders.","title":"Run you own Analyzers &amp; Responders"},{"location":"cortex/installation-and-configuration/analyzers-responders/#dedicated-folder","text":"Create a dedicated folder to host your programs: cd /opt mkdir -p Custom-Analyzers/ { analyzers,responder } chown -R cortex:cortex /opt/Cortex-Analyzers","title":"Dedicated folder"},{"location":"cortex/installation-and-configuration/analyzers-responders/#update-cortex-configuration","text":"Update analyzer.urls and responders.urls accordingly. /etc/cortex/application.conf [ .. ] analyzer { # Directory that holds analyzers urls = [ \"https://download.thehive-project.org/analyzers.json\", \"/opt/Custom-Analyzers/analyzers\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } responder { # Directory that holds responders urls = [ \"https://download.thehive-project.org/responders.json\", \"/opt/Custom-Analyzers/responders\" ] fork-join-executor { # Min number of threads available for analyze parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads available for analyze parallelism-max = 4 } } [..] Then restart Cortex for the changes to take effect. How to develop your own Analyzers or Responders ? Have a look at the dedicated documentation: https://thehive-project.github.io/Cortex-Analyzers/dev_guides/how-to-create-an-analyzer/","title":"Update Cortex configuration"},{"location":"cortex/installation-and-configuration/authentication/","text":"Authentication # Like TheHive, Cortex supports local, LDAP, Active Directory (AD), X.509 SSO and/or API keys for authentication and OAuth2. Please note that API keys can only be used to interact with the Cortex API (for example when TheHive is interfaced with a Cortex instance, it must use an API key to authenticate to it). API keys cannot be used to authenticate to the Web UI. By default, Cortex relies on local credentials stored in Elasticsearch. Authentication methods are stored in the auth.provider parameter, which is multi-valued. When a user logs in, each authentication method is tried in order until one succeeds. If no authentication method works, an error is returned and the user cannot log in. The default values within the configuration file are: auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # services.LocalAuthSrv : passwords are stored in user entity (in Elasticsearch). No configuration is required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key # oauth2 : use OAuth/OIDC to authenticate users. Configuration is under \"auth.oauth2\" and \"auth.sso\" keys provider = [local] # By default, basic authentication is disabled. You can enable it by setting \"method.basic\" to true. method.basic = false ad { # The name of the Microsoft Windows domain using the DNS format. This parameter is required. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers. If not set, Cortex uses \"domainFQDN\". #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Microsoft Windows domain name using the short format. This parameter is required. #domainName = \"MYDOMAIN\" # Use SSL to connect to the domain controller(s). #useSSL = true } ldap { # LDAP server name or address. Port can be specified (host:port). This parameter is required. #serverName = \"ldap.mydomain.local:389\" # If you have multiple ldap servers, use the multi-valued settings. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Use SSL to connect to directory server #useSSL = true # Account to use to bind on LDAP server. This parameter is required. #bindDN = \"cn=cortex,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user {0} is replaced by user name. This parameter is required. #filter = \"(cn={0})\" } oauth2 { # URL of the authorization server #clientId = \"client-id\" #clientSecret = \"client-secret\" #redirectUri = \"https://my-cortex-instance.example/api/ssoLogin\" #responseType = \"code\" #grantType = \"authorization_code\" # URL from where to get the access token #authorizationUrl = \"https://auth-site.com/OAuth/Authorize\" #tokenUrl = \"https://auth-site.com/OAuth/Token\" # The endpoint from which to obtain user details using the OAuth token, after successful login #userUrl = \"https://auth-site.com/api/User\" #scope = [\"openid profile\"] } # Single-Sign On sso { # Autocreate user in database? #autocreate = false # Autoupdate its profile and roles? #autoupdate = false # Autologin user using SSO? #autologin = false # Name of mapping class from user resource to backend user ('simple' or 'group') #mapper = group #attributes { # login = \"user\" # name = \"name\" # groups = \"groups\" # organization = \"org\" #} #defaultRoles = [\"read\"] #defaultOrganization = \"csirt\" #groups { # # URL to retreive groups (leave empty if you are using OIDC) # #url = \"https://auth-site.com/api/Groups\" # # Group mappings, you can have multiple roles for each group: they are merged # mappings { # admin-profile-name = [\"admin\"] # editor-profile-name = [\"write\"] # reader-profile-name = [\"read\"] # } #} #mapper = simple #attributes { # login = \"user\" # name = \"name\" # roles = \"roles\" # organization = \"org\" #} #defaultRoles = [\"read\"] #defaultOrganization = \"csirt\" } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h } OAuth2/OpenID Connect # To enable authentication using OAuth2/OpenID Connect, edit the application.conf file and supply the values of auth.oauth2 according to your environment. In addition, you need to supply: auth.sso.attributes.login : name of the attribute containing the OAuth2 user's login in retreived user info (mandatory) auth.sso.attributes.name : name of the attribute containing the OAuth2 user's name in retreived user info (mandatory) auth.sso.attributes.groups : name of the attribute containing the OAuth2 user's groups (mandatory using groups mappings) auth.sso.attributes.roles : name of the attribute containing the OAuth2 user's roles in retreived user info (mandatory using simple mapping) Important note Authenticate the user using an external OAuth2 authenticator server. The configuration is: clientId (string) client ID in the OAuth2 server. clientSecret (string) client secret in the OAuth2 server. redirectUri (string) the url of TheHive AOuth2 page (.../api/ssoLogin). responseType (string) type of the response. Currently only \"code\" is accepted. grantType (string) type of the grant. Currently only \"authorization_code\" is accepted. authorizationUrl (string) the url of the OAuth2 server. authorizationHeader (string) prefix of the authorization header to get user info: Bearer, token, ... tokenUrl (string) the token url of the OAuth2 server. userUrl (string) the url to get user information in OAuth2 server. scope (list of string) list of scope. Example configuration for SSO w/ Oauth2 & Github auth { provider = [local, oauth2] [..] sso { autocreate: false autoupdate: false mapper: \"simple\" attributes { login: \"login\" name: \"name\" roles: \"role\" } defaultRoles: [\"read\", \"analyze\"] defaultOrganization: \"demo\" } oauth2 { name: oauth2 clientId: \"Client_ID\" clientSecret: \"Client_ID\" redirectUri: \"http://localhost:9001/api/ssoLogin\" responseType: code grantType: \"authorization_code\" authorizationUrl: \"https://github.com/login/oauth/authorize\" authorizationHeader: \"token\" tokenUrl: \"https://github.com/login/oauth/access_token\" userUrl: \"https://api.github.com/user\" scope: [\"user\"] } [..] }","title":"Authentication options"},{"location":"cortex/installation-and-configuration/authentication/#authentication","text":"Like TheHive, Cortex supports local, LDAP, Active Directory (AD), X.509 SSO and/or API keys for authentication and OAuth2. Please note that API keys can only be used to interact with the Cortex API (for example when TheHive is interfaced with a Cortex instance, it must use an API key to authenticate to it). API keys cannot be used to authenticate to the Web UI. By default, Cortex relies on local credentials stored in Elasticsearch. Authentication methods are stored in the auth.provider parameter, which is multi-valued. When a user logs in, each authentication method is tried in order until one succeeds. If no authentication method works, an error is returned and the user cannot log in. The default values within the configuration file are: auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # services.LocalAuthSrv : passwords are stored in user entity (in Elasticsearch). No configuration is required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key # oauth2 : use OAuth/OIDC to authenticate users. Configuration is under \"auth.oauth2\" and \"auth.sso\" keys provider = [local] # By default, basic authentication is disabled. You can enable it by setting \"method.basic\" to true. method.basic = false ad { # The name of the Microsoft Windows domain using the DNS format. This parameter is required. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers. If not set, Cortex uses \"domainFQDN\". #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Microsoft Windows domain name using the short format. This parameter is required. #domainName = \"MYDOMAIN\" # Use SSL to connect to the domain controller(s). #useSSL = true } ldap { # LDAP server name or address. Port can be specified (host:port). This parameter is required. #serverName = \"ldap.mydomain.local:389\" # If you have multiple ldap servers, use the multi-valued settings. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Use SSL to connect to directory server #useSSL = true # Account to use to bind on LDAP server. This parameter is required. #bindDN = \"cn=cortex,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user {0} is replaced by user name. This parameter is required. #filter = \"(cn={0})\" } oauth2 { # URL of the authorization server #clientId = \"client-id\" #clientSecret = \"client-secret\" #redirectUri = \"https://my-cortex-instance.example/api/ssoLogin\" #responseType = \"code\" #grantType = \"authorization_code\" # URL from where to get the access token #authorizationUrl = \"https://auth-site.com/OAuth/Authorize\" #tokenUrl = \"https://auth-site.com/OAuth/Token\" # The endpoint from which to obtain user details using the OAuth token, after successful login #userUrl = \"https://auth-site.com/api/User\" #scope = [\"openid profile\"] } # Single-Sign On sso { # Autocreate user in database? #autocreate = false # Autoupdate its profile and roles? #autoupdate = false # Autologin user using SSO? #autologin = false # Name of mapping class from user resource to backend user ('simple' or 'group') #mapper = group #attributes { # login = \"user\" # name = \"name\" # groups = \"groups\" # organization = \"org\" #} #defaultRoles = [\"read\"] #defaultOrganization = \"csirt\" #groups { # # URL to retreive groups (leave empty if you are using OIDC) # #url = \"https://auth-site.com/api/Groups\" # # Group mappings, you can have multiple roles for each group: they are merged # mappings { # admin-profile-name = [\"admin\"] # editor-profile-name = [\"write\"] # reader-profile-name = [\"read\"] # } #} #mapper = simple #attributes { # login = \"user\" # name = \"name\" # roles = \"roles\" # organization = \"org\" #} #defaultRoles = [\"read\"] #defaultOrganization = \"csirt\" } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h }","title":"Authentication"},{"location":"cortex/installation-and-configuration/authentication/#oauth2openid-connect","text":"To enable authentication using OAuth2/OpenID Connect, edit the application.conf file and supply the values of auth.oauth2 according to your environment. In addition, you need to supply: auth.sso.attributes.login : name of the attribute containing the OAuth2 user's login in retreived user info (mandatory) auth.sso.attributes.name : name of the attribute containing the OAuth2 user's name in retreived user info (mandatory) auth.sso.attributes.groups : name of the attribute containing the OAuth2 user's groups (mandatory using groups mappings) auth.sso.attributes.roles : name of the attribute containing the OAuth2 user's roles in retreived user info (mandatory using simple mapping) Important note Authenticate the user using an external OAuth2 authenticator server. The configuration is: clientId (string) client ID in the OAuth2 server. clientSecret (string) client secret in the OAuth2 server. redirectUri (string) the url of TheHive AOuth2 page (.../api/ssoLogin). responseType (string) type of the response. Currently only \"code\" is accepted. grantType (string) type of the grant. Currently only \"authorization_code\" is accepted. authorizationUrl (string) the url of the OAuth2 server. authorizationHeader (string) prefix of the authorization header to get user info: Bearer, token, ... tokenUrl (string) the token url of the OAuth2 server. userUrl (string) the url to get user information in OAuth2 server. scope (list of string) list of scope. Example configuration for SSO w/ Oauth2 & Github auth { provider = [local, oauth2] [..] sso { autocreate: false autoupdate: false mapper: \"simple\" attributes { login: \"login\" name: \"name\" roles: \"role\" } defaultRoles: [\"read\", \"analyze\"] defaultOrganization: \"demo\" } oauth2 { name: oauth2 clientId: \"Client_ID\" clientSecret: \"Client_ID\" redirectUri: \"http://localhost:9001/api/ssoLogin\" responseType: code grantType: \"authorization_code\" authorizationUrl: \"https://github.com/login/oauth/authorize\" authorizationHeader: \"token\" tokenUrl: \"https://github.com/login/oauth/access_token\" userUrl: \"https://api.github.com/user\" scope: [\"user\"] } [..] }","title":"OAuth2/OpenID Connect"},{"location":"cortex/installation-and-configuration/database/","text":"Database configuration # /etc/cortex/application.conf [ .. ] ## ElasticSearch search { index = cortex # For cluster, join address:port with ',' : \"http://ip1:9200,ip2:9200,ip3:9200\" uri = \"http://127.0.0.1:9200\" ## Advanced configuration # Scroll keepalive. #keepalive = 1m # Scroll page size. #pagesize = 50 # Number of shards #nbshards = 5 # Number of replicas #nbreplicas = 1 # Arbitrary settings #settings { # # Maximum number of nested fields # mapping.nested_fields.limit = 100 #} ## Authentication configuration #username = \"\" #password = \"\" ## SSL configuration #keyStore { # path = \"/path/to/keystore\" # type = \"JKS\" # or PKCS12 # password = \"keystore-password\" #} #trustStore { # path = \"/path/to/trustStore\" # type = \"JKS\" # or PKCS12 # password = \"trustStore-password\" #} }","title":"Database configuration"},{"location":"cortex/installation-and-configuration/database/#database-configuration","text":"/etc/cortex/application.conf [ .. ] ## ElasticSearch search { index = cortex # For cluster, join address:port with ',' : \"http://ip1:9200,ip2:9200,ip3:9200\" uri = \"http://127.0.0.1:9200\" ## Advanced configuration # Scroll keepalive. #keepalive = 1m # Scroll page size. #pagesize = 50 # Number of shards #nbshards = 5 # Number of replicas #nbreplicas = 1 # Arbitrary settings #settings { # # Maximum number of nested fields # mapping.nested_fields.limit = 100 #} ## Authentication configuration #username = \"\" #password = \"\" ## SSL configuration #keyStore { # path = \"/path/to/keystore\" # type = \"JKS\" # or PKCS12 # password = \"keystore-password\" #} #trustStore { # path = \"/path/to/trustStore\" # type = \"JKS\" # or PKCS12 # password = \"trustStore-password\" #} }","title":"Database configuration"},{"location":"cortex/installation-and-configuration/docker/","text":"Parameters for Docker # list of options # docker.container.capAdd : (array of string) Add Linux capabilities docker.container.capDrop : (array of string) Drop Linux capabilities docker.container.cgroupParent : (string) Cgroup to run a container in docker.container.cpuPeriod : (integer) Limit the CPU CFS (Completely Fair Scheduler) period docker.container.cpuQuota : (integer) Limit the CPU CFS (Completely Fair Scheduler) quota docker.container.dns : (array of string) Set custom dns servers for the container docker.container.dnsSearch : (array of string) Search list for host-name lookup. docker.container.extraHosts : (array of string) Add a line to /etc/hosts (host:IP) docker.container.kernelMemory : (integer) Kernel memory limit docker.container.memoryReservation : (integer) Memory soft limit docker.container.memory : (integer) Memory limit docker.container.memorySwap : (integer) Total memory limit (memory + swap) docker.container.memorySwappiness : (integer) Tune a container\u2019s memory swappiness behavior. Accepts an integer between 0 and 100 docker.container.networkMode : (string) name of the network docker.container.privileged : (boolean) Give extended privileges to this container job.directory : (string) Folder used by Cortex binary inside the container to share input and output data of Analyzers & Responders job.dockerDirectory = (string) Folder on the host used by Analyzers & Responders to share input and output data with Cortex Dockerized analyzers / responders # To run Analyzers&Responders as docker images, use our available catalogs to register them. In Cortex configuration file, update analyzer.urls and responder.urls and tell Cortex how to find analyzers and responders. These settings accept: - a path to a directory where workers are installed (like previous version of Cortex) - a path or an url (http(s)) to a JSON file containing all worker definitions (merge of all JSON in one array) If you want to use dockerized analyzers, you can add the following urls: - analyzers-stable.json (once used, analyzer is never updated) - analyzers.json (updated when new version is released) - analyzers-devel.json (updated at each commit, used for development) For responders urls are: - responders-stable.json (once used, analyzer is never updated) - responders.json (updated when new version is released) - responders-devel.json (updated at each commit, used for development)","title":"Docker parameters"},{"location":"cortex/installation-and-configuration/docker/#parameters-for-docker","text":"","title":"Parameters for Docker"},{"location":"cortex/installation-and-configuration/docker/#list-of-options","text":"docker.container.capAdd : (array of string) Add Linux capabilities docker.container.capDrop : (array of string) Drop Linux capabilities docker.container.cgroupParent : (string) Cgroup to run a container in docker.container.cpuPeriod : (integer) Limit the CPU CFS (Completely Fair Scheduler) period docker.container.cpuQuota : (integer) Limit the CPU CFS (Completely Fair Scheduler) quota docker.container.dns : (array of string) Set custom dns servers for the container docker.container.dnsSearch : (array of string) Search list for host-name lookup. docker.container.extraHosts : (array of string) Add a line to /etc/hosts (host:IP) docker.container.kernelMemory : (integer) Kernel memory limit docker.container.memoryReservation : (integer) Memory soft limit docker.container.memory : (integer) Memory limit docker.container.memorySwap : (integer) Total memory limit (memory + swap) docker.container.memorySwappiness : (integer) Tune a container\u2019s memory swappiness behavior. Accepts an integer between 0 and 100 docker.container.networkMode : (string) name of the network docker.container.privileged : (boolean) Give extended privileges to this container job.directory : (string) Folder used by Cortex binary inside the container to share input and output data of Analyzers & Responders job.dockerDirectory = (string) Folder on the host used by Analyzers & Responders to share input and output data with Cortex","title":"list of options"},{"location":"cortex/installation-and-configuration/docker/#dockerized-analyzers-responders","text":"To run Analyzers&Responders as docker images, use our available catalogs to register them. In Cortex configuration file, update analyzer.urls and responder.urls and tell Cortex how to find analyzers and responders. These settings accept: - a path to a directory where workers are installed (like previous version of Cortex) - a path or an url (http(s)) to a JSON file containing all worker definitions (merge of all JSON in one array) If you want to use dockerized analyzers, you can add the following urls: - analyzers-stable.json (once used, analyzer is never updated) - analyzers.json (updated when new version is released) - analyzers-devel.json (updated at each commit, used for development) For responders urls are: - responders-stable.json (once used, analyzer is never updated) - responders.json (updated when new version is released) - responders-devel.json (updated at each commit, used for development)","title":"Dockerized analyzers / responders"},{"location":"cortex/installation-and-configuration/proxy-settings/","text":"Proxy settings # Make Cortex use a HTTP proxy server # Basically, Cortex required to connect to Internet, especially to gather catalogs of docker images of public Analyzers & Responders. /etc/cortex/application.conf [ .. ] play.ws.proxy { host = http://PROXYSERVERADDRESS:PORT port = http://PROXYSERVERADDRESS:PORT } [..] Operating System # /etc/environment export http_proxy=http://PROXYSERVERADDRESS:PORT export https_proxy=http://PROXYSERVERADDRESS:PORT Specific configuration for Debian apt application /etc/apt/apt.conf.d/80proxy HTTP::proxy \"http://PROXYSERVERADDRESS:PORT\"; HTTPS::proxy \"http://PROXYSERVERADDRESS:PORT\"; pip # If Analyzers and Responders requirements have to be installed on the host, and the host is behind a proxy server, configure the pip command to use the proxy server ; use the option --proxy http://PROXYSERVERADDRESS:PORT\" , and --cert path/to/cacert.pem if a custom certificate is used by the proxy. pip3 install --proxy http://PROXYSERVERADDRESS:PORT\" -r analyzers/*/requirements.txt or pip3 install --proxy http://PROXYSERVERADDRESS:PORT\" --cert path/to/cacert.pem -r analyzers/*/requirements.txt Git # sudo git config --global http.proxy http://PROXYSERVERADDRESS:PORT sudo git config --global https.proxy http://PROXYSERVERADDRESS:PORT Docker # If using Analyzers & Responders as docker images, setting up proxy parameters could be required to download images. Update Docker engine configuration by editing/creating the file /etc/systemd/system/docker.service.d/http-proxy.conf : /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=http://PROXYSERVERADDRESS:PORT\" Environment=\"http://PROXYSERVERADDRESS:PORT\" Then run: sudo systemctl daemon-reload sudo systemctl restart docker","title":"Using Cortex behind a proxy"},{"location":"cortex/installation-and-configuration/proxy-settings/#proxy-settings","text":"","title":"Proxy settings"},{"location":"cortex/installation-and-configuration/proxy-settings/#make-cortex-use-a-http-proxy-server","text":"Basically, Cortex required to connect to Internet, especially to gather catalogs of docker images of public Analyzers & Responders. /etc/cortex/application.conf [ .. ] play.ws.proxy { host = http://PROXYSERVERADDRESS:PORT port = http://PROXYSERVERADDRESS:PORT } [..]","title":"Make Cortex use a HTTP proxy server"},{"location":"cortex/installation-and-configuration/proxy-settings/#operating-system","text":"/etc/environment export http_proxy=http://PROXYSERVERADDRESS:PORT export https_proxy=http://PROXYSERVERADDRESS:PORT Specific configuration for Debian apt application /etc/apt/apt.conf.d/80proxy HTTP::proxy \"http://PROXYSERVERADDRESS:PORT\"; HTTPS::proxy \"http://PROXYSERVERADDRESS:PORT\";","title":"Operating System"},{"location":"cortex/installation-and-configuration/proxy-settings/#pip","text":"If Analyzers and Responders requirements have to be installed on the host, and the host is behind a proxy server, configure the pip command to use the proxy server ; use the option --proxy http://PROXYSERVERADDRESS:PORT\" , and --cert path/to/cacert.pem if a custom certificate is used by the proxy. pip3 install --proxy http://PROXYSERVERADDRESS:PORT\" -r analyzers/*/requirements.txt or pip3 install --proxy http://PROXYSERVERADDRESS:PORT\" --cert path/to/cacert.pem -r analyzers/*/requirements.txt","title":"pip"},{"location":"cortex/installation-and-configuration/proxy-settings/#git","text":"sudo git config --global http.proxy http://PROXYSERVERADDRESS:PORT sudo git config --global https.proxy http://PROXYSERVERADDRESS:PORT","title":"Git"},{"location":"cortex/installation-and-configuration/proxy-settings/#docker","text":"If using Analyzers & Responders as docker images, setting up proxy parameters could be required to download images. Update Docker engine configuration by editing/creating the file /etc/systemd/system/docker.service.d/http-proxy.conf : /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=http://PROXYSERVERADDRESS:PORT\" Environment=\"http://PROXYSERVERADDRESS:PORT\" Then run: sudo systemctl daemon-reload sudo systemctl restart docker","title":"Docker"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/","text":"Docker # To use the Docker image, you must use Docker (courtesy of Captain Obvious). Alternatively, it's also possible to run the image using Podman . By default, the docker image generate a configuration file for Cortex with: - the Elasticsearch uri is determined by resolving the host name \"elasticsearch\", - the analyzers and responders official location, - a generated secret (used to protect the user sessions). The behaviour of the Cortex Docker image can be customized using environment variables or parameters: Parameter Env variable Description --no-config no_config=1 Do not configure Cortex --no-config-secret no_config_secret=1 Do not add the random secret to the configuration --no-config-es no_config_es=1 do not add elasticsearch hosts to configuration --es-uri <uri> es_uri=<uri> use this string to configure elasticsearch hosts (format: http(s)://host:port,host:port(/prefix)?querystring) --es-hostname <host> es_hostname=host resolve this hostname to find elasticsearch instances --secret <secret> secret=<secret> secret to secure sessions --show-secret show_secret=1 show the generated secret --job-directory <dir> job_directory=<dir> use this directory to store job files --docker-job-directory <dir> docker_job_directory=<dir> indicate the job directory in the host (not inside container) --analyzer-url <url> analyzer_urls=<url>,<url>,... where analyzers are located (url or path) --responder-url <url> responder_urls=<url>,<url>,... where responders are located (url or path) --start-docker start_docker=1 start an internal docker (inside container) to run analyzers/responders --daemon-user <user> daemon_user=<user> run cortex using this user At the end of the generated configuration, the file /etc/cortex/application.conf is included. Thus you can override any setting by binding your own application.conf into this file: docker run --volume /path/to/my/application.conf:/etc/cortex/application.conf thehiveproject/cortex:latest --es-uri http://elasticsearch.local:9200 Cortex uses docker to run analyzers and responders. If you run Cortex inside a docker, you can: give Cortex access to docker service or podman service (recommended solution) start a docker service inside Cortex docker container Cortex uses main docker service # In order to use docker service the docker socket must be bound into Cortex container. Moreover, as Cortex shares files with analyzers, a folder must be bound between them. docker run --volume /var/run/docker.sock:/var/run/docker.sock --volume /var/run/cortex/jobs:/tmp/cortex-jobs thehiveproject/cortex:latest --job-directory /tmp/cortex-jobs --docker-job-directory /var/run/cortex/jobs Cortex can instantiate docker container by using the docker socket /var/run/docker.sock . The folder /var/run/cortex/jobs is used to store temporary file of jobs. The folder /tmp/cortex-jobs is job folder inside the docker. In order to make job file visible to analyzer docker, Cortex needs to know both folders (parameters --job-directory and -docker-job-directory ). On most cases, job directories are the same and --docker-job-directory can be omitted. If you run Cortex in Windows, the docker service is accessible through the named pipe \\\\.\\pipe\\docker_engine . The command becomes docker run --volume //./pipe/docker_engine://./pipe/docker_engine --volume C:\\\\CORTEX\\\\JOBS:/tmp/cortex-jobs thehiveproject/cortex:latest --job-directory /tmp/cortex-jobs --docker-job-directory C:\\\\CORTEX\\\\JOBS Docker in docker (docker-ception) # You can also run docker service inside Cortex container, a docker in a docker with --start-docker parameter. The container must be run in privileged mode. docker run --privileged thehiveproject/cortex:latest --start-docker In this case you don't need to bind job directory. Use Docker-compose # Cortex requires Elasticsearch to run. You can use docker-compose to start them together in Docker or install and configure Elasticsearch manually. Docker-compose can start multiple dockers and link them together. The following docker-compose.yml file starts Elasticsearch and Cortex: version: \"2\" services: elasticsearch: image: elasticsearch:7.9.1 environment: - http.host=0.0.0.0 - discovery.type=single-node - script.allowed_types=inline - thread_pool.search.queue_size=100000 - thread_pool.write.queue_size=10000 volumes: - /path/to/data:/usr/share/elasticsearch/data cortex: image: thehiveproject/cortex:3.1.1 environment: - job_directory=${job_directory} volumes: - /var/run/docker.sock:/var/run/docker.sock - ${job_directory}:${job_directory} depends_on: - elasticsearch ports: - \"0.0.0.0:9001:9001\" Put this docker-compose file and .env in an empty folder and run docker-compose up . Cortex is exposed on 9001/tcp port. These ports can be changed by modifying the docker-compose file. For advanced configuration, visit our Docker Templates repository Cortex with podman # Like docker, podman will be able to run the container image of cortex and of its analyzers. The examples below assume that the containers are run as rootful . For Cortex to interact with podman, it needs to use the podman socket . On some systems, podman will automatically install and enable this service. You can check this on your system with: systemctl status podman.socket Here we assume that the podman socket is accessible on /run/podman/podman.sock . This may change based on your system. Cortex uses podman service You need to mount the podman socket inside the container to /var/run/docker.sock podman run \\ --rm \\ --name cortex \\ -p 9001 :9001 \\ -v /var/run/cortex/jobs:/tmp/cortex-jobs \\ -v /run/podman/podman.sock:/var/run/docker.sock \\ docker.io/thehiveproject/cortex:3.1.7 \\ --job-directory /tmp/cortex-jobs \\ --docker-job-directory /var/run/cortex/jobs \\ --es-uri http:// $ES_IP :9200 With this configuration, Cortex analyzers will be run by podman. Image not found Podman may have trouble pulling cortex neurons images from the regular docker registry. You may have to add docker.io as an unqualified registry. To do this, add this line to your config /etc/containers/registries.conf : unqualified-search-registries = ['docker.io'] Then restart the podman socket service too Docker in podman By running with the flag --privileged , it is possible to start docker inside a podman container podman run \\ --privileged \\ --rm \\ --name cortex \\ -p 9001 :9001 \\ docker.io/thehiveproject/cortex:3.1.7 \\ --es-uri http:// $ES_IP :9200 --start-docker","title":"Run Cortex with Docker"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#docker","text":"To use the Docker image, you must use Docker (courtesy of Captain Obvious). Alternatively, it's also possible to run the image using Podman . By default, the docker image generate a configuration file for Cortex with: - the Elasticsearch uri is determined by resolving the host name \"elasticsearch\", - the analyzers and responders official location, - a generated secret (used to protect the user sessions). The behaviour of the Cortex Docker image can be customized using environment variables or parameters: Parameter Env variable Description --no-config no_config=1 Do not configure Cortex --no-config-secret no_config_secret=1 Do not add the random secret to the configuration --no-config-es no_config_es=1 do not add elasticsearch hosts to configuration --es-uri <uri> es_uri=<uri> use this string to configure elasticsearch hosts (format: http(s)://host:port,host:port(/prefix)?querystring) --es-hostname <host> es_hostname=host resolve this hostname to find elasticsearch instances --secret <secret> secret=<secret> secret to secure sessions --show-secret show_secret=1 show the generated secret --job-directory <dir> job_directory=<dir> use this directory to store job files --docker-job-directory <dir> docker_job_directory=<dir> indicate the job directory in the host (not inside container) --analyzer-url <url> analyzer_urls=<url>,<url>,... where analyzers are located (url or path) --responder-url <url> responder_urls=<url>,<url>,... where responders are located (url or path) --start-docker start_docker=1 start an internal docker (inside container) to run analyzers/responders --daemon-user <user> daemon_user=<user> run cortex using this user At the end of the generated configuration, the file /etc/cortex/application.conf is included. Thus you can override any setting by binding your own application.conf into this file: docker run --volume /path/to/my/application.conf:/etc/cortex/application.conf thehiveproject/cortex:latest --es-uri http://elasticsearch.local:9200 Cortex uses docker to run analyzers and responders. If you run Cortex inside a docker, you can: give Cortex access to docker service or podman service (recommended solution) start a docker service inside Cortex docker container","title":"Docker"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#cortex-uses-main-docker-service","text":"In order to use docker service the docker socket must be bound into Cortex container. Moreover, as Cortex shares files with analyzers, a folder must be bound between them. docker run --volume /var/run/docker.sock:/var/run/docker.sock --volume /var/run/cortex/jobs:/tmp/cortex-jobs thehiveproject/cortex:latest --job-directory /tmp/cortex-jobs --docker-job-directory /var/run/cortex/jobs Cortex can instantiate docker container by using the docker socket /var/run/docker.sock . The folder /var/run/cortex/jobs is used to store temporary file of jobs. The folder /tmp/cortex-jobs is job folder inside the docker. In order to make job file visible to analyzer docker, Cortex needs to know both folders (parameters --job-directory and -docker-job-directory ). On most cases, job directories are the same and --docker-job-directory can be omitted. If you run Cortex in Windows, the docker service is accessible through the named pipe \\\\.\\pipe\\docker_engine . The command becomes docker run --volume //./pipe/docker_engine://./pipe/docker_engine --volume C:\\\\CORTEX\\\\JOBS:/tmp/cortex-jobs thehiveproject/cortex:latest --job-directory /tmp/cortex-jobs --docker-job-directory C:\\\\CORTEX\\\\JOBS","title":"Cortex uses main docker service"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#docker-in-docker-docker-ception","text":"You can also run docker service inside Cortex container, a docker in a docker with --start-docker parameter. The container must be run in privileged mode. docker run --privileged thehiveproject/cortex:latest --start-docker In this case you don't need to bind job directory.","title":"Docker in docker (docker-ception)"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#use-docker-compose","text":"Cortex requires Elasticsearch to run. You can use docker-compose to start them together in Docker or install and configure Elasticsearch manually. Docker-compose can start multiple dockers and link them together. The following docker-compose.yml file starts Elasticsearch and Cortex: version: \"2\" services: elasticsearch: image: elasticsearch:7.9.1 environment: - http.host=0.0.0.0 - discovery.type=single-node - script.allowed_types=inline - thread_pool.search.queue_size=100000 - thread_pool.write.queue_size=10000 volumes: - /path/to/data:/usr/share/elasticsearch/data cortex: image: thehiveproject/cortex:3.1.1 environment: - job_directory=${job_directory} volumes: - /var/run/docker.sock:/var/run/docker.sock - ${job_directory}:${job_directory} depends_on: - elasticsearch ports: - \"0.0.0.0:9001:9001\" Put this docker-compose file and .env in an empty folder and run docker-compose up . Cortex is exposed on 9001/tcp port. These ports can be changed by modifying the docker-compose file. For advanced configuration, visit our Docker Templates repository","title":"Use Docker-compose"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#cortex-with-podman","text":"Like docker, podman will be able to run the container image of cortex and of its analyzers. The examples below assume that the containers are run as rootful . For Cortex to interact with podman, it needs to use the podman socket . On some systems, podman will automatically install and enable this service. You can check this on your system with: systemctl status podman.socket Here we assume that the podman socket is accessible on /run/podman/podman.sock . This may change based on your system. Cortex uses podman service You need to mount the podman socket inside the container to /var/run/docker.sock podman run \\ --rm \\ --name cortex \\ -p 9001 :9001 \\ -v /var/run/cortex/jobs:/tmp/cortex-jobs \\ -v /run/podman/podman.sock:/var/run/docker.sock \\ docker.io/thehiveproject/cortex:3.1.7 \\ --job-directory /tmp/cortex-jobs \\ --docker-job-directory /var/run/cortex/jobs \\ --es-uri http:// $ES_IP :9200 With this configuration, Cortex analyzers will be run by podman. Image not found Podman may have trouble pulling cortex neurons images from the regular docker registry. You may have to add docker.io as an unqualified registry. To do this, add this line to your config /etc/containers/registries.conf : unqualified-search-registries = ['docker.io'] Then restart the podman socket service too Docker in podman By running with the flag --privileged , it is possible to start docker inside a podman container podman run \\ --privileged \\ --rm \\ --name cortex \\ -p 9001 :9001 \\ docker.io/thehiveproject/cortex:3.1.7 \\ --es-uri http:// $ES_IP :9200 --start-docker","title":"Cortex with podman"},{"location":"cortex/installation-and-configuration/secret/","text":"Secret key configuration # Setup a secret key for this instance: cat > /etc/cortex/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ Then, in the file /etc/cortex/application.conf , replace the line including play.http.secret.key= by: /etc/cortex/application.conf [ .. ] include \"/etc/cortex/secret.conf\" [..]","title":"Set up a secret key"},{"location":"cortex/installation-and-configuration/secret/#secret-key-configuration","text":"Setup a secret key for this instance: cat > /etc/cortex/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ Then, in the file /etc/cortex/application.conf , replace the line including play.http.secret.key= by: /etc/cortex/application.conf [ .. ] include \"/etc/cortex/secret.conf\" [..]","title":"Secret key configuration"},{"location":"cortex/installation-and-configuration/ssl/","text":"Configure SSL # Connect Cortex using HTTPS # We recommend using a reverse proxy to manage SSL layer; for example, Nginx. Nginx Reference : Configuring HTTPS servers on nginx.org /etc/nginx/sites-available/cortex.conf server { listen 443 ssl http2; server_name cortex; ssl on; ssl_certificate path-to/cortex-server-chained-cert.pem; ssl_certificate_key path-to/cortex-server-key.pem; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; proxy_pass http://127.0.0.1:9001/; proxy_http_version 1.1; } } Certificate manager # Certificate manager is used to store client certificates and certificate authorities. Use custom Certificate Authorities # The prefered way to use custom Certificate Authorities is to use the system configuration. If setting up a custom Certificate Authority (to connect web proxies, remote services like LPAPS server ...) is required globally in the application, the better solution consists of installing it on the OS and restarting Cortex. Debian RPM Ensure the package ca-certificates-java is installed , and copy the CA certificate in the right folder. Then run dpkg-reconfigure ca-certificates and restart Cortex service. apt-get install -y ca-certificates-java mkdir /usr/share/ca-certificates/extra cp mycustomcert.crt /usr/share/ca-certificates/extra dpkg-reconfigure ca-certificates service cortex restart No additionnal packages is required on Fedora or RHEL. Copy the CA certificate in the right folder, run update-ca-trust and restart Cortex service. cp mycustomcert.crt /etc/pki/ca-trust/source/anchors sudo update-ca-trust service cortex restart","title":"SSL configuration"},{"location":"cortex/installation-and-configuration/ssl/#configure-ssl","text":"","title":"Configure SSL"},{"location":"cortex/installation-and-configuration/ssl/#connect-cortex-using-https","text":"We recommend using a reverse proxy to manage SSL layer; for example, Nginx. Nginx Reference : Configuring HTTPS servers on nginx.org /etc/nginx/sites-available/cortex.conf server { listen 443 ssl http2; server_name cortex; ssl on; ssl_certificate path-to/cortex-server-chained-cert.pem; ssl_certificate_key path-to/cortex-server-key.pem; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; proxy_pass http://127.0.0.1:9001/; proxy_http_version 1.1; } }","title":"Connect Cortex using HTTPS"},{"location":"cortex/installation-and-configuration/ssl/#certificate-manager","text":"Certificate manager is used to store client certificates and certificate authorities.","title":"Certificate manager"},{"location":"cortex/installation-and-configuration/ssl/#use-custom-certificate-authorities","text":"The prefered way to use custom Certificate Authorities is to use the system configuration. If setting up a custom Certificate Authority (to connect web proxies, remote services like LPAPS server ...) is required globally in the application, the better solution consists of installing it on the OS and restarting Cortex. Debian RPM Ensure the package ca-certificates-java is installed , and copy the CA certificate in the right folder. Then run dpkg-reconfigure ca-certificates and restart Cortex service. apt-get install -y ca-certificates-java mkdir /usr/share/ca-certificates/extra cp mycustomcert.crt /usr/share/ca-certificates/extra dpkg-reconfigure ca-certificates service cortex restart No additionnal packages is required on Fedora or RHEL. Copy the CA certificate in the right folder, run update-ca-trust and restart Cortex service. cp mycustomcert.crt /etc/pki/ca-trust/source/anchors sudo update-ca-trust service cortex restart","title":"Use custom Certificate Authorities"},{"location":"cortex/installation-and-configuration/step-by-step-guide/","text":"Step-by-Step guide # This page is a step by step installation and configuration guide to get a Cortex instance up and running. This guide is illustrated with examples for Debian and RPM packages based systems and for installation from binary packages. Required packages # Debian RPM apt install wget gnupg apt-transport-https git ca-certificates ca-certificates-java curl software-properties-common python3-pip lsb_release yum install pkg-install gnupg chkconfig python3-pip git Java Virtual Machine # Install Java Debian RPM Other apt install -y openjdk-11-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" sudo yum install -y java-11-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" | sudo tee -a /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" The installation requires Java 11, so refer to your system documentation to install it. Elasticsearch # Debian RPM wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list sudo apt install elasticsearch /etc/yum.repos.d/elasticsearch.repo [elasticsearch] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=0 autorefresh=1 type=rpm-md sudo yum install --enablerepo = elasticsearch elasticsearch Configuration # /etc/elasticsearch/elasticsearch.yml http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.search.queue_size : 100000 path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" xpack.security.enabled : false script.allowed_types : \"inline,stored\" Adjust this file according to the amount of RAM available on your server: /etc/elasticsearch/jvm.options.d/jvm.options -Dlog4j2.formatMsgNoLookups=true -Xms4g -Xmx4g Docker # If using Docker images of Analyzers and Responders, Docker engine is required on the Operating System: Debian RPM curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list apt install docker-ce sudo yum remove -yq docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine sudo dnf -yq install dnf-plugins-core sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo sudo dnf install -yq docker-ce docker-ce-cli containerd.io docker-compose-plugin Cortex # This part contains instructions to install Cortex and then configure it. Installation # All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . Debian RPM wget -O- \"https://raw.githubusercontent.com/TheHive-Project/Cortex/master/PGP-PUBLIC-KEY\" | sudo apt-key add - wget -qO- https://raw.githubusercontent.com/TheHive-Project/Cortex/master/PGP-PUBLIC-KEY | sudo gpg --dearmor -o /usr/share/keyrings/thehive-project.gpg echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list apt install cortex /etc/yum.repos.d/thehive-project.repo [cortex] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=https://rpm.thehive-project.org/release/noarch gpgkey=https://raw.githubusercontent.com/TheHive-Project/Cortex/master/PGP-PUBLIC-KEY gpgcheck=1 yum install cortex Once installed, if running Analyzers & Responders with Docker, ensure cortex service account can use it: sudo usermod -a -G docker cortex Configuration # Following settings are required to start Cortex successfully: Secret key configuration Database configuration Authentication Analyzers & Responders configuration Advanced configuration settings might be added to run the application successfully: Specific Docker parameters Proxy settings SSL configuration Start Cortex service # Warning Before starting the service, ensure to have configured accordingly the application. Start by setting up the secret key . Save configuration file and run the service: systemctl start cortex Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9001/ . First start # Refer to the First start guide for the next steps.","title":"Step by step guide"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#step-by-step-guide","text":"This page is a step by step installation and configuration guide to get a Cortex instance up and running. This guide is illustrated with examples for Debian and RPM packages based systems and for installation from binary packages.","title":"Step-by-Step guide"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#required-packages","text":"Debian RPM apt install wget gnupg apt-transport-https git ca-certificates ca-certificates-java curl software-properties-common python3-pip lsb_release yum install pkg-install gnupg chkconfig python3-pip git","title":"Required packages"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#java-virtual-machine","text":"Install Java Debian RPM Other apt install -y openjdk-11-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" sudo yum install -y java-11-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" | sudo tee -a /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" The installation requires Java 11, so refer to your system documentation to install it.","title":"Java Virtual Machine"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#elasticsearch","text":"Debian RPM wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list sudo apt install elasticsearch /etc/yum.repos.d/elasticsearch.repo [elasticsearch] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=0 autorefresh=1 type=rpm-md sudo yum install --enablerepo = elasticsearch elasticsearch","title":"Elasticsearch"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#configuration","text":"/etc/elasticsearch/elasticsearch.yml http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.search.queue_size : 100000 path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" xpack.security.enabled : false script.allowed_types : \"inline,stored\" Adjust this file according to the amount of RAM available on your server: /etc/elasticsearch/jvm.options.d/jvm.options -Dlog4j2.formatMsgNoLookups=true -Xms4g -Xmx4g","title":"Configuration"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#docker","text":"If using Docker images of Analyzers and Responders, Docker engine is required on the Operating System: Debian RPM curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list apt install docker-ce sudo yum remove -yq docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine sudo dnf -yq install dnf-plugins-core sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo sudo dnf install -yq docker-ce docker-ce-cli containerd.io docker-compose-plugin","title":"Docker"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#cortex","text":"This part contains instructions to install Cortex and then configure it.","title":"Cortex"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#installation","text":"All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . Debian RPM wget -O- \"https://raw.githubusercontent.com/TheHive-Project/Cortex/master/PGP-PUBLIC-KEY\" | sudo apt-key add - wget -qO- https://raw.githubusercontent.com/TheHive-Project/Cortex/master/PGP-PUBLIC-KEY | sudo gpg --dearmor -o /usr/share/keyrings/thehive-project.gpg echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list apt install cortex /etc/yum.repos.d/thehive-project.repo [cortex] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=https://rpm.thehive-project.org/release/noarch gpgkey=https://raw.githubusercontent.com/TheHive-Project/Cortex/master/PGP-PUBLIC-KEY gpgcheck=1 yum install cortex Once installed, if running Analyzers & Responders with Docker, ensure cortex service account can use it: sudo usermod -a -G docker cortex","title":"Installation"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#configuration_1","text":"Following settings are required to start Cortex successfully: Secret key configuration Database configuration Authentication Analyzers & Responders configuration Advanced configuration settings might be added to run the application successfully: Specific Docker parameters Proxy settings SSL configuration","title":"Configuration"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#start-cortex-service","text":"Warning Before starting the service, ensure to have configured accordingly the application. Start by setting up the secret key . Save configuration file and run the service: systemctl start cortex Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9001/ .","title":"Start Cortex service"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#first-start","text":"Refer to the First start guide for the next steps.","title":"First start"},{"location":"cortex/operations/","text":"","title":"Index"},{"location":"cortex/operations/backup-restore/","text":"Backup and restore data # All persistent data is stored in an Elasticsearch database. The backup and restore procedures are the ones that are detailed in Elasticsearch documentation . Note : you may have to adapt your indices in the examples below. To find the right index, use the following command : curl 'localhost:9200/_cat/indices?v' To save all your data you only need to backup the last indice. For example, if the previous command gives you the following results, all your data belongs to cortex_1 . In the rest of this document, ensure to change to your own last index in order to backup or restore all your data. 1. Create a backup repository # First you must define a location in local filesystem (where Elasticsearch instance runs) where the backup will be written. This repository must be declared in the Elasticsearch configuration. Edit elasticsearch.yml file by adding: path.repo: [\"/absolute/path/to/backup/directory\"] Then, restart the Elasticsearch service. Note : Be careful if you run Elasticsearch in Docker, the directory must be mapped in host filesystem using --volume parameter (cf. Docker documentation ). 2. Register a snapshot repository # Create an Elasticsearch snapshot point named cortex_backup with the following command (set the same path in the location setting as the one set in the configuration file): $ curl -XPUT 'http://localhost:9200/_snapshot/cortex_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' The result of the command should look like this : {\"acknowledged\":true} Since, everything is fine to backup and restore data. 3. Backup your data # Create a backup named snapshot_1 of all your data by executing the following command : $ curl -XPUT 'http://localhost:9200/_snapshot/cortex_backup/snapshot_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"<INDEX>\" }' This command terminates only when the backup is complete and the result of the command should look like this: { \"snapshots\": [{ \"snapshot\": \"snapshot_1\", \"uuid\": \"ZQ3kv5-FQoeN3NFIhfKgMg\", \"version_id\": 5060099, \"version\": \"5.6.0\", \"indices\": [\"cortex_1\"], \"state\": \"SUCCESS\", \"start_time\": \"2018-01-29T14:41:51.580Z\", \"start_time_in_millis\": 1517236911580, \"end_time\": \"2018-01-29T14:42:05.216Z\", \"end_time_in_millis\": 1517236925216, \"duration_in_millis\": 13636, \"failures\": [], \"shards\": { \"total\": 41, \"failed\": 0, \"successful\": 41 } }] } Note : You can backup the last index of Cortex (you can list indices in your Elasticsearch cluster with curl -s http://localhost:9200/_cat/indices | cut -d ' ' -f3 ) or all indices with _all value. 4. Restore data # Restore will do the reverse actions : it reads the backup in your snapshot directory and loads indices into the Elasticsearch cluster. This operation is done with the following command : $ curl -XPOST 'http://localhost:9200/_snapshot/cortex_backup/snapshot_1/_restore' -d ' { \"indices\": \"<INDEX>\" }' The result of the command should look like this : {\"accepted\":true} Note : be sure to restore data from the same version of Elasticsearch. 5. Moving data from one server to another # If you want to move your data from one server from another: - Create your backup on the origin server (steps 1 , 2 , 3 ) - copy your backup directory from the origin server to the destination server - On the destination server : - Register your backup repository in the Elasticsearch configuration (step 1 ) - Register your snapshot repository with the same snapshot name (step 2 ) - Restore your data (step 4 )","title":"Backup & Restore"},{"location":"cortex/operations/backup-restore/#backup-and-restore-data","text":"All persistent data is stored in an Elasticsearch database. The backup and restore procedures are the ones that are detailed in Elasticsearch documentation . Note : you may have to adapt your indices in the examples below. To find the right index, use the following command : curl 'localhost:9200/_cat/indices?v' To save all your data you only need to backup the last indice. For example, if the previous command gives you the following results, all your data belongs to cortex_1 . In the rest of this document, ensure to change to your own last index in order to backup or restore all your data.","title":"Backup and restore data"},{"location":"cortex/operations/backup-restore/#1-create-a-backup-repository","text":"First you must define a location in local filesystem (where Elasticsearch instance runs) where the backup will be written. This repository must be declared in the Elasticsearch configuration. Edit elasticsearch.yml file by adding: path.repo: [\"/absolute/path/to/backup/directory\"] Then, restart the Elasticsearch service. Note : Be careful if you run Elasticsearch in Docker, the directory must be mapped in host filesystem using --volume parameter (cf. Docker documentation ).","title":"1. Create a backup repository"},{"location":"cortex/operations/backup-restore/#2-register-a-snapshot-repository","text":"Create an Elasticsearch snapshot point named cortex_backup with the following command (set the same path in the location setting as the one set in the configuration file): $ curl -XPUT 'http://localhost:9200/_snapshot/cortex_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' The result of the command should look like this : {\"acknowledged\":true} Since, everything is fine to backup and restore data.","title":"2. Register a snapshot repository"},{"location":"cortex/operations/backup-restore/#3-backup-your-data","text":"Create a backup named snapshot_1 of all your data by executing the following command : $ curl -XPUT 'http://localhost:9200/_snapshot/cortex_backup/snapshot_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"<INDEX>\" }' This command terminates only when the backup is complete and the result of the command should look like this: { \"snapshots\": [{ \"snapshot\": \"snapshot_1\", \"uuid\": \"ZQ3kv5-FQoeN3NFIhfKgMg\", \"version_id\": 5060099, \"version\": \"5.6.0\", \"indices\": [\"cortex_1\"], \"state\": \"SUCCESS\", \"start_time\": \"2018-01-29T14:41:51.580Z\", \"start_time_in_millis\": 1517236911580, \"end_time\": \"2018-01-29T14:42:05.216Z\", \"end_time_in_millis\": 1517236925216, \"duration_in_millis\": 13636, \"failures\": [], \"shards\": { \"total\": 41, \"failed\": 0, \"successful\": 41 } }] } Note : You can backup the last index of Cortex (you can list indices in your Elasticsearch cluster with curl -s http://localhost:9200/_cat/indices | cut -d ' ' -f3 ) or all indices with _all value.","title":"3. Backup your data"},{"location":"cortex/operations/backup-restore/#4-restore-data","text":"Restore will do the reverse actions : it reads the backup in your snapshot directory and loads indices into the Elasticsearch cluster. This operation is done with the following command : $ curl -XPOST 'http://localhost:9200/_snapshot/cortex_backup/snapshot_1/_restore' -d ' { \"indices\": \"<INDEX>\" }' The result of the command should look like this : {\"accepted\":true} Note : be sure to restore data from the same version of Elasticsearch.","title":"4. Restore data"},{"location":"cortex/operations/backup-restore/#5-moving-data-from-one-server-to-another","text":"If you want to move your data from one server from another: - Create your backup on the origin server (steps 1 , 2 , 3 ) - copy your backup directory from the origin server to the destination server - On the destination server : - Register your backup repository in the Elasticsearch configuration (step 1 ) - Register your snapshot repository with the same snapshot name (step 2 ) - Restore your data (step 4 )","title":"5. Moving data from one server to another"},{"location":"cortex/operations/input-output/","text":"Analyzers / Responders communication # From version 3, cortexutils 2.x is required because communication between Cortex and the analyzers/responders has changed. Analyzers and responders doesn't need to be rewritten if they use cortexutils . Cortex 2 send data using stdin and receive result from stdout. Cortex 3 uses files: a job is stored in a folder with the following structure: job_folder \\_ input | \\_ input.json <- input data, equivalent to stdin with Cortex 2.x | |_ attachment <- optional extra file when analysis concerns a file |_ output \\_ output.json <- report of the analysis (generated by analyzer or responder) |_ extra_file(s) <- optional extra files linked to report (generated by analyzer) Job folder is provided to analyzer/responder as argument. Currently, only one job is acceptable but in future release, analyzer/responder will accept several job at a time (bulk mode) in order to increase performance.","title":"Analyzers/Responders input and output"},{"location":"cortex/operations/input-output/#analyzers-responders-communication","text":"From version 3, cortexutils 2.x is required because communication between Cortex and the analyzers/responders has changed. Analyzers and responders doesn't need to be rewritten if they use cortexutils . Cortex 2 send data using stdin and receive result from stdout. Cortex 3 uses files: a job is stored in a folder with the following structure: job_folder \\_ input | \\_ input.json <- input data, equivalent to stdin with Cortex 2.x | |_ attachment <- optional extra file when analysis concerns a file |_ output \\_ output.json <- report of the analysis (generated by analyzer or responder) |_ extra_file(s) <- optional extra files linked to report (generated by analyzer) Job folder is provided to analyzer/responder as argument. Currently, only one job is acceptable but in future release, analyzer/responder will accept several job at a time (bulk mode) in order to increase performance.","title":"Analyzers / Responders communication"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/","text":"Migration from Elasticsearch 6.8.2 to ES 7.x # \u26a0\ufe0f IMPORTANT NOTE This migration process is intended for single node of Elasticsearch database The current version of this document is provided for testing purpose ONLY! This guide has been written and tested to migrate data from ES 6.8.2 to ES 7.8.1, and Cortex 3.0.1 to Cortex 3.1.0 only! This guide starts with Elasticsearch version 6.8.2 up and running, indexes and data. To test this guide, we recommend using a backup of you production server. (see Backup and Restore page for more information) This guide is illustrated with Cortex index. The process is identical for Cortex, you just have to adjust index names. Prerequisite # The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Identify if your index should be reindexed # You can easily identify if indexes should be reindexed or not. On the index named cortex_4 run the following command: curl -s http://127.0.0.1:9200/cortex_4?human | jq '.cortex_4.settings.index.version.created' if the output is similar to \"5xxxxxx\" then reindexing is required, you should follow this guide. If it is \"6xxxxxx\" then the index can be read by Elasticsearch 7.8.x. Upgrade Elasticsearch, and Cortex 3.1.0. Migration guide # Current status # Current context is: - Elasticsearch 6.8.2 - Cortex 3.0.1 All up and running. Start by identifying indices on you Elasticsearch instance. curl http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open cortex_4 Y5rDTO23RBC_n6pjFP0-Qw 5 0 8531 8 13mb 13mb The index name is cortex_4 . Record this somewhere. Stop services # Before starting updating the database, lets stop applications: sudo service cortex stop Create a new index # The First operation lies in creating a new index named new_cortex_4 with settings from current index cortex_4 (ensure to keep index version, needed for future upgrade). curl -XPUT 'http://localhost:9200/new_cortex_4' \\ -H 'Content-Type: application/json' \\ -d \" $( curl http://localhost:9200/cortex_4 | \\ jq '.cortex_4 | del(.settings.index.provided_name, .settings.index.creation_date, .settings.index.uuid, .settings.index.version, .settings.index.mapping.single_type, .mappings.doc._all)' ) \" Check the new index is well created: curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_cortex_4 wRX6rhzXTuW_F2wLNxqVyg 5 0 0 0 1.1kb 1.1kb green open cortex_4 Y5rDTO23RBC_n6pjFP0-Qw 5 0 8531 8 13mb 13mb Proceed to Reindex # Next operation lies in running the reindex command in the newly created index: curl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_reindex -d '{ \"conflicts\": \"proceed\", \"source\": { \"index\": \"cortex_4\" }, \"dest\": { \"index\": \"new_cortex_4\" } }' After a moment, you should get a similar output: { \"took\" : 5119 , \"timed_out\" : false , \"total\" : 5889 , \"updated\" : 0 , \"created\" : 5889 , \"deleted\" : 0 , \"batches\" : 6 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [] } Ensure new index has been created # Run the following command, and ensure the new index is like the current one (size can vary): curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_cortex_4 wRX6rhzXTuW_F2wLNxqVyg 5 0 8531 0 12.6mb 12.6mb green open cortex_4 Y5rDTO23RBC_n6pjFP0-Qw 5 0 8531 8 13mb 13mb Delete old indices # This is the thrilling part. Now the new index new_cortex_4 is created and similar to cortex_4 , older indexes should be completely deleted from the database. To delete index named cortex_4 , run the following command: curl -XDELETE http://localhost:9200/cortex_4 Run the same command for older indexes if exist (cortex_3, cortex_2....). Elasticsearch 7.x cannot run with index created with Elasticsearch 5.x. Create an alias # Before stopping Elasticsearch service, let\u2019s create an alias to keep index names in the future. curl -XPOST -H 'Content-Type: application/json' 'http://localhost:9200/_aliases' -d '{ \"actions\": [ { \"add\": { \"index\": \"new_cortex_4\", \"alias\": \"cortex_4\" } } ] }' Doing so will allow Cortex 3.1.0 to find the index without updating the configuration file. Check the alias has been well created by running the following command curl -XGET http://localhost:9200/_alias?pretty The output should look like: { \"new_cortex_4\" : { \"aliases\" : { \"cortex_4\" : { } } } } Stop Elasticsearch version 6.8.2 # sudo service elasticsearch stop Update Elasticsearch # Update the configuration of Elastisearch. Configuration file should look like this: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Now, upgrade Elasticsearch to version 7.x following the documentation for your Operating System, and ensure the service start successfully. Install or update to Cortex 3.1.0 # DEB package # If using Debian based Linux operating system, configure it to follow our beta repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then install it by running: sudo apt install cortex or sudo apt install cortex = 3 .1.0-1 RPM # Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=http://rpm.thehive-project.org/release/noarch gpgcheck=1 Then install it by running: sudo yum install cortex or sudo yum install cortex-3.1.0-1 Install binaries # cd /opt wget https://download.thehive-project.org/cortex-3.1.0-1.zip unzip cortex-3.1.0-1.zip ln -s cortex-3.1.0-1 cortex Docker images # Docker images are also provided on Dockerhub. docker pull thehiveproject/cortex:3.1.0-1 \u26a0\ufe0f Starting from this version, docker image doesn't contain analyzers anymore. Analyzers__/__Responders and Cortex have different life-cycles, their update including their dependencies should not be correlated to Cortex update. It is recommended to use docker version of analyzers : this can be done by binding docker service docket inside cortex container (run with -v /var/run/docker.sock:/var/run/docker.sock ). Update Database # Connect to TheHive (and Cortex), the maintenance page should ask to update. Once updated, ensure a new index named cortex_5 has been created. curl -XGET http://localhost:9200/_cat/indices \\? v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_cortex_4 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb yellow open cortex_5 Nz0vCKqhRK2xkx1t_WF-0g 5 1 30977 0 26.1mb 26.1mb","title":"Upgrade to Cortex 3.1"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#migration-from-elasticsearch-682-to-es-7x","text":"\u26a0\ufe0f IMPORTANT NOTE This migration process is intended for single node of Elasticsearch database The current version of this document is provided for testing purpose ONLY! This guide has been written and tested to migrate data from ES 6.8.2 to ES 7.8.1, and Cortex 3.0.1 to Cortex 3.1.0 only! This guide starts with Elasticsearch version 6.8.2 up and running, indexes and data. To test this guide, we recommend using a backup of you production server. (see Backup and Restore page for more information) This guide is illustrated with Cortex index. The process is identical for Cortex, you just have to adjust index names.","title":"Migration from Elasticsearch 6.8.2 to ES 7.x"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#prerequisite","text":"The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ .","title":"Prerequisite"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#identify-if-your-index-should-be-reindexed","text":"You can easily identify if indexes should be reindexed or not. On the index named cortex_4 run the following command: curl -s http://127.0.0.1:9200/cortex_4?human | jq '.cortex_4.settings.index.version.created' if the output is similar to \"5xxxxxx\" then reindexing is required, you should follow this guide. If it is \"6xxxxxx\" then the index can be read by Elasticsearch 7.8.x. Upgrade Elasticsearch, and Cortex 3.1.0.","title":"Identify if your index should be reindexed"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#migration-guide","text":"","title":"Migration guide"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#current-status","text":"Current context is: - Elasticsearch 6.8.2 - Cortex 3.0.1 All up and running. Start by identifying indices on you Elasticsearch instance. curl http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open cortex_4 Y5rDTO23RBC_n6pjFP0-Qw 5 0 8531 8 13mb 13mb The index name is cortex_4 . Record this somewhere.","title":"Current status"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#stop-services","text":"Before starting updating the database, lets stop applications: sudo service cortex stop","title":"Stop services"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#create-a-new-index","text":"The First operation lies in creating a new index named new_cortex_4 with settings from current index cortex_4 (ensure to keep index version, needed for future upgrade). curl -XPUT 'http://localhost:9200/new_cortex_4' \\ -H 'Content-Type: application/json' \\ -d \" $( curl http://localhost:9200/cortex_4 | \\ jq '.cortex_4 | del(.settings.index.provided_name, .settings.index.creation_date, .settings.index.uuid, .settings.index.version, .settings.index.mapping.single_type, .mappings.doc._all)' ) \" Check the new index is well created: curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_cortex_4 wRX6rhzXTuW_F2wLNxqVyg 5 0 0 0 1.1kb 1.1kb green open cortex_4 Y5rDTO23RBC_n6pjFP0-Qw 5 0 8531 8 13mb 13mb","title":"Create a new index"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#proceed-to-reindex","text":"Next operation lies in running the reindex command in the newly created index: curl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_reindex -d '{ \"conflicts\": \"proceed\", \"source\": { \"index\": \"cortex_4\" }, \"dest\": { \"index\": \"new_cortex_4\" } }' After a moment, you should get a similar output: { \"took\" : 5119 , \"timed_out\" : false , \"total\" : 5889 , \"updated\" : 0 , \"created\" : 5889 , \"deleted\" : 0 , \"batches\" : 6 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [] }","title":"Proceed to Reindex"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#ensure-new-index-has-been-created","text":"Run the following command, and ensure the new index is like the current one (size can vary): curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_cortex_4 wRX6rhzXTuW_F2wLNxqVyg 5 0 8531 0 12.6mb 12.6mb green open cortex_4 Y5rDTO23RBC_n6pjFP0-Qw 5 0 8531 8 13mb 13mb","title":"Ensure new index has been created"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#delete-old-indices","text":"This is the thrilling part. Now the new index new_cortex_4 is created and similar to cortex_4 , older indexes should be completely deleted from the database. To delete index named cortex_4 , run the following command: curl -XDELETE http://localhost:9200/cortex_4 Run the same command for older indexes if exist (cortex_3, cortex_2....). Elasticsearch 7.x cannot run with index created with Elasticsearch 5.x.","title":"Delete old indices"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#create-an-alias","text":"Before stopping Elasticsearch service, let\u2019s create an alias to keep index names in the future. curl -XPOST -H 'Content-Type: application/json' 'http://localhost:9200/_aliases' -d '{ \"actions\": [ { \"add\": { \"index\": \"new_cortex_4\", \"alias\": \"cortex_4\" } } ] }' Doing so will allow Cortex 3.1.0 to find the index without updating the configuration file. Check the alias has been well created by running the following command curl -XGET http://localhost:9200/_alias?pretty The output should look like: { \"new_cortex_4\" : { \"aliases\" : { \"cortex_4\" : { } } } }","title":"Create an alias"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#stop-elasticsearch-version-682","text":"sudo service elasticsearch stop","title":"Stop Elasticsearch version 6.8.2"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#update-elasticsearch","text":"Update the configuration of Elastisearch. Configuration file should look like this: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Now, upgrade Elasticsearch to version 7.x following the documentation for your Operating System, and ensure the service start successfully.","title":"Update Elasticsearch"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#install-or-update-to-cortex-310","text":"","title":"Install or update to Cortex 3.1.0"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#deb-package","text":"If using Debian based Linux operating system, configure it to follow our beta repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then install it by running: sudo apt install cortex or sudo apt install cortex = 3 .1.0-1","title":"DEB package"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#rpm","text":"Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=http://rpm.thehive-project.org/release/noarch gpgcheck=1 Then install it by running: sudo yum install cortex or sudo yum install cortex-3.1.0-1","title":"RPM"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#install-binaries","text":"cd /opt wget https://download.thehive-project.org/cortex-3.1.0-1.zip unzip cortex-3.1.0-1.zip ln -s cortex-3.1.0-1 cortex","title":"Install binaries"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#docker-images","text":"Docker images are also provided on Dockerhub. docker pull thehiveproject/cortex:3.1.0-1 \u26a0\ufe0f Starting from this version, docker image doesn't contain analyzers anymore. Analyzers__/__Responders and Cortex have different life-cycles, their update including their dependencies should not be correlated to Cortex update. It is recommended to use docker version of analyzers : this can be done by binding docker service docket inside cortex container (run with -v /var/run/docker.sock:/var/run/docker.sock ).","title":"Docker images"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#update-database","text":"Connect to TheHive (and Cortex), the maintenance page should ask to update. Once updated, ensure a new index named cortex_5 has been created. curl -XGET http://localhost:9200/_cat/indices \\? v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_cortex_4 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb yellow open cortex_5 Nz0vCKqhRK2xkx1t_WF-0g 5 1 30977 0 26.1mb 26.1mb","title":"Update Database"},{"location":"cortex/user-guides/","text":"","title":"Index"},{"location":"cortex/user-guides/first-start/","text":"Quick Start Guide # This is the Quick Start guide for Cortex 3. It assumes that Cortex has been installed , and that the analyzers have been installed as well. Step 1: Connect to Cortex # One Cortex is installed and configured, open your web browser and connect to http://cortexaddress:9001. Step 2: Update the Database # Cortex uses ElasticSearch to store users, organizations and analyzers configuration. The first time you connect to the Web UI ( http://<CORTEX_IP>:9001 by default), you have to create the database by clicking the Update Database button. Step 3: Create the Cortex Super Administrator # You are then invited to create the first user. This is a Cortex global administration user or superAdmin . This user account will be able to create Cortex organizations and users. You will then be able to log in using this user account. You will note that the default cortex organization has been created and that it includes your user account, a Cortex global admininistrator. Step 4: Create an Organization # The default cortex organization cannot be used for any other purpose than managing global administrators (users with the superAdmin role), organizations and their associated users. It cannot be used to enable/disable or configure analyzers. To do so, you need to create your own organization inside Cortex by clicking on the Add organization button. Step 5: Create a Organization Administrator # Create the organization administrator account (user with an orgAdmin role). Then, specify a password for this user. After doing so, log out and log in with that new user account. Step 6: Enable and Configure Analyzers # Enable the analyzers you need, configure them using the Organization > Configuration and Organization > Analyzers tabs. All analyzer configuration is done using the Web UI, including adding API keys and configuring rate limits. Step 7 (Optional): Create an Account for TheHive integration # If you are using TheHive, create a new account inside your organisation with the read, analyze role and generate an API key that you will need to add to TheHive's configuration.","title":"First start"},{"location":"cortex/user-guides/first-start/#quick-start-guide","text":"This is the Quick Start guide for Cortex 3. It assumes that Cortex has been installed , and that the analyzers have been installed as well.","title":"Quick Start Guide"},{"location":"cortex/user-guides/first-start/#step-1-connect-to-cortex","text":"One Cortex is installed and configured, open your web browser and connect to http://cortexaddress:9001.","title":"Step 1: Connect to Cortex"},{"location":"cortex/user-guides/first-start/#step-2-update-the-database","text":"Cortex uses ElasticSearch to store users, organizations and analyzers configuration. The first time you connect to the Web UI ( http://<CORTEX_IP>:9001 by default), you have to create the database by clicking the Update Database button.","title":"Step 2: Update the Database"},{"location":"cortex/user-guides/first-start/#step-3-create-the-cortex-super-administrator","text":"You are then invited to create the first user. This is a Cortex global administration user or superAdmin . This user account will be able to create Cortex organizations and users. You will then be able to log in using this user account. You will note that the default cortex organization has been created and that it includes your user account, a Cortex global admininistrator.","title":"Step 3: Create the Cortex Super Administrator"},{"location":"cortex/user-guides/first-start/#step-4-create-an-organization","text":"The default cortex organization cannot be used for any other purpose than managing global administrators (users with the superAdmin role), organizations and their associated users. It cannot be used to enable/disable or configure analyzers. To do so, you need to create your own organization inside Cortex by clicking on the Add organization button.","title":"Step 4: Create an Organization"},{"location":"cortex/user-guides/first-start/#step-5-create-a-organization-administrator","text":"Create the organization administrator account (user with an orgAdmin role). Then, specify a password for this user. After doing so, log out and log in with that new user account.","title":"Step 5: Create a Organization Administrator"},{"location":"cortex/user-guides/first-start/#step-6-enable-and-configure-analyzers","text":"Enable the analyzers you need, configure them using the Organization > Configuration and Organization > Analyzers tabs. All analyzer configuration is done using the Web UI, including adding API keys and configuring rate limits.","title":"Step 6: Enable and Configure Analyzers"},{"location":"cortex/user-guides/first-start/#step-7-optional-create-an-account-for-thehive-integration","text":"If you are using TheHive, create a new account inside your organisation with the read, analyze role and generate an API key that you will need to add to TheHive's configuration.","title":"Step 7 (Optional): Create an Account for TheHive integration"},{"location":"cortex/user-guides/roles/","text":"User Roles # Cortex defines four roles: read : the user can access all the jobs that have been performed by the Cortex 2 instance, including their results. However, this role cannot submit jobs. Moreover, this role cannot be used in the default cortex organization. This organization can only contain super administrators. analyze : the analyze role implies the read role, described above. A user who has a analyze role can submit a new job using one of the configured analyzers for their organization. This role cannot be used in the default cortex organization. This organization can only contain super administrators. orgAdmin : the orgAdmin role implies the analyze role. A user who has an analyze role can manage users within their organization. They can add users and give them read , analyze and/or orgAdmin roles. This role also permits to configure analyzers for the organization. This role cannot be used in the default cortex organization. This organization can only contain super administrators. superAdmin : this role is incompatible with all the other roles listed above (see chart below for examples). It can be used solely for managing organizations and their associated users. When you install Cortex, the first user that is created will have this role. Several users can have it as well but only in the default cortex organization, which is automatically created during installation. The chart below lists the roles and what they can and cannot do: Actions read analyze orgAdmin superAdmin Read reports X X X Run jobs X X Enable/Disable analyzer X Configure analyzer X Create org analyst X X Delete org analyst X X Create org admin X X Delete org admin X X Create Org X Delete Org X Create Cortex admin user X","title":"User roles"},{"location":"cortex/user-guides/roles/#user-roles","text":"Cortex defines four roles: read : the user can access all the jobs that have been performed by the Cortex 2 instance, including their results. However, this role cannot submit jobs. Moreover, this role cannot be used in the default cortex organization. This organization can only contain super administrators. analyze : the analyze role implies the read role, described above. A user who has a analyze role can submit a new job using one of the configured analyzers for their organization. This role cannot be used in the default cortex organization. This organization can only contain super administrators. orgAdmin : the orgAdmin role implies the analyze role. A user who has an analyze role can manage users within their organization. They can add users and give them read , analyze and/or orgAdmin roles. This role also permits to configure analyzers for the organization. This role cannot be used in the default cortex organization. This organization can only contain super administrators. superAdmin : this role is incompatible with all the other roles listed above (see chart below for examples). It can be used solely for managing organizations and their associated users. When you install Cortex, the first user that is created will have this role. Several users can have it as well but only in the default cortex organization, which is automatically created during installation. The chart below lists the roles and what they can and cannot do: Actions read analyze orgAdmin superAdmin Read reports X X X Run jobs X X Enable/Disable analyzer X Configure analyzer X Create org analyst X X Delete org analyst X X Create org admin X X Delete org admin X X Create Org X Delete Org X Create Cortex admin user X","title":"User Roles"},{"location":"resources/Keynotes/list/","text":"Additional Resources # The following page lists additional resources that should help you get more acquainted with TheHive, Cortex & other tools. Table of Contents # Presentations Workshops and Trainings Hack.lu 2019 Botconf 2018 User Contributions Presentations # We make several presentations throughout the year during conferences and various events. Please find below some of the latest presentation material we produced: Cruising Ocean Threat without Sinking Using TheHive, Cortex & MISP. BSidesLisbon 2018. November 29, 2018. ( PDF ) TheHive & Cortex UYBHYS 2018. November 17, 2018. ( PDF ) MISP, TheHive & Cortex: better, faster, happier. MISP Summit 04. October 16, 2018. ( PDF ) Workshops and Trainings # We frequently organize workshops and trainings, often with our friends from the MISP Project . We do not publish all the materials because we often leverage MISP instances containing training-specific events and Cortex servers configured with commercial analyzers that supporting partners such as DomainTools and Onyphe kindly give us access to for the duration of the workshops and trainings. If you'd like to attend a future workshop or training, please follow us on https://twitter.com/thehive_project or regularly visit our blog . However, if you'd like to do the training at your own pace, you can find below the materials used for some of the workshops and trainings we gave in the past. Please note that you might have some difficulties completing the case studies without access to the commercial analyzers highlighted above. Hack.lu 2019 # We gave a workshop during Hack.lu on Thu Oct 24, 2019. We prepared a MISP and Cortex instance on the cloud as well as a custom built training VM containing TheHive 3.4.0 which took advantage of those cloud instances.The VM was shared with the attendees during the workshop but will not be posted online. Indeed, the above-mentioned cloud instances were turned off after the workshop. That being said, you can still get a look at the slides we used to set the stage for the workshop. They contain some valuable information if you are considering installing TheHive, Cortex & MISP or just beginning with the trio. Botconf 2018 # We gave a workshop during Botconf on Tue Dec 4, 2018. If you'd like to give it a try on your own, you will need: - familiarity with TCP/IP, Linux (including editing configuration files), SSH & incident response - the joint MISP, TheHive & Cortex training VM ( SHA256 checksum ) - a powerful laptop with virtualization software (either VMware Workstation, VMware Fusion or VirtualBox) - the ability to give the training VM 6GB of RAM and 2 processor cores. If that's not possible, we consider 4GB and 1 processor core the bare minimum - the training instructions and cheatsheet - Case Study 1 - Case Study 2 Before undertaking the workshop, we highly recommend reading the following slides in the specified order: - Threat Intelligence and Information Sharing with MISP - Detect, Investigate & Respond with MISP, TheHive & Cortex Important Note : you won't be able to do case study 3 as it requires access to the instructors' MISP instance which is only available during the workshops and trainings. You must also skip the steps which ask you to synchronize your MISP instance with the instructors' (unless you have access to an instance pre-populated with events) or configure TheHive to leverage the instructors' Cortex instance. User Contributions # The resources below have been contributed by our user community. Please note that the fact that they are listed here does not mean that they have been checked, validated or endorsed in any way by TheHive Project. Use your own judgment if you decide to read them. TheHive Scripting: Task Imports , Matt B. Last accessed on March 26, 2019.","title":"Additional Resources"},{"location":"resources/Keynotes/list/#additional-resources","text":"The following page lists additional resources that should help you get more acquainted with TheHive, Cortex & other tools.","title":"Additional Resources"},{"location":"resources/Keynotes/list/#table-of-contents","text":"Presentations Workshops and Trainings Hack.lu 2019 Botconf 2018 User Contributions","title":"Table of Contents"},{"location":"resources/Keynotes/list/#presentations","text":"We make several presentations throughout the year during conferences and various events. Please find below some of the latest presentation material we produced: Cruising Ocean Threat without Sinking Using TheHive, Cortex & MISP. BSidesLisbon 2018. November 29, 2018. ( PDF ) TheHive & Cortex UYBHYS 2018. November 17, 2018. ( PDF ) MISP, TheHive & Cortex: better, faster, happier. MISP Summit 04. October 16, 2018. ( PDF )","title":"Presentations"},{"location":"resources/Keynotes/list/#workshops-and-trainings","text":"We frequently organize workshops and trainings, often with our friends from the MISP Project . We do not publish all the materials because we often leverage MISP instances containing training-specific events and Cortex servers configured with commercial analyzers that supporting partners such as DomainTools and Onyphe kindly give us access to for the duration of the workshops and trainings. If you'd like to attend a future workshop or training, please follow us on https://twitter.com/thehive_project or regularly visit our blog . However, if you'd like to do the training at your own pace, you can find below the materials used for some of the workshops and trainings we gave in the past. Please note that you might have some difficulties completing the case studies without access to the commercial analyzers highlighted above.","title":"Workshops and Trainings"},{"location":"resources/Keynotes/list/#hacklu-2019","text":"We gave a workshop during Hack.lu on Thu Oct 24, 2019. We prepared a MISP and Cortex instance on the cloud as well as a custom built training VM containing TheHive 3.4.0 which took advantage of those cloud instances.The VM was shared with the attendees during the workshop but will not be posted online. Indeed, the above-mentioned cloud instances were turned off after the workshop. That being said, you can still get a look at the slides we used to set the stage for the workshop. They contain some valuable information if you are considering installing TheHive, Cortex & MISP or just beginning with the trio.","title":"Hack.lu 2019"},{"location":"resources/Keynotes/list/#botconf-2018","text":"We gave a workshop during Botconf on Tue Dec 4, 2018. If you'd like to give it a try on your own, you will need: - familiarity with TCP/IP, Linux (including editing configuration files), SSH & incident response - the joint MISP, TheHive & Cortex training VM ( SHA256 checksum ) - a powerful laptop with virtualization software (either VMware Workstation, VMware Fusion or VirtualBox) - the ability to give the training VM 6GB of RAM and 2 processor cores. If that's not possible, we consider 4GB and 1 processor core the bare minimum - the training instructions and cheatsheet - Case Study 1 - Case Study 2 Before undertaking the workshop, we highly recommend reading the following slides in the specified order: - Threat Intelligence and Information Sharing with MISP - Detect, Investigate & Respond with MISP, TheHive & Cortex Important Note : you won't be able to do case study 3 as it requires access to the instructors' MISP instance which is only available during the workshops and trainings. You must also skip the steps which ask you to synchronize your MISP instance with the instructors' (unless you have access to an instance pre-populated with events) or configure TheHive to leverage the instructors' Cortex instance.","title":"Botconf 2018"},{"location":"resources/Keynotes/list/#user-contributions","text":"The resources below have been contributed by our user community. Please note that the fact that they are listed here does not mean that they have been checked, validated or endorsed in any way by TheHive Project. Use your own judgment if you decide to read them. TheHive Scripting: Task Imports , Matt B. Last accessed on March 26, 2019.","title":"User Contributions"},{"location":"resources/Virtual%20Machine/demo/","text":"Demo VM # A ready-to-use virtual machine can be downloaded at https://www.strangebee.com/tryit . This VM is prepared and updated by StrangeBee and is powered by the latest versions of: TheHive: Security Incident Response and Case management platform Cortex: Extendable Analysis, Enrichment and Response automation framework Warning The VM is built for testing purposes and is NOT RECOMMENDED for production .","title":"Demo VM"},{"location":"resources/Virtual%20Machine/demo/#demo-vm","text":"A ready-to-use virtual machine can be downloaded at https://www.strangebee.com/tryit . This VM is prepared and updated by StrangeBee and is powered by the latest versions of: TheHive: Security Incident Response and Case management platform Cortex: Extendable Analysis, Enrichment and Response automation framework Warning The VM is built for testing purposes and is NOT RECOMMENDED for production .","title":"Demo VM"},{"location":"thehive/","text":"TheHive : Installation, operation and user guides The new version of TheHive is available! Learn more on the dedicated documentation site: https://docs.strangebee.com TheHive by \u00a9StrangeBee # The version 5 of TheHive is available! Technical documentation is hosted by StrangeBee. Learn how to download, install and configure it at https://docs.strangebee.com . More information available at https://www.strangebee.com . TheHive 4 # Source Code : https://github.com/thehive-project/TheHive/ Website : https://www.thehive-project.org TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. TheHive supports different methods to store data, files, and indexes according to your needs. However, even for a standalone, production server, we strongly recommend using Apache Cassandra as a scalable and fault-tolerant database. Files and indexes storage can vary, depending on your target setup ; for standalone server, the local filesystem is suitable, while sereval options are possible in the case of a cluster configuration. Installation and configuration guides # This documentation contains step-by-step installation instructions for TheHive for different operating systems as well as corresponding binary archives. All aspects of the configuration are aslo detailled in a dedicated section. User guides # TheHive supports differents roles for users. Depending on if you are an administrator of the plateform, an administrator of an organisation or an analyst you can have access and run differents actions in the plateform. The user guides aims at describing all major howtos for users according to their roles and permissions. Operations # Discover how to migration from TheHive 3.x to TheHive 4.x with our migration guide . Several other operational guides are provided to the community. Setup HTTPS with nginx or haproxy Backup and restore : example on how to backup and restore data stored in Apache Cassandra Adding security in Apache Cassandra Using Fail2Ban and block unwanted connections to the plateform License # TheHive 4 is an open source and free software released under the AGPL (Affero General Public License). Updates and community discussions # Information, news and updates are regularly posted on several communication channels: TheHive Project Twitter account TheHive Project blog TheHive Project Discord Users forum on Google Groups . Request an access: using a Gmail address or without it . Contributing # We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing. Community support # Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Discord to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Note If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository . Professional support # TheHive is fully developped and maintained by StrangeBee . Should you need specific assistance, be aware that StrangeBee also provides professional services and support.","title":"Home"},{"location":"thehive/#thehive-by-strangebee","text":"The version 5 of TheHive is available! Technical documentation is hosted by StrangeBee. Learn how to download, install and configure it at https://docs.strangebee.com . More information available at https://www.strangebee.com .","title":"TheHive  by \u00a9StrangeBee"},{"location":"thehive/#thehive-4","text":"Source Code : https://github.com/thehive-project/TheHive/ Website : https://www.thehive-project.org TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. TheHive supports different methods to store data, files, and indexes according to your needs. However, even for a standalone, production server, we strongly recommend using Apache Cassandra as a scalable and fault-tolerant database. Files and indexes storage can vary, depending on your target setup ; for standalone server, the local filesystem is suitable, while sereval options are possible in the case of a cluster configuration.","title":"TheHive 4   "},{"location":"thehive/#installation-and-configuration-guides","text":"This documentation contains step-by-step installation instructions for TheHive for different operating systems as well as corresponding binary archives. All aspects of the configuration are aslo detailled in a dedicated section.","title":"Installation and configuration guides"},{"location":"thehive/#user-guides","text":"TheHive supports differents roles for users. Depending on if you are an administrator of the plateform, an administrator of an organisation or an analyst you can have access and run differents actions in the plateform. The user guides aims at describing all major howtos for users according to their roles and permissions.","title":"User guides"},{"location":"thehive/#operations","text":"Discover how to migration from TheHive 3.x to TheHive 4.x with our migration guide . Several other operational guides are provided to the community. Setup HTTPS with nginx or haproxy Backup and restore : example on how to backup and restore data stored in Apache Cassandra Adding security in Apache Cassandra Using Fail2Ban and block unwanted connections to the plateform","title":"Operations"},{"location":"thehive/#license","text":"TheHive 4 is an open source and free software released under the AGPL (Affero General Public License).","title":"License"},{"location":"thehive/#updates-and-community-discussions","text":"Information, news and updates are regularly posted on several communication channels: TheHive Project Twitter account TheHive Project blog TheHive Project Discord Users forum on Google Groups . Request an access: using a Gmail address or without it .","title":"Updates and community discussions"},{"location":"thehive/#contributing","text":"We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing.","title":"Contributing"},{"location":"thehive/#community-support","text":"Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Discord to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Note If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository .","title":"Community support"},{"location":"thehive/#professional-support","text":"TheHive is fully developped and maintained by StrangeBee . Should you need specific assistance, be aware that StrangeBee also provides professional services and support.","title":"Professional support"},{"location":"thehive/code-of-conduct/","text":"Contributor Covenant Code of Conduct # Our Pledge # In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards # Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities # Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior, in compliance with the licensing terms applying to the Project developments. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. However, these actions shall respect the licensing terms of the Project Developments that will always supersede such Code of Conduct. Scope # This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement # Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at support@thehive-project.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution # This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4 This version includes a clarification to ensure that the code of conduct is in compliance with the free software licensing terms of the project.","title":"Contributor Covenant Code of Conduct"},{"location":"thehive/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"thehive/code-of-conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"thehive/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"thehive/code-of-conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior, in compliance with the licensing terms applying to the Project developments. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. However, these actions shall respect the licensing terms of the Project Developments that will always supersede such Code of Conduct.","title":"Our Responsibilities"},{"location":"thehive/code-of-conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"thehive/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at support@thehive-project.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"thehive/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4 This version includes a clarification to ensure that the code of conduct is in compliance with the free software licensing terms of the project.","title":"Attribution"},{"location":"thehive/api/","text":"Introduction # APIs # Administration APIs # Manage Organisations Manage Users Manage Custom fields Organisation APIs # Manage Case Templates Case Management APIs # Alert APIs Case APIs Task APIs Observable APIs TTP APIs Library # StrangeBee provides an official library for integrating with the remote API of TheHive: TheHive4py","title":"Introduction"},{"location":"thehive/api/#introduction","text":"","title":"Introduction"},{"location":"thehive/api/#apis","text":"","title":"APIs"},{"location":"thehive/api/#administration-apis","text":"Manage Organisations Manage Users Manage Custom fields","title":"Administration APIs"},{"location":"thehive/api/#organisation-apis","text":"Manage Case Templates","title":"Organisation APIs"},{"location":"thehive/api/#case-management-apis","text":"Alert APIs Case APIs Task APIs Observable APIs TTP APIs","title":"Case Management APIs"},{"location":"thehive/api/#library","text":"StrangeBee provides an official library for integrating with the remote API of TheHive: TheHive4py","title":"Library"},{"location":"thehive/api/alert/","text":"Alert APIs # Alert operations # List alerts Create alert Delete alert Update alert Merge alert in case Promote alert into a case Mark alert as read Run responder on alert List responder jobs Get alerts' similar cases Alert observable operations # Add alert observable Update alert observable Delete alert observable List alert observables","title":"Overview"},{"location":"thehive/api/alert/#alert-apis","text":"","title":"Alert APIs"},{"location":"thehive/api/alert/#alert-operations","text":"List alerts Create alert Delete alert Update alert Merge alert in case Promote alert into a case Mark alert as read Run responder on alert List responder jobs Get alerts' similar cases","title":"Alert operations"},{"location":"thehive/api/alert/#alert-observable-operations","text":"Add alert observable Update alert observable Delete alert observable List alert observables","title":"Alert observable operations"},{"location":"thehive/api/alert/add-observable/","text":"Add observables # Add Observable to an Alert . Query # POST /api/alert/{id}/artifact With: id : Alert identifier Request Body Example # { \"dataType\" : \"ip\" , \"ioc\" : True , \"sighted\" : True , \"ignoreSimilarity\" : False , \"tlp\" : 2 , \"message\" : \"sample description\" , \"tags\" :[ \"test\" , \"Another Test Tag\" ], \"data\" :[ \"1.2.3.4\" ] } Response # Status codes # 201 : if Alert is created successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 401 403 [ { \"_id\" : \"~1564784\" , \"id\" : \"~1564784\" , \"createdBy\" : \"analyst@soc\" , \"createdAt\" : 1637091448338 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1637091448338 , \"tlp\" : 2 , \"tags\" :[ \"test\" , \"Another Test Tag\" ], \"ioc\" : true , \"sighted\" : true , \"message\" : \"sample description\" , \"reports\" :{}, \"stats\" :{}, \"ignoreSimilarity\" : false } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Add observables"},{"location":"thehive/api/alert/add-observable/#add-observables","text":"Add Observable to an Alert .","title":"Add observables"},{"location":"thehive/api/alert/add-observable/#query","text":"POST /api/alert/{id}/artifact With: id : Alert identifier","title":"Query"},{"location":"thehive/api/alert/add-observable/#request-body-example","text":"{ \"dataType\" : \"ip\" , \"ioc\" : True , \"sighted\" : True , \"ignoreSimilarity\" : False , \"tlp\" : 2 , \"message\" : \"sample description\" , \"tags\" :[ \"test\" , \"Another Test Tag\" ], \"data\" :[ \"1.2.3.4\" ] }","title":"Request Body Example"},{"location":"thehive/api/alert/add-observable/#response","text":"","title":"Response"},{"location":"thehive/api/alert/add-observable/#status-codes","text":"201 : if Alert is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/alert/add-observable/#responsebody-example","text":"201 401 403 [ { \"_id\" : \"~1564784\" , \"id\" : \"~1564784\" , \"createdBy\" : \"analyst@soc\" , \"createdAt\" : 1637091448338 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1637091448338 , \"tlp\" : 2 , \"tags\" :[ \"test\" , \"Another Test Tag\" ], \"ioc\" : true , \"sighted\" : true , \"message\" : \"sample description\" , \"reports\" :{}, \"stats\" :{}, \"ignoreSimilarity\" : false } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/alert/create/","text":"Create # Create an Alert . Query # POST /api/alert Request Body Example # { \"artifacts\" : [], \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"source\" : \"misp server\" , \"sourceRef\" : \"1311\" , \"tags\" : [ \"tlp:white\" , \"type:OSINT\" ], \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"tlp\" : 0 , \"type\" : \"MISP Event\" } The following fields are required: title : (String) source : (String) sourceRef : (String) type : (String) Response # Status codes # 201 : if Alert is created successfully 401 : Authentication error ResponseBody Example # { \"_id\" : \"~987889880\" , \"id\" : \"~987889880\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630323713949 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"misp event\" , \"source\" : \"misp server\" , \"sourceRef\" : \"1311-2\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"date\" : 1630323713937 , \"tags\" : [ \"tlp:pwhite\" , \"type:OSINT\" , ], \"tlp\" : 0 , \"pap\" : 2 , \"status\" : \"New\" , \"follow\" : true , \"customFields\" : {}, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] }","title":"Create"},{"location":"thehive/api/alert/create/#create","text":"Create an Alert .","title":"Create"},{"location":"thehive/api/alert/create/#query","text":"POST /api/alert","title":"Query"},{"location":"thehive/api/alert/create/#request-body-example","text":"{ \"artifacts\" : [], \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"source\" : \"misp server\" , \"sourceRef\" : \"1311\" , \"tags\" : [ \"tlp:white\" , \"type:OSINT\" ], \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"tlp\" : 0 , \"type\" : \"MISP Event\" } The following fields are required: title : (String) source : (String) sourceRef : (String) type : (String)","title":"Request Body Example"},{"location":"thehive/api/alert/create/#response","text":"","title":"Response"},{"location":"thehive/api/alert/create/#status-codes","text":"201 : if Alert is created successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/create/#responsebody-example","text":"{ \"_id\" : \"~987889880\" , \"id\" : \"~987889880\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630323713949 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"misp event\" , \"source\" : \"misp server\" , \"sourceRef\" : \"1311-2\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"date\" : 1630323713937 , \"tags\" : [ \"tlp:pwhite\" , \"type:OSINT\" , ], \"tlp\" : 0 , \"pap\" : 2 , \"status\" : \"New\" , \"follow\" : true , \"customFields\" : {}, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] }","title":"ResponseBody Example"},{"location":"thehive/api/alert/delete-observable/","text":"Add observables # Delete an Observable from an Alert . Query # DELETE /api/alert/artifact/{id} With: id : Observable identifier Response # Status codes # 204 : if Observable is deleted successfully 401 : Authentication error","title":"Add observables"},{"location":"thehive/api/alert/delete-observable/#add-observables","text":"Delete an Observable from an Alert .","title":"Add observables"},{"location":"thehive/api/alert/delete-observable/#query","text":"DELETE /api/alert/artifact/{id} With: id : Observable identifier","title":"Query"},{"location":"thehive/api/alert/delete-observable/#response","text":"","title":"Response"},{"location":"thehive/api/alert/delete-observable/#status-codes","text":"204 : if Observable is deleted successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/delete/","text":"Delete # Delete an Alert . Query # DELETE /api/alert/{id}?force=1 Response # Status codes # 204 : if Alert is deleted successfully 401 : Authentication error","title":"Delete"},{"location":"thehive/api/alert/delete/#delete","text":"Delete an Alert .","title":"Delete"},{"location":"thehive/api/alert/delete/#query","text":"DELETE /api/alert/{id}?force=1","title":"Query"},{"location":"thehive/api/alert/delete/#response","text":"","title":"Response"},{"location":"thehive/api/alert/delete/#status-codes","text":"204 : if Alert is deleted successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/list-observables/","text":"List Observables # List observables of an Alerts . Query # POST /api/v0/query?name Request Body Example # List last 15 added observables: { \"query\" : [ { \"_name\" : \"getAlert\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"observables\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"extraData\" : [ \"seen\" ] } ] } With: id : id of the Alert Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # [ ... { \"_id\" : \"~11111462234\" , \"id\" : \"~11111462234\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"system@thehive.local\" , \"_createdAt\" : 1629309258431 , \"dataType\" : \"other\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1629309258431 , \"tlp\" : 0 , \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} } ... ]","title":"List Observables"},{"location":"thehive/api/alert/list-observables/#list-observables","text":"List observables of an Alerts .","title":"List Observables"},{"location":"thehive/api/alert/list-observables/#query","text":"POST /api/v0/query?name","title":"Query"},{"location":"thehive/api/alert/list-observables/#request-body-example","text":"List last 15 added observables: { \"query\" : [ { \"_name\" : \"getAlert\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"observables\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"extraData\" : [ \"seen\" ] } ] } With: id : id of the Alert","title":"Request Body Example"},{"location":"thehive/api/alert/list-observables/#response","text":"","title":"Response"},{"location":"thehive/api/alert/list-observables/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/list-observables/#responsebody-example","text":"[ ... { \"_id\" : \"~11111462234\" , \"id\" : \"~11111462234\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"system@thehive.local\" , \"_createdAt\" : 1629309258431 , \"dataType\" : \"other\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1629309258431 , \"tlp\" : 0 , \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} } ... ]","title":"ResponseBody Example"},{"location":"thehive/api/alert/list-responder-jobs/","text":"List responder actions # List actions run on an Alert . Query # GET /api/connector/cortex/action/responder/alert/{id} With: id : Alert identifier Response # Status codes # 200 : if query is run successfully 401 : Authentication error Response Body Example # 200 401 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Alert\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"user@thehive.local\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } List available Responders # Request # To get the list of Responders available for an Alert , based on its TLP and PAP, you can call the following API: GET /api/connector/cortex/responder/alert/{id} With: id : Alert identifier Response # 200 401 [ { \"id\" : \"e33d63082066c739c07d2bbc199bfe7e\" , \"name\" : \"MALSPAM_Reply_to_user_1_0\" , \"version\" : \"1.0\" , \"description\" : \"Reply to user with an email. Applies on tasks\" , \"dataTypeList\" : [ \"thehive:Alert\" ], \"cortexIds\" : [ \"Demo\" ] } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List responder actions"},{"location":"thehive/api/alert/list-responder-jobs/#list-responder-actions","text":"List actions run on an Alert .","title":"List responder actions"},{"location":"thehive/api/alert/list-responder-jobs/#query","text":"GET /api/connector/cortex/action/responder/alert/{id} With: id : Alert identifier","title":"Query"},{"location":"thehive/api/alert/list-responder-jobs/#response","text":"","title":"Response"},{"location":"thehive/api/alert/list-responder-jobs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/list-responder-jobs/#response-body-example","text":"200 401 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Alert\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"user@thehive.local\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response Body Example"},{"location":"thehive/api/alert/list-responder-jobs/#list-available-responders","text":"","title":"List available Responders"},{"location":"thehive/api/alert/list-responder-jobs/#request","text":"To get the list of Responders available for an Alert , based on its TLP and PAP, you can call the following API: GET /api/connector/cortex/responder/alert/{id} With: id : Alert identifier","title":"Request"},{"location":"thehive/api/alert/list-responder-jobs/#response_1","text":"200 401 [ { \"id\" : \"e33d63082066c739c07d2bbc199bfe7e\" , \"name\" : \"MALSPAM_Reply_to_user_1_0\" , \"version\" : \"1.0\" , \"description\" : \"Reply to user with an email. Applies on tasks\" , \"dataTypeList\" : [ \"thehive:Alert\" ], \"cortexIds\" : [ \"Demo\" ] } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response"},{"location":"thehive/api/alert/list/","text":"List / Search # List Alerts . Query # POST /api/v1/query?name=alerts Request Body Example # List last 15 alerts: { \"query\" : [ { \"_name\" : \"listAlert\" }, { \"_name\" : \"filter\" , \"_field\" : \"imported\" , \"_value\" : false }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"date\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"extraData\" : [ \"importDate\" , \"caseNumber\" ] } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # ```json [ ... { \"_id\": \"~789196976\", \"_type\": \"Alert\", \"_createdBy\": \"florian@strangebee.com\", \"_createdAt\": 1620393156944, \"status\": \"New\", \"type\": \"external\", \"source\": \"MISP server\", \"sourceRef\": \"event_1576\", \"externalLink\": null, \"title\": \"Phishing list update 7.5.2021\", \"description\": \"A curated list of phishing IOCs\", \"severity\": 2, \"date\": 1620393156000, \"tags\": [ \"source:MISP\", \"origin:CIRCL_LU\" ], \"tlp\": 3, \"pap\": 2, \"read\": false, \"follow\": true, \"customFields\": [], \"caseTemplate\": null, \"artifacts\": [], \"similarCases\": [] } ... ] ```","title":"List / Search"},{"location":"thehive/api/alert/list/#list-search","text":"List Alerts .","title":"List / Search"},{"location":"thehive/api/alert/list/#query","text":"POST /api/v1/query?name=alerts","title":"Query"},{"location":"thehive/api/alert/list/#request-body-example","text":"List last 15 alerts: { \"query\" : [ { \"_name\" : \"listAlert\" }, { \"_name\" : \"filter\" , \"_field\" : \"imported\" , \"_value\" : false }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"date\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"extraData\" : [ \"importDate\" , \"caseNumber\" ] } ] }","title":"Request Body Example"},{"location":"thehive/api/alert/list/#response","text":"","title":"Response"},{"location":"thehive/api/alert/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/list/#responsebody-example","text":"```json [ ... { \"_id\": \"~789196976\", \"_type\": \"Alert\", \"_createdBy\": \"florian@strangebee.com\", \"_createdAt\": 1620393156944, \"status\": \"New\", \"type\": \"external\", \"source\": \"MISP server\", \"sourceRef\": \"event_1576\", \"externalLink\": null, \"title\": \"Phishing list update 7.5.2021\", \"description\": \"A curated list of phishing IOCs\", \"severity\": 2, \"date\": 1620393156000, \"tags\": [ \"source:MISP\", \"origin:CIRCL_LU\" ], \"tlp\": 3, \"pap\": 2, \"read\": false, \"follow\": true, \"customFields\": [], \"caseTemplate\": null, \"artifacts\": [], \"similarCases\": [] } ... ] ```","title":"ResponseBody Example"},{"location":"thehive/api/alert/merge/","text":"Merge # Merge an Alert into an existing Case . Query # POST /api/alert/{id1}/merge/{id2} With: id1 : id of the Alert to merge id2 : id of the destination Case Response # Status codes # 200 : if Alert is successfully merged 401 : Authentication error ResponseBody Example # { \"_id\" : \"~6658533455\" , \"id\" : \"~6658533455\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620397519028 , \"updatedAt\" : 1624373852175 , \"_type\" : \"case\" , \"caseId\" : 114 , \"title\" : \"User connected to known malicious IP over Telnet / Malicious payload detected\" , \"description\" : \"EDR automated alert: the user robb@training.org has connected to known malicious IP over Telnet\\n\\nEDR automated alert: malicious payload detected on computer PC-Robb\\n \\n#### Merged with alert #90e044 User posted information on known phishing URL\\n\\nSIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"startDate\" : 1620396059728 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [ \"log-source:proxy\" , \"source:edr\" , \"log-source:endpoint-protection\" , \"source:siem\" , \"protocol: telnet\" , \"ex2\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"florian@strangebee.com\" , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" , \"order\" : 0 }, \"location\" : { \"string\" : \"Sydney\" , \"order\" : 1 } }, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"Merge"},{"location":"thehive/api/alert/merge/#merge","text":"Merge an Alert into an existing Case .","title":"Merge"},{"location":"thehive/api/alert/merge/#query","text":"POST /api/alert/{id1}/merge/{id2} With: id1 : id of the Alert to merge id2 : id of the destination Case","title":"Query"},{"location":"thehive/api/alert/merge/#response","text":"","title":"Response"},{"location":"thehive/api/alert/merge/#status-codes","text":"200 : if Alert is successfully merged 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/merge/#responsebody-example","text":"{ \"_id\" : \"~6658533455\" , \"id\" : \"~6658533455\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620397519028 , \"updatedAt\" : 1624373852175 , \"_type\" : \"case\" , \"caseId\" : 114 , \"title\" : \"User connected to known malicious IP over Telnet / Malicious payload detected\" , \"description\" : \"EDR automated alert: the user robb@training.org has connected to known malicious IP over Telnet\\n\\nEDR automated alert: malicious payload detected on computer PC-Robb\\n \\n#### Merged with alert #90e044 User posted information on known phishing URL\\n\\nSIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"startDate\" : 1620396059728 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [ \"log-source:proxy\" , \"source:edr\" , \"log-source:endpoint-protection\" , \"source:siem\" , \"protocol: telnet\" , \"ex2\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"florian@strangebee.com\" , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" , \"order\" : 0 }, \"location\" : { \"string\" : \"Sydney\" , \"order\" : 1 } }, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"ResponseBody Example"},{"location":"thehive/api/alert/promote-as-case/","text":"Promote # Promote an Alert as a new Case . Query # POST /api/alert/{id}/createCase With: id : id of the Alert to promote Request Body example # Specify a Case template applied with Case creation: { \"caseTemplate\" : \"SIEM_Alert\" } The following fields are optional: caseTemplate : (String) Response # Status codes # 201 : if Case is successfully created 401 : Authentication error ResponseBody Example # { \"_id\" : \"~907709843\" , \"id\" : \"~907709843\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630416621805 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 126 , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" , \"severity\" : 2 , \"startDate\" : 1630416621797 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"jerome@strangebee.com\" , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" , \"order\" : 0 }, \"location\" : { \"string\" : \"Sydney\" , \"order\" : 1 } }, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"Promote"},{"location":"thehive/api/alert/promote-as-case/#promote","text":"Promote an Alert as a new Case .","title":"Promote"},{"location":"thehive/api/alert/promote-as-case/#query","text":"POST /api/alert/{id}/createCase With: id : id of the Alert to promote","title":"Query"},{"location":"thehive/api/alert/promote-as-case/#request-body-example","text":"Specify a Case template applied with Case creation: { \"caseTemplate\" : \"SIEM_Alert\" } The following fields are optional: caseTemplate : (String)","title":"Request Body example"},{"location":"thehive/api/alert/promote-as-case/#response","text":"","title":"Response"},{"location":"thehive/api/alert/promote-as-case/#status-codes","text":"201 : if Case is successfully created 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/promote-as-case/#responsebody-example","text":"{ \"_id\" : \"~907709843\" , \"id\" : \"~907709843\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630416621805 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 126 , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" , \"severity\" : 2 , \"startDate\" : 1630416621797 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"jerome@strangebee.com\" , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" , \"order\" : 0 }, \"location\" : { \"string\" : \"Sydney\" , \"order\" : 1 } }, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"ResponseBody Example"},{"location":"thehive/api/alert/read/","text":"Mark as Read/Unread # Mark an Alert as read Query # Mark as read # POST /api/alert/{id}/markAsRead with: id : id of the Alert Mark as unread # POST /api/alert/{id}/markAsUnead with: id : id of the Alert Response # Status codes # 200 : if Alert is updated successfully 401 : Authentication error ResponseBody Example # { \"_id\" : \"~911601872\" , \"id\" : \"~911601872\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1620333017135 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"external\" , \"source\" : \"SIEM\" , \"sourceRef\" : \"8257b4\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"date\" : 1620333017000 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Ignored\" , \"follow\" : true , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" }, \"location\" : { \"string\" : \"Sydney\" } }, \"caseTemplate\" : null , \"artifacts\" : [ { \"_id\" : \"~624226312\" , \"id\" : \"~624226312\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017175 , \"_type\" : \"case_artifact\" , \"dataType\" : \"mail\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620333017175 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~788742360\" , \"id\" : \"~788742360\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017168 , \"_type\" : \"case_artifact\" , \"dataType\" : \"url\" , \"data\" : \"https://moneyfornothing.pl-getbuys.icu/\" , \"startDate\" : 1620333017168 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"message\" : \"http method: POST\" , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~870416536\" , \"id\" : \"~870416536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017157 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"94.154.129.50\" , \"startDate\" : 1620333017157 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} } ], \"similarCases\" : [] }","title":"Mark as Read/Unread"},{"location":"thehive/api/alert/read/#mark-as-readunread","text":"Mark an Alert as read","title":"Mark as Read/Unread"},{"location":"thehive/api/alert/read/#query","text":"","title":"Query"},{"location":"thehive/api/alert/read/#mark-as-read","text":"POST /api/alert/{id}/markAsRead with: id : id of the Alert","title":"Mark as read"},{"location":"thehive/api/alert/read/#mark-as-unread","text":"POST /api/alert/{id}/markAsUnead with: id : id of the Alert","title":"Mark as unread"},{"location":"thehive/api/alert/read/#response","text":"","title":"Response"},{"location":"thehive/api/alert/read/#status-codes","text":"200 : if Alert is updated successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/read/#responsebody-example","text":"{ \"_id\" : \"~911601872\" , \"id\" : \"~911601872\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1620333017135 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"external\" , \"source\" : \"SIEM\" , \"sourceRef\" : \"8257b4\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"date\" : 1620333017000 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Ignored\" , \"follow\" : true , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" }, \"location\" : { \"string\" : \"Sydney\" } }, \"caseTemplate\" : null , \"artifacts\" : [ { \"_id\" : \"~624226312\" , \"id\" : \"~624226312\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017175 , \"_type\" : \"case_artifact\" , \"dataType\" : \"mail\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620333017175 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~788742360\" , \"id\" : \"~788742360\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017168 , \"_type\" : \"case_artifact\" , \"dataType\" : \"url\" , \"data\" : \"https://moneyfornothing.pl-getbuys.icu/\" , \"startDate\" : 1620333017168 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"message\" : \"http method: POST\" , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~870416536\" , \"id\" : \"~870416536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017157 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"94.154.129.50\" , \"startDate\" : 1620333017157 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} } ], \"similarCases\" : [] }","title":"ResponseBody Example"},{"location":"thehive/api/alert/run-responder/","text":"Run Responder # Run a Responder on an Alert . Query # POST /api/connector/cortex/action Request Body Example # { \"responderId\" : \"05521ec727f75d69e828604dc5ae4c03\" , \"objectType\" : \"alert\" , \"objectId\" : \"~947478656\" } The following fields are required: responderId : (String) objectType : \"alert\" objectId : (String) Response # Status codes # 200 : if Responder is run successfully 401 : Authentication error ResponseBody Example # { \"responderId\" : \"05521ec727f75d69e828604dc5ae4bed\" , \"responderName\" : \"JIRA_Create_Ticket_1_0\" , \"responderDefinition\" : \"JIRA_Create_Ticket_1_0\" , \"cortexId\" : \"CORTEX_INTERNAL\" , \"cortexJobId\" : \"_v2EnHsB8Pn57ilsukA3\" , \"objectType\" : \"Alert\" , \"objectId\" : \"~947478656\" , \"status\" : \"Waiting\" , \"startDate\" : 1630418550145 , \"operations\" : \"[]\" , \"report\" : \"{}\" }","title":"Run Responder"},{"location":"thehive/api/alert/run-responder/#run-responder","text":"Run a Responder on an Alert .","title":"Run Responder"},{"location":"thehive/api/alert/run-responder/#query","text":"POST /api/connector/cortex/action","title":"Query"},{"location":"thehive/api/alert/run-responder/#request-body-example","text":"{ \"responderId\" : \"05521ec727f75d69e828604dc5ae4c03\" , \"objectType\" : \"alert\" , \"objectId\" : \"~947478656\" } The following fields are required: responderId : (String) objectType : \"alert\" objectId : (String)","title":"Request Body Example"},{"location":"thehive/api/alert/run-responder/#response","text":"","title":"Response"},{"location":"thehive/api/alert/run-responder/#status-codes","text":"200 : if Responder is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/run-responder/#responsebody-example","text":"{ \"responderId\" : \"05521ec727f75d69e828604dc5ae4bed\" , \"responderName\" : \"JIRA_Create_Ticket_1_0\" , \"responderDefinition\" : \"JIRA_Create_Ticket_1_0\" , \"cortexId\" : \"CORTEX_INTERNAL\" , \"cortexJobId\" : \"_v2EnHsB8Pn57ilsukA3\" , \"objectType\" : \"Alert\" , \"objectId\" : \"~947478656\" , \"status\" : \"Waiting\" , \"startDate\" : 1630418550145 , \"operations\" : \"[]\" , \"report\" : \"{}\" }","title":"ResponseBody Example"},{"location":"thehive/api/alert/similar-cases/","text":"List similar Cases # List similar Cases . Query # POST /api/v1/query?name=alert-similar-cases Request Body Example # { \"query\" : [ { \"_name\" : \"getAlert\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"similarCases\" , \"caseFilter\" : { \"_field\" : \"status\" , \"_value\" : \"Open\" } } ] } with: id : id of the Alert. Response # Status codes # 200 : if query is successful 401 : Authentication error ResponseBody Example # [ { \"case\" : { \"_id\" : \"~665851112\" , \"_type\" : \"Case\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_updatedBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620397519028 , \"_updatedAt\" : 1624373852175 , \"number\" : 114 , \"title\" : \"User connected to known malicious IP over Telnet / Malicious payload detected\" , \"description\" : \"EDR automated alert: the user robb@training.org has connected to known malicious IP over Telnet\\n\\nEDR automated alert: malicious payload detected on computer PC-Robb\" , \"severity\" : 2 , \"startDate\" : 1620396059728 , \"tags\" : [ \"source:edr\" , \"protocol: telnet\" , \"log-source:endpoint-protection\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"assignee\" : \"florian@strangebee.com\" , \"customFields\" : [], \"extraData\" : {} }, \"similarObservableCount\" : 1 , \"observableCount\" : 6 , \"similarIocCount\" : 0 , \"iocCount\" : 0 , \"observableTypes\" : { \"username\" : 1 } }, { \"case\" : { \"_id\" : \"~789202345\" , \"_type\" : \"Case\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620393185339 , \"number\" : 111 , \"title\" : \"Phishing -User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"startDate\" : 1620393185257 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" , \"category:Phishing\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"assignee\" : \"florian@strangebee.com\" , \"customFields\" : [], \"extraData\" : {} }, \"similarObservableCount\" : 2 , \"observableCount\" : 4 , \"similarIocCount\" : 0 , \"iocCount\" : 1 , \"observableTypes\" : { \"username\" : 1 , \"mail\" : 1 } } ]","title":"List similar Cases"},{"location":"thehive/api/alert/similar-cases/#list-similar-cases","text":"List similar Cases .","title":"List similar Cases"},{"location":"thehive/api/alert/similar-cases/#query","text":"POST /api/v1/query?name=alert-similar-cases","title":"Query"},{"location":"thehive/api/alert/similar-cases/#request-body-example","text":"{ \"query\" : [ { \"_name\" : \"getAlert\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"similarCases\" , \"caseFilter\" : { \"_field\" : \"status\" , \"_value\" : \"Open\" } } ] } with: id : id of the Alert.","title":"Request Body Example"},{"location":"thehive/api/alert/similar-cases/#response","text":"","title":"Response"},{"location":"thehive/api/alert/similar-cases/#status-codes","text":"200 : if query is successful 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/similar-cases/#responsebody-example","text":"[ { \"case\" : { \"_id\" : \"~665851112\" , \"_type\" : \"Case\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_updatedBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620397519028 , \"_updatedAt\" : 1624373852175 , \"number\" : 114 , \"title\" : \"User connected to known malicious IP over Telnet / Malicious payload detected\" , \"description\" : \"EDR automated alert: the user robb@training.org has connected to known malicious IP over Telnet\\n\\nEDR automated alert: malicious payload detected on computer PC-Robb\" , \"severity\" : 2 , \"startDate\" : 1620396059728 , \"tags\" : [ \"source:edr\" , \"protocol: telnet\" , \"log-source:endpoint-protection\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"assignee\" : \"florian@strangebee.com\" , \"customFields\" : [], \"extraData\" : {} }, \"similarObservableCount\" : 1 , \"observableCount\" : 6 , \"similarIocCount\" : 0 , \"iocCount\" : 0 , \"observableTypes\" : { \"username\" : 1 } }, { \"case\" : { \"_id\" : \"~789202345\" , \"_type\" : \"Case\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620393185339 , \"number\" : 111 , \"title\" : \"Phishing -User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"startDate\" : 1620393185257 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" , \"category:Phishing\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"assignee\" : \"florian@strangebee.com\" , \"customFields\" : [], \"extraData\" : {} }, \"similarObservableCount\" : 2 , \"observableCount\" : 4 , \"similarIocCount\" : 0 , \"iocCount\" : 1 , \"observableTypes\" : { \"username\" : 1 , \"mail\" : 1 } } ]","title":"ResponseBody Example"},{"location":"thehive/api/alert/update-observable/","text":"Update observable # update an Alert Observable . Query # PATCH /api/alert/artifact/{id} With: id : Alert identifier Updatable fields are: tlp , ioc , sighted , tags , message , ignoreSimilarity Request Body Example # { \"ioc\" : True , \"tags\" :[ \"malicious\" ] } Response # Status codes # 200 : if Alert observable is updated successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 401 403 { \"_id\" : \"~1564784\" , \"id\" : \"~1564784\" , \"createdBy\" : \"analyst@soc\" , \"updatedBy\" : \"analyst@soc\" , \"createdAt\" : 1637091448338 , \"updatedAt\" : 1637092980667 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1637091448338 , \"tlp\" : 2 , \"tags\" :[ \"malicious\" ], \"ioc\" : true , \"sighted\" : true , \"message\" : \"sample description\" , \"reports\" :{}, \"stats\" :{}, \"ignoreSimilarity\" : false } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Update observable"},{"location":"thehive/api/alert/update-observable/#update-observable","text":"update an Alert Observable .","title":"Update observable"},{"location":"thehive/api/alert/update-observable/#query","text":"PATCH /api/alert/artifact/{id} With: id : Alert identifier Updatable fields are: tlp , ioc , sighted , tags , message , ignoreSimilarity","title":"Query"},{"location":"thehive/api/alert/update-observable/#request-body-example","text":"{ \"ioc\" : True , \"tags\" :[ \"malicious\" ] }","title":"Request Body Example"},{"location":"thehive/api/alert/update-observable/#response","text":"","title":"Response"},{"location":"thehive/api/alert/update-observable/#status-codes","text":"200 : if Alert observable is updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/alert/update-observable/#responsebody-example","text":"200 401 403 { \"_id\" : \"~1564784\" , \"id\" : \"~1564784\" , \"createdBy\" : \"analyst@soc\" , \"updatedBy\" : \"analyst@soc\" , \"createdAt\" : 1637091448338 , \"updatedAt\" : 1637092980667 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1637091448338 , \"tlp\" : 2 , \"tags\" :[ \"malicious\" ], \"ioc\" : true , \"sighted\" : true , \"message\" : \"sample description\" , \"reports\" :{}, \"stats\" :{}, \"ignoreSimilarity\" : false } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/alert/update/","text":"Update # Update an Alert . Query # PATCH /api/alert/{id} with: id : id of the Alert Request Body Example # { \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" } Response # Status codes # 200 : if Alert is updated successfully 401 : Authentication error ResponseBody Example # { \"_id\" : \"~624443400\" , \"id\" : \"~624443400\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1620373264377 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"external\" , \"source\" : \"SIEM\" , \"sourceRef\" : \"47e379\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" , \"severity\" : 2 , \"date\" : 1620373264000 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Ignored\" , \"follow\" : true , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" }, \"location\" : { \"string\" : \"Sydney\" } }, \"caseTemplate\" : null , \"artifacts\" : [ { \"_id\" : \"~665772152\" , \"id\" : \"~665772152\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264410 , \"_type\" : \"case_artifact\" , \"dataType\" : \"username\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620373264410 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~677015568\" , \"id\" : \"~677015568\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264398 , \"_type\" : \"case_artifact\" , \"dataType\" : \"domain\" , \"data\" : \"pl-getbuys.icu\" , \"startDate\" : 1620373264398 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~677019664\" , \"id\" : \"~677019664\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264405 , \"_type\" : \"case_artifact\" , \"dataType\" : \"mail\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620373264405 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~706650224\" , \"id\" : \"~706650224\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264391 , \"_type\" : \"case_artifact\" , \"dataType\" : \"url\" , \"data\" : \"https://poczta.pl-getbuys.icu/\" , \"startDate\" : 1620373264391 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"message\" : \"http method: POST\" , \"reports\" : {}, \"stats\" : {} } ], \"similarCases\" : [] }","title":"Update"},{"location":"thehive/api/alert/update/#update","text":"Update an Alert .","title":"Update"},{"location":"thehive/api/alert/update/#query","text":"PATCH /api/alert/{id} with: id : id of the Alert","title":"Query"},{"location":"thehive/api/alert/update/#request-body-example","text":"{ \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" }","title":"Request Body Example"},{"location":"thehive/api/alert/update/#response","text":"","title":"Response"},{"location":"thehive/api/alert/update/#status-codes","text":"200 : if Alert is updated successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/update/#responsebody-example","text":"{ \"_id\" : \"~624443400\" , \"id\" : \"~624443400\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1620373264377 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"external\" , \"source\" : \"SIEM\" , \"sourceRef\" : \"47e379\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" , \"severity\" : 2 , \"date\" : 1620373264000 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Ignored\" , \"follow\" : true , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" }, \"location\" : { \"string\" : \"Sydney\" } }, \"caseTemplate\" : null , \"artifacts\" : [ { \"_id\" : \"~665772152\" , \"id\" : \"~665772152\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264410 , \"_type\" : \"case_artifact\" , \"dataType\" : \"username\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620373264410 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~677015568\" , \"id\" : \"~677015568\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264398 , \"_type\" : \"case_artifact\" , \"dataType\" : \"domain\" , \"data\" : \"pl-getbuys.icu\" , \"startDate\" : 1620373264398 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~677019664\" , \"id\" : \"~677019664\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264405 , \"_type\" : \"case_artifact\" , \"dataType\" : \"mail\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620373264405 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~706650224\" , \"id\" : \"~706650224\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264391 , \"_type\" : \"case_artifact\" , \"dataType\" : \"url\" , \"data\" : \"https://poczta.pl-getbuys.icu/\" , \"startDate\" : 1620373264391 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"message\" : \"http method: POST\" , \"reports\" : {}, \"stats\" : {} } ], \"similarCases\" : [] }","title":"ResponseBody Example"},{"location":"thehive/api/case/","text":"Case APIs # Create case Update case Delete case Merge cases Export case to MISP List related case List related alerts List attachments Run responder List responder jobs","title":"Overview"},{"location":"thehive/api/case/#case-apis","text":"Create case Update case Delete case Merge cases Export case to MISP List related case List related alerts List attachments Run responder List responder jobs","title":"Case APIs"},{"location":"thehive/api/case/attachments/","text":"List related Alerts # List attachments added to task logs of a Case . Query # POST /api/v0/query Request Body Example # List attachments added to task logs or a Case identified by {id} : { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"tasks\" }, { \"_name\" : \"filter\" , \"_ne\" : { \"_field\" : \"status\" , \"_value\" : \"Cancel\" } }, { \"_name\" : \"logs\" }, { \"_contains\" : \"attachment.id\" , \"_name\" : \"filter\" }, { \"_name\" : \"page\" , \"extraData\" : [ \"taskId\" ], \"from\" : 0 , \"to\" : 100 } ] } With: id : id of the Case Response # Status codes # 200 : if query is run successfully 401 : Authentication error 404 : if the Case is not found Response Body Example # [ ... { \"_id\" : \"~122892472\" , \"id\" : \"~122892472\" , \"createdBy\" : \"user@thehive.local\" , \"createdAt\" : 1632124353194 , \"_type\" : \"case_task_log\" , \"message\" : \"message\" , \"startDate\" : 1632124353194 , \"attachment\" : { \"name\" : \"filename.png\" , \"hashes\" : [ \"0b62003cb73578d9e738a70aa7a81e89d3683282ac393856a96ef364cd1038cb\" , \"caa75ff1e33ee8bfba764c9a6139fb72e7f4e20a\" , \"a3e41c32ff817fc759bafeb1a106a433\" ], \"size\" : 42213 , \"contentType\" : \"image/png\" , \"id\" : \"0b62003cb73578d9e738a70aa7a81e89d3683282ac393856a96ef364cd1038cb\" }, \"status\" : \"Ok\" , \"owner\" : \"user@thehive.local\" } ... ]","title":"List related Alerts"},{"location":"thehive/api/case/attachments/#list-related-alerts","text":"List attachments added to task logs of a Case .","title":"List related Alerts"},{"location":"thehive/api/case/attachments/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/case/attachments/#request-body-example","text":"List attachments added to task logs or a Case identified by {id} : { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"tasks\" }, { \"_name\" : \"filter\" , \"_ne\" : { \"_field\" : \"status\" , \"_value\" : \"Cancel\" } }, { \"_name\" : \"logs\" }, { \"_contains\" : \"attachment.id\" , \"_name\" : \"filter\" }, { \"_name\" : \"page\" , \"extraData\" : [ \"taskId\" ], \"from\" : 0 , \"to\" : 100 } ] } With: id : id of the Case","title":"Request Body Example"},{"location":"thehive/api/case/attachments/#response","text":"","title":"Response"},{"location":"thehive/api/case/attachments/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 404 : if the Case is not found","title":"Status codes"},{"location":"thehive/api/case/attachments/#response-body-example","text":"[ ... { \"_id\" : \"~122892472\" , \"id\" : \"~122892472\" , \"createdBy\" : \"user@thehive.local\" , \"createdAt\" : 1632124353194 , \"_type\" : \"case_task_log\" , \"message\" : \"message\" , \"startDate\" : 1632124353194 , \"attachment\" : { \"name\" : \"filename.png\" , \"hashes\" : [ \"0b62003cb73578d9e738a70aa7a81e89d3683282ac393856a96ef364cd1038cb\" , \"caa75ff1e33ee8bfba764c9a6139fb72e7f4e20a\" , \"a3e41c32ff817fc759bafeb1a106a433\" ], \"size\" : 42213 , \"contentType\" : \"image/png\" , \"id\" : \"0b62003cb73578d9e738a70aa7a81e89d3683282ac393856a96ef364cd1038cb\" }, \"status\" : \"Ok\" , \"owner\" : \"user@thehive.local\" } ... ]","title":"Response Body Example"},{"location":"thehive/api/case/create/","text":"Create # Create a Case Query # POST /api/case With mandatory fields: title : (String) title of the Case description : (String) description of the Case Request Body Example # Basic request # { \"title\" : \"my first case\" , \"description\" : \"my first case description\" } Request with more details, customFields and tasks # { \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"Severity\" : 3 , \"tlp\" : 2 , \"pap\" : 2 , \"startDate\" : 1635876967233 , \"tags\" : [ \"Test Tag\" , \"Another Test Tag\" ], \"flag\" : false , \"owner\" : \"username@org\" , \"tasks\" : [{ \"title\" : \"mytask\" , \"description\" : \"description of my task\" }], \"customFields\" :{ \"cvss\" : { \"integer\" : 9 }, \"businessUnit\" : { \"string\" : \"Sales\" } } } Response # Status code # 201 : if Case is created successfully 401 : Authentication error 403 : Authorization error Response Body Example # 201 401 403 { \"_id\" : \"~41644112\" , \"id\" : \"~41644112\" , \"createdBy\" : \"user@org\" , \"updatedBy\" : null , \"createdAt\" : 1635876967235 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 4 , \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"severity\" : 2 , \"startDate\" : 1635876967233 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" :[], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"user@org\" , \"customFields\" :{}, \"stats\" :{}, \"permissions\" :[ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Create"},{"location":"thehive/api/case/create/#create","text":"Create a Case","title":"Create"},{"location":"thehive/api/case/create/#query","text":"POST /api/case With mandatory fields: title : (String) title of the Case description : (String) description of the Case","title":"Query"},{"location":"thehive/api/case/create/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/case/create/#basic-request","text":"{ \"title\" : \"my first case\" , \"description\" : \"my first case description\" }","title":"Basic request"},{"location":"thehive/api/case/create/#request-with-more-details-customfields-and-tasks","text":"{ \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"Severity\" : 3 , \"tlp\" : 2 , \"pap\" : 2 , \"startDate\" : 1635876967233 , \"tags\" : [ \"Test Tag\" , \"Another Test Tag\" ], \"flag\" : false , \"owner\" : \"username@org\" , \"tasks\" : [{ \"title\" : \"mytask\" , \"description\" : \"description of my task\" }], \"customFields\" :{ \"cvss\" : { \"integer\" : 9 }, \"businessUnit\" : { \"string\" : \"Sales\" } } }","title":"Request with more details, customFields and tasks"},{"location":"thehive/api/case/create/#response","text":"","title":"Response"},{"location":"thehive/api/case/create/#status-code","text":"201 : if Case is created successfully 401 : Authentication error 403 : Authorization error","title":"Status code"},{"location":"thehive/api/case/create/#response-body-example","text":"201 401 403 { \"_id\" : \"~41644112\" , \"id\" : \"~41644112\" , \"createdBy\" : \"user@org\" , \"updatedBy\" : null , \"createdAt\" : 1635876967235 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 4 , \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"severity\" : 2 , \"startDate\" : 1635876967233 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" :[], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"user@org\" , \"customFields\" :{}, \"stats\" :{}, \"permissions\" :[ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Response Body Example"},{"location":"thehive/api/case/delete/","text":"Delete # Permanently delete a Case . Query # DELETE /api/case/{id}?force=1 With: id : id of the Case Response # Status codes # 204 : if Case is deleted successfully 401 : Authentication error 404 : if Case is not found","title":"Delete"},{"location":"thehive/api/case/delete/#delete","text":"Permanently delete a Case .","title":"Delete"},{"location":"thehive/api/case/delete/#query","text":"DELETE /api/case/{id}?force=1 With: id : id of the Case","title":"Query"},{"location":"thehive/api/case/delete/#response","text":"","title":"Response"},{"location":"thehive/api/case/delete/#status-codes","text":"204 : if Case is deleted successfully 401 : Authentication error 404 : if Case is not found","title":"Status codes"},{"location":"thehive/api/case/export/","text":"Export Case to MISP # Export Case to a MISP server to create an event including the Case observables marked as IOC. Query # POST /api/connector/misp/export/{id}/{misp-server} With: id : id of the Case misp-server : name of the MISP server as defined in the configuration Note Only MISP servers with purpose equals to ExportOnly or ImportAndExport can recieve Case exports Response # Status codes # 204 : if Case is successfully exported 401 : Authentication error 404 : if Case or MISP server is not found.","title":"Export Case to MISP"},{"location":"thehive/api/case/export/#export-case-to-misp","text":"Export Case to a MISP server to create an event including the Case observables marked as IOC.","title":"Export Case to MISP"},{"location":"thehive/api/case/export/#query","text":"POST /api/connector/misp/export/{id}/{misp-server} With: id : id of the Case misp-server : name of the MISP server as defined in the configuration Note Only MISP servers with purpose equals to ExportOnly or ImportAndExport can recieve Case exports","title":"Query"},{"location":"thehive/api/case/export/#response","text":"","title":"Response"},{"location":"thehive/api/case/export/#status-codes","text":"204 : if Case is successfully exported 401 : Authentication error 404 : if Case or MISP server is not found.","title":"Status codes"},{"location":"thehive/api/case/merge/","text":"Merge # Merge two Cases in a single Case . This APIs permanently removes the source Cases and creates a Case by merging all the data from the sources. Query # POST /api/v0/case/{id1}/_merge/{id2} with: id1 : id of the first Case id2 : id of the second Case Response # Status codes # 204 : if the Cases are merged successfully 401 : Authentication error 404 : if at least one of the Cases is not found Response Body Example # { \"_id\" : \"~81928240\" , \"id\" : \"~81928240\" , \"createdBy\" : \"user@thehive.local\" , \"updatedBy\" : null , \"createdAt\" : 1632132365250 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 87 , \"title\" : \"Case 1 / Case 2\" , \"description\" : \"test\\n\\ntest\" , \"severity\" : 2 , \"startDate\" : 1632124020000 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"user@thehive.local\" , \"customFields\" : {}, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"Merge"},{"location":"thehive/api/case/merge/#merge","text":"Merge two Cases in a single Case . This APIs permanently removes the source Cases and creates a Case by merging all the data from the sources.","title":"Merge"},{"location":"thehive/api/case/merge/#query","text":"POST /api/v0/case/{id1}/_merge/{id2} with: id1 : id of the first Case id2 : id of the second Case","title":"Query"},{"location":"thehive/api/case/merge/#response","text":"","title":"Response"},{"location":"thehive/api/case/merge/#status-codes","text":"204 : if the Cases are merged successfully 401 : Authentication error 404 : if at least one of the Cases is not found","title":"Status codes"},{"location":"thehive/api/case/merge/#response-body-example","text":"{ \"_id\" : \"~81928240\" , \"id\" : \"~81928240\" , \"createdBy\" : \"user@thehive.local\" , \"updatedBy\" : null , \"createdAt\" : 1632132365250 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 87 , \"title\" : \"Case 1 / Case 2\" , \"description\" : \"test\\n\\ntest\" , \"severity\" : 2 , \"startDate\" : 1632124020000 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"user@thehive.local\" , \"customFields\" : {}, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"Response Body Example"},{"location":"thehive/api/case/related-alerts/","text":"List related Alerts # List alerts merged in a Case . Query # POST /api/v0/query Request Body Example # List last 5 merged alerts in a Case identified by {id} : { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"alerts\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 5 } ] } With: id : id of the Case Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # [ ... [ { \"_id\" : \"~43618512\" , \"id\" : \"~43618512\" , \"createdBy\" : \"demo@thehive.local\" , \"updatedBy\" : null , \"createdAt\" : 1618344277475 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"testing\" , \"source\" : \"create-alert.py\" , \"sourceRef\" : \"85a766ec\" , \"externalLink\" : null , \"case\" : \"~122884120\" , \"title\" : \"Alert 85a766ec-060a-49a0-bc82-c672b6e51e6c\" , \"description\" : \"N/A\" , \"severity\" : 1 , \"date\" : 1618344277000 , \"tags\" : [ \"sample\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Imported\" , \"follow\" : true , \"customFields\" : { \"company\" : { \"string\" : \"Customer 1\" } }, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] } ] ... ]","title":"List related Alerts"},{"location":"thehive/api/case/related-alerts/#list-related-alerts","text":"List alerts merged in a Case .","title":"List related Alerts"},{"location":"thehive/api/case/related-alerts/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/case/related-alerts/#request-body-example","text":"List last 5 merged alerts in a Case identified by {id} : { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"alerts\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 5 } ] } With: id : id of the Case","title":"Request Body Example"},{"location":"thehive/api/case/related-alerts/#response","text":"","title":"Response"},{"location":"thehive/api/case/related-alerts/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/case/related-alerts/#responsebody-example","text":"[ ... [ { \"_id\" : \"~43618512\" , \"id\" : \"~43618512\" , \"createdBy\" : \"demo@thehive.local\" , \"updatedBy\" : null , \"createdAt\" : 1618344277475 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"testing\" , \"source\" : \"create-alert.py\" , \"sourceRef\" : \"85a766ec\" , \"externalLink\" : null , \"case\" : \"~122884120\" , \"title\" : \"Alert 85a766ec-060a-49a0-bc82-c672b6e51e6c\" , \"description\" : \"N/A\" , \"severity\" : 1 , \"date\" : 1618344277000 , \"tags\" : [ \"sample\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Imported\" , \"follow\" : true , \"customFields\" : { \"company\" : { \"string\" : \"Customer 1\" } }, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] } ] ... ]","title":"ResponseBody Example"},{"location":"thehive/api/case/related-cases/","text":"List related Cases # List similar Cases of a given Case . This API uses observable based similarity to find related Cases Query # GET /api/case/{id}/links With: id : id of the Case Response # Status codes # 200 : if query is run successfully 401 : Authentication error 404 : if the case doesn't exist ResponseBody Example # [ { \"_id\" : \"~48144448\" , \"_type\" : \"case\" , \"caseId\" : 66 , \"createdAt\" : 1618344529302 , \"createdBy\" : \"user@thehive.local\" , \"customFields\" : {}, \"description\" : \"N/A\" , \"endDate\" : null , \"flag\" : false , \"id\" : \"~48144448\" , \"impactStatus\" : null , \"linkedWith\" : [ { \"_id\" : \"~122888216\" , \"_type\" : \"case_artifact\" , \"createdAt\" : 1632114988895 , \"createdBy\" : \"user@strangebee.com\" , \"data\" : \"google.com\" , \"dataType\" : \"domain\" , \"id\" : \"~122888216\" , \"ignoreSimilarity\" : false , \"ioc\" : false , \"message\" : \"test\" , \"reports\" : {}, \"sighted\" : false , \"startDate\" : 1632114988895 , \"stats\" : {}, \"tags\" : [], \"tlp\" : 2 } ], \"linksCount\" : 1 , \"owner\" : \"nabil@thehive.local\" , \"pap\" : 1 , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"resolutionStatus\" : null , \"severity\" : 4 , \"startDate\" : 1618344529000 , \"stats\" : {}, \"status\" : \"Open\" , \"summary\" : null , \"tags\" : [ \"sample\" ], \"title\" : \"Case a31acfad-8368-4395-bf1d-6d5c1675c0ba\" , \"tlp\" : 1 , \"updatedAt\" : null , \"updatedBy\" : null } ]","title":"List related Cases"},{"location":"thehive/api/case/related-cases/#list-related-cases","text":"List similar Cases of a given Case . This API uses observable based similarity to find related Cases","title":"List related Cases"},{"location":"thehive/api/case/related-cases/#query","text":"GET /api/case/{id}/links With: id : id of the Case","title":"Query"},{"location":"thehive/api/case/related-cases/#response","text":"","title":"Response"},{"location":"thehive/api/case/related-cases/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 404 : if the case doesn't exist","title":"Status codes"},{"location":"thehive/api/case/related-cases/#responsebody-example","text":"[ { \"_id\" : \"~48144448\" , \"_type\" : \"case\" , \"caseId\" : 66 , \"createdAt\" : 1618344529302 , \"createdBy\" : \"user@thehive.local\" , \"customFields\" : {}, \"description\" : \"N/A\" , \"endDate\" : null , \"flag\" : false , \"id\" : \"~48144448\" , \"impactStatus\" : null , \"linkedWith\" : [ { \"_id\" : \"~122888216\" , \"_type\" : \"case_artifact\" , \"createdAt\" : 1632114988895 , \"createdBy\" : \"user@strangebee.com\" , \"data\" : \"google.com\" , \"dataType\" : \"domain\" , \"id\" : \"~122888216\" , \"ignoreSimilarity\" : false , \"ioc\" : false , \"message\" : \"test\" , \"reports\" : {}, \"sighted\" : false , \"startDate\" : 1632114988895 , \"stats\" : {}, \"tags\" : [], \"tlp\" : 2 } ], \"linksCount\" : 1 , \"owner\" : \"nabil@thehive.local\" , \"pap\" : 1 , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"resolutionStatus\" : null , \"severity\" : 4 , \"startDate\" : 1618344529000 , \"stats\" : {}, \"status\" : \"Open\" , \"summary\" : null , \"tags\" : [ \"sample\" ], \"title\" : \"Case a31acfad-8368-4395-bf1d-6d5c1675c0ba\" , \"tlp\" : 1 , \"updatedAt\" : null , \"updatedBy\" : null } ]","title":"ResponseBody Example"},{"location":"thehive/api/case/responder-jobs/","text":"List responder actions # List actions run on a Case . Query # GET /api/connector/cortex/action/case/{id} With: id : Case identifier Response # Status codes # 200 : if query is run successfully 401 : Authentication error Response Body Example # 200 401 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Case\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"user@thehive.local\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } List available Responders # Request # To get the list of Responders available for a Case , based on its TLP and PAP, you can call the following API: GET /api/connector/cortex/responder/case/{id} With: id : Case identifier Response # 200 401 [ { \"id\" : \"e33d63082066c739c07d2bbc199bfe7e\" , \"name\" : \"MALSPAM_Reply_to_user_1_0\" , \"version\" : \"1.0\" , \"description\" : \"Reply to user with an email. Applies on tasks\" , \"dataTypeList\" : [ \"thehive:case\" , \"thehive:case_task\" , \"thehive:case_task_log\" ], \"cortexIds\" : [ \"Demo\" ] } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List responder actions"},{"location":"thehive/api/case/responder-jobs/#list-responder-actions","text":"List actions run on a Case .","title":"List responder actions"},{"location":"thehive/api/case/responder-jobs/#query","text":"GET /api/connector/cortex/action/case/{id} With: id : Case identifier","title":"Query"},{"location":"thehive/api/case/responder-jobs/#response","text":"","title":"Response"},{"location":"thehive/api/case/responder-jobs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/case/responder-jobs/#response-body-example","text":"200 401 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Case\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"user@thehive.local\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response Body Example"},{"location":"thehive/api/case/responder-jobs/#list-available-responders","text":"","title":"List available Responders"},{"location":"thehive/api/case/responder-jobs/#request","text":"To get the list of Responders available for a Case , based on its TLP and PAP, you can call the following API: GET /api/connector/cortex/responder/case/{id} With: id : Case identifier","title":"Request"},{"location":"thehive/api/case/responder-jobs/#response_1","text":"200 401 [ { \"id\" : \"e33d63082066c739c07d2bbc199bfe7e\" , \"name\" : \"MALSPAM_Reply_to_user_1_0\" , \"version\" : \"1.0\" , \"description\" : \"Reply to user with an email. Applies on tasks\" , \"dataTypeList\" : [ \"thehive:case\" , \"thehive:case_task\" , \"thehive:case_task_log\" ], \"cortexIds\" : [ \"Demo\" ] } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response"},{"location":"thehive/api/case/run-responder/","text":"Run responder # Run a responder on a Case (requires manageAction permission). Query # POST /api/connector/cortex/action Request Body Example # { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case\" , \"objectId\" : \"{id}\" } With: id : Case identifier The required fields are responderId , objectType and objectId . Response # Status codes # 201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Case is not found Response Body Example # 201 401 404 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Case\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Run responder"},{"location":"thehive/api/case/run-responder/#run-responder","text":"Run a responder on a Case (requires manageAction permission).","title":"Run responder"},{"location":"thehive/api/case/run-responder/#query","text":"POST /api/connector/cortex/action","title":"Query"},{"location":"thehive/api/case/run-responder/#request-body-example","text":"{ \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case\" , \"objectId\" : \"{id}\" } With: id : Case identifier The required fields are responderId , objectType and objectId .","title":"Request Body Example"},{"location":"thehive/api/case/run-responder/#response","text":"","title":"Response"},{"location":"thehive/api/case/run-responder/#status-codes","text":"201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Case is not found","title":"Status codes"},{"location":"thehive/api/case/run-responder/#response-body-example","text":"201 401 404 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Case\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Response Body Example"},{"location":"thehive/api/case/update/","text":"Update # Update a Case Query # PATCH /api/case/{id} Request Body Example # Case details update # { \"severity\" : 3 , \"tags\" : [ \"updated\" ] } Case customFields update # To update specific customFields : { \"customFields.business-unit.string\" : \"VIP\" , \"customFields.cvss.integer\" : 3 } To patch Case customFields : \"customFields\" : { \"business-unit\" : { \"string\" : \"VIP\" } } Danger Case customFields not mentionned in this request will be erased. Response # Status codes # 200 : Case has been updated successfully 401 : Authentication error 403 : Authorization error Response Body Example # 201 401 403 { \"_id\" : \"~311352\" , \"id\" : \"~311352\" , \"createdBy\" : \"analyst@soc\" , \"updatedBy\" : \"analyst@soc\" , \"createdAt\" : 1635879111239 , \"updatedAt\" : 1637083041511 , \"_type\" : \"case\" , \"caseId\" : 6 , \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"severity\" : 3 , \"startDate\" : 1635876967233 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" :[ \"updated\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"analyst@soc\" , \"customFields\" :{ \"business-unit\" :{ \"string\" : \"Sales\" , \"order\" : 1 }, \"cvss\" :{ \"integer\" : 9 , \"order\" : 0 } }, \"stats\" :{}, \"permissions\" :[ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Update"},{"location":"thehive/api/case/update/#update","text":"Update a Case","title":"Update"},{"location":"thehive/api/case/update/#query","text":"PATCH /api/case/{id}","title":"Query"},{"location":"thehive/api/case/update/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/case/update/#case-details-update","text":"{ \"severity\" : 3 , \"tags\" : [ \"updated\" ] }","title":"Case details update"},{"location":"thehive/api/case/update/#case-customfields-update","text":"To update specific customFields : { \"customFields.business-unit.string\" : \"VIP\" , \"customFields.cvss.integer\" : 3 } To patch Case customFields : \"customFields\" : { \"business-unit\" : { \"string\" : \"VIP\" } } Danger Case customFields not mentionned in this request will be erased.","title":"Case customFields update"},{"location":"thehive/api/case/update/#response","text":"","title":"Response"},{"location":"thehive/api/case/update/#status-codes","text":"200 : Case has been updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/case/update/#response-body-example","text":"201 401 403 { \"_id\" : \"~311352\" , \"id\" : \"~311352\" , \"createdBy\" : \"analyst@soc\" , \"updatedBy\" : \"analyst@soc\" , \"createdAt\" : 1635879111239 , \"updatedAt\" : 1637083041511 , \"_type\" : \"case\" , \"caseId\" : 6 , \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"severity\" : 3 , \"startDate\" : 1635876967233 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" :[ \"updated\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"analyst@soc\" , \"customFields\" :{ \"business-unit\" :{ \"string\" : \"Sales\" , \"order\" : 1 }, \"cvss\" :{ \"integer\" : 9 , \"order\" : 0 } }, \"stats\" :{}, \"permissions\" :[ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Response Body Example"},{"location":"thehive/api/case-template/","text":"Case template APIs # List case templates Create case template Delete case template Update case template","title":"Overview"},{"location":"thehive/api/case-template/#case-template-apis","text":"List case templates Create case template Delete case template Update case template","title":"Case template APIs"},{"location":"thehive/api/case-template/create/","text":"Create # Create a Case Templates . Query # POST /api/case/template Request Body Example # { \"name\" : \"MISPEvent\" , \"titlePrefix\" : \"\" , \"severity\" : 2 , \"tlp\" : 2 , \"pap\" : 2 , \"tags\" : [ \"hunting\" ], \"tasks\" : [ { \"order\" : 0 , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" }, { \"order\" : 1 , \"title\" : \"Search for IOCs on Firewall logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in firewall logs and look for IOcs of type IP, port\" }, { \"order\" : 2 , \"title\" : \"Search for IOCs on Web proxy logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in web proxy logs and look for IOcs of type IP, domain, hostname, user-agent\" } ], \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 } }, \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"displayName\" : \"MISP\" } name should be unique. Otherwise an error 400 Bad Request is returned Response # Status codes # 201 : if template was created successfully 400 : in case of error in input 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 400 401 { \"_id\" : \"~910319824\" , \"id\" : \"~910319824\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267739 , \"_type\" : \"caseTemplate\" , \"name\" : \"MISPEvent\" , \"displayName\" : \"MISP\" , \"titlePrefix\" : \"[MISP]\" , \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"severity\" : 2 , \"tags\" : [ \"hunting\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"tasks\" : [ { \"id\" : \"~122896536\" , \"_id\" : \"~122896536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267741 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 }, { \"id\" : \"~81932320\" , \"_id\" : \"~81932320\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267743 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Firewall logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in firewall logs and look for IOcs of type IP, port\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 1 }, { \"id\" : \"~81928376\" , \"_id\" : \"~81928376\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267750 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Web proxy logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in web proxy logs and look for IOcs of type IP, domain, hostname, user-agent\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 2 } ], \"status\" : \"Ok\" , \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 , \"_id\" : \"~122900632\" } }, \"metrics\" : {} } { \"type\" : \"CreateError\" , \"message\" : \"The case template \\\"MISPEvent\\\" already exists\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Create"},{"location":"thehive/api/case-template/create/#create","text":"Create a Case Templates .","title":"Create"},{"location":"thehive/api/case-template/create/#query","text":"POST /api/case/template","title":"Query"},{"location":"thehive/api/case-template/create/#request-body-example","text":"{ \"name\" : \"MISPEvent\" , \"titlePrefix\" : \"\" , \"severity\" : 2 , \"tlp\" : 2 , \"pap\" : 2 , \"tags\" : [ \"hunting\" ], \"tasks\" : [ { \"order\" : 0 , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" }, { \"order\" : 1 , \"title\" : \"Search for IOCs on Firewall logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in firewall logs and look for IOcs of type IP, port\" }, { \"order\" : 2 , \"title\" : \"Search for IOCs on Web proxy logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in web proxy logs and look for IOcs of type IP, domain, hostname, user-agent\" } ], \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 } }, \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"displayName\" : \"MISP\" } name should be unique. Otherwise an error 400 Bad Request is returned","title":"Request Body Example"},{"location":"thehive/api/case-template/create/#response","text":"","title":"Response"},{"location":"thehive/api/case-template/create/#status-codes","text":"201 : if template was created successfully 400 : in case of error in input 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/case-template/create/#responsebody-example","text":"201 400 401 { \"_id\" : \"~910319824\" , \"id\" : \"~910319824\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267739 , \"_type\" : \"caseTemplate\" , \"name\" : \"MISPEvent\" , \"displayName\" : \"MISP\" , \"titlePrefix\" : \"[MISP]\" , \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"severity\" : 2 , \"tags\" : [ \"hunting\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"tasks\" : [ { \"id\" : \"~122896536\" , \"_id\" : \"~122896536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267741 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 }, { \"id\" : \"~81932320\" , \"_id\" : \"~81932320\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267743 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Firewall logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in firewall logs and look for IOcs of type IP, port\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 1 }, { \"id\" : \"~81928376\" , \"_id\" : \"~81928376\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267750 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Web proxy logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in web proxy logs and look for IOcs of type IP, domain, hostname, user-agent\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 2 } ], \"status\" : \"Ok\" , \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 , \"_id\" : \"~122900632\" } }, \"metrics\" : {} } { \"type\" : \"CreateError\" , \"message\" : \"The case template \\\"MISPEvent\\\" already exists\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/case-template/delete/","text":"Delete # Delete a Case Template by its id. Query # DELETE /api/case/template/{id} With: id : Case template identifier Response # Status codes # 200 : if Case Template is deleted successfully 401 : Authentication error 403 : Authorization error 404 : Case template does not exists (or was already deleted)","title":"Delete"},{"location":"thehive/api/case-template/delete/#delete","text":"Delete a Case Template by its id.","title":"Delete"},{"location":"thehive/api/case-template/delete/#query","text":"DELETE /api/case/template/{id} With: id : Case template identifier","title":"Query"},{"location":"thehive/api/case-template/delete/#response","text":"","title":"Response"},{"location":"thehive/api/case-template/delete/#status-codes","text":"200 : if Case Template is deleted successfully 401 : Authentication error 403 : Authorization error 404 : Case template does not exists (or was already deleted)","title":"Status codes"},{"location":"thehive/api/case-template/list/","text":"Get / List # List Case Templates of a given organisation. Query # POST /api/v1/query Request Body Example # { \"query\" : [ { \"_name\" : \"getOrganisation\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"caseTemplates\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"displayName\" : \"asc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } With: id : Organisation identifier of Name Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 401 403 [ ... { \"_id\" : \"~910319824\" , \"_type\" : \"CaseTemplate\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_updatedBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620297081745 , \"_updatedAt\" : 1620389292177 , \"name\" : \"Phishing\" , \"displayName\" : \"Phishing\" , \"titlePrefix\" : \"Phishing -\" , \"description\" : \"Phishing attempt has succeed.\" , \"severity\" : 2 , \"tags\" : [ \"category:Phishing\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"customFields\" : [], \"tasks\" : [ { \"_id\" : \"~677056528\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292172 , \"title\" : \"Initial alert\" , \"group\" : \"default\" , \"description\" : \"-What happened?\\n-When does it happened?\\n-How did it happened?\\n-How did we detected the anomaly/alert/incident?\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 , \"extraData\" : {} }, { \"_id\" : \"~677060624\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292173 , \"title\" : \"Remediation\" , \"group\" : \"default\" , \"description\" : \"Explain here all the actions performed to contain and remediate the threat.\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 3 , \"extraData\" : {} }, { \"_id\" : \"~677064720\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292174 , \"title\" : \"Lessons learnt\" , \"group\" : \"default\" , \"description\" : \"Write here the lessons learnt for the case.\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 4 , \"extraData\" : {} }, { \"_id\" : \"~706662512\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292171 , \"title\" : \"Notification / Communication\" , \"group\" : \"default\" , \"description\" : \"Write here all the communications related to this case\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 2 , \"extraData\" : {} }, { \"_id\" : \"~789033176\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292174 , \"title\" : \"Analysis\" , \"group\" : \"default\" , \"description\" : \"-Technical analysis of the incident\\n-Current impact\\n-Potential damages due to the incident\\n-...\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 1 , \"extraData\" : {} } ] } ... ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Get / List"},{"location":"thehive/api/case-template/list/#get-list","text":"List Case Templates of a given organisation.","title":"Get / List"},{"location":"thehive/api/case-template/list/#query","text":"POST /api/v1/query","title":"Query"},{"location":"thehive/api/case-template/list/#request-body-example","text":"{ \"query\" : [ { \"_name\" : \"getOrganisation\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"caseTemplates\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"displayName\" : \"asc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } With: id : Organisation identifier of Name","title":"Request Body Example"},{"location":"thehive/api/case-template/list/#response","text":"","title":"Response"},{"location":"thehive/api/case-template/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/case-template/list/#responsebody-example","text":"200 401 403 [ ... { \"_id\" : \"~910319824\" , \"_type\" : \"CaseTemplate\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_updatedBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620297081745 , \"_updatedAt\" : 1620389292177 , \"name\" : \"Phishing\" , \"displayName\" : \"Phishing\" , \"titlePrefix\" : \"Phishing -\" , \"description\" : \"Phishing attempt has succeed.\" , \"severity\" : 2 , \"tags\" : [ \"category:Phishing\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"customFields\" : [], \"tasks\" : [ { \"_id\" : \"~677056528\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292172 , \"title\" : \"Initial alert\" , \"group\" : \"default\" , \"description\" : \"-What happened?\\n-When does it happened?\\n-How did it happened?\\n-How did we detected the anomaly/alert/incident?\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 , \"extraData\" : {} }, { \"_id\" : \"~677060624\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292173 , \"title\" : \"Remediation\" , \"group\" : \"default\" , \"description\" : \"Explain here all the actions performed to contain and remediate the threat.\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 3 , \"extraData\" : {} }, { \"_id\" : \"~677064720\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292174 , \"title\" : \"Lessons learnt\" , \"group\" : \"default\" , \"description\" : \"Write here the lessons learnt for the case.\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 4 , \"extraData\" : {} }, { \"_id\" : \"~706662512\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292171 , \"title\" : \"Notification / Communication\" , \"group\" : \"default\" , \"description\" : \"Write here all the communications related to this case\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 2 , \"extraData\" : {} }, { \"_id\" : \"~789033176\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292174 , \"title\" : \"Analysis\" , \"group\" : \"default\" , \"description\" : \"-Technical analysis of the incident\\n-Current impact\\n-Potential damages due to the incident\\n-...\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 1 , \"extraData\" : {} } ] } ... ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/case-template/update/","text":"Update # Update a Case Template by its id. Query # PATCH /api/case/template/{id} Request Body Example # Example { \"displayName\" : \"New Display name\" , \"tlp\" : 4 , \"tasks\" : [ { \"order\" : 0 , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" } ] } Fields that can be updated: name displayName titlePrefix description severity tags flag tlp pap summary customFields tasks ResponseBody Example # Example { \"_id\" : \"~910319824\" , \"id\" : \"~910319824\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267739 , \"_type\" : \"caseTemplate\" , \"name\" : \"MISPEvent\" , \"displayName\" : \"New Display name\" , \"titlePrefix\" : \"[MISP]\" , \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"severity\" : 2 , \"tags\" : [ \"hunting\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"tasks\" : [ { \"id\" : \"~122896536\" , \"_id\" : \"~122896536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267741 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } ], \"status\" : \"Ok\" , \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 , \"_id\" : \"~122900632\" } }, \"metrics\" : {} }","title":"Update"},{"location":"thehive/api/case-template/update/#update","text":"Update a Case Template by its id.","title":"Update"},{"location":"thehive/api/case-template/update/#query","text":"PATCH /api/case/template/{id}","title":"Query"},{"location":"thehive/api/case-template/update/#request-body-example","text":"Example { \"displayName\" : \"New Display name\" , \"tlp\" : 4 , \"tasks\" : [ { \"order\" : 0 , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" } ] } Fields that can be updated: name displayName titlePrefix description severity tags flag tlp pap summary customFields tasks","title":"Request Body Example"},{"location":"thehive/api/case-template/update/#responsebody-example","text":"Example { \"_id\" : \"~910319824\" , \"id\" : \"~910319824\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267739 , \"_type\" : \"caseTemplate\" , \"name\" : \"MISPEvent\" , \"displayName\" : \"New Display name\" , \"titlePrefix\" : \"[MISP]\" , \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"severity\" : 2 , \"tags\" : [ \"hunting\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"tasks\" : [ { \"id\" : \"~122896536\" , \"_id\" : \"~122896536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267741 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } ], \"status\" : \"Ok\" , \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 , \"_id\" : \"~122900632\" } }, \"metrics\" : {} }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/","text":"Custom Field APIs # List custom fields Create a custom field Update custom field Delete a custom field Get a custom field Get custom field useage","title":"Overview"},{"location":"thehive/api/custom-field/#custom-field-apis","text":"List custom fields Create a custom field Update custom field Delete a custom field Get a custom field Get custom field useage","title":"Custom Field APIs"},{"location":"thehive/api/custom-field/create/","text":"Create # Create a Custom Field (requires manageCustomField permission). Query # POST /api/customField Request Body Example # { \"name\" : \"BusinesUnit\" , \"reference\" : \"businessunit\" , \"description\" : \"Targeted business unit\" , \"type\" : \"string\" , \"mandatory\" : false , \"options\" : [ \"VIP\" , \"HR\" , \"Security\" , \"Sys Administrators\" , \"Developers\" , \"Sales\" , \"Marketing\" , \"Procurement\" , \"Legal\" ] } The following fields are required: name : (String) reference : (String) description : (String) type : [string|integer|boolean|date|float] Response # Status codes # 201 : if Custom Fields is created successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 401 403 { \"id\" : \"~32912\" , \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"Sales\" , \"Marketing\" , \"VIP\" , \"Security\" , \"Sys admins\" , \"HR\" , \"Procurement\" , \"Legal\" ], \"mandatory\" : false } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Create"},{"location":"thehive/api/custom-field/create/#create","text":"Create a Custom Field (requires manageCustomField permission).","title":"Create"},{"location":"thehive/api/custom-field/create/#query","text":"POST /api/customField","title":"Query"},{"location":"thehive/api/custom-field/create/#request-body-example","text":"{ \"name\" : \"BusinesUnit\" , \"reference\" : \"businessunit\" , \"description\" : \"Targeted business unit\" , \"type\" : \"string\" , \"mandatory\" : false , \"options\" : [ \"VIP\" , \"HR\" , \"Security\" , \"Sys Administrators\" , \"Developers\" , \"Sales\" , \"Marketing\" , \"Procurement\" , \"Legal\" ] } The following fields are required: name : (String) reference : (String) description : (String) type : [string|integer|boolean|date|float]","title":"Request Body Example"},{"location":"thehive/api/custom-field/create/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/create/#status-codes","text":"201 : if Custom Fields is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/create/#responsebody-example","text":"201 401 403 { \"id\" : \"~32912\" , \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"Sales\" , \"Marketing\" , \"VIP\" , \"Security\" , \"Sys admins\" , \"HR\" , \"Procurement\" , \"Legal\" ], \"mandatory\" : false } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/delete/","text":"Delete # Delete a Custom Field (requires manageCustomField permission). Query # DELETE /api/customField/{id} with: id : id or name of the Custom Field. Response # Status codes # 204 : if Custom Fields is successfully deleted 401 : Authentication error 403 : Authorization error","title":"Delete"},{"location":"thehive/api/custom-field/delete/#delete","text":"Delete a Custom Field (requires manageCustomField permission).","title":"Delete"},{"location":"thehive/api/custom-field/delete/#query","text":"DELETE /api/customField/{id} with: id : id or name of the Custom Field.","title":"Query"},{"location":"thehive/api/custom-field/delete/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/delete/#status-codes","text":"204 : if Custom Fields is successfully deleted 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/get/","text":"Get # Get Custom Field by id; Query # GET /api/customField/{id} with: id : id or name of the custom field. Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 401 403 { \"id\" : \"~28672\" , \"name\" : \"Number of Accounts\" , \"reference\" : \"Number of Accounts\" , \"description\" : \"Number of accounts leaked\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Get"},{"location":"thehive/api/custom-field/get/#get","text":"Get Custom Field by id;","title":"Get"},{"location":"thehive/api/custom-field/get/#query","text":"GET /api/customField/{id} with: id : id or name of the custom field.","title":"Query"},{"location":"thehive/api/custom-field/get/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/get/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/get/#responsebody-example","text":"200 401 403 { \"id\" : \"~28672\" , \"name\" : \"Number of Accounts\" , \"reference\" : \"Number of Accounts\" , \"description\" : \"Number of accounts leaked\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/getUse/","text":"Use count # Get Custom Field use count by id. Query # GET /api/customField/{id}/use with: id : id or name of the custom field. Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 401 403 { \"case\" : 12 , \"alert\" : 1 , \"case_artifact\" : 9 , \"total\" : 22 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Use count"},{"location":"thehive/api/custom-field/getUse/#use-count","text":"Get Custom Field use count by id.","title":"Use count"},{"location":"thehive/api/custom-field/getUse/#query","text":"GET /api/customField/{id}/use with: id : id or name of the custom field.","title":"Query"},{"location":"thehive/api/custom-field/getUse/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/getUse/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/getUse/#responsebody-example","text":"200 401 403 { \"case\" : 12 , \"alert\" : 1 , \"case_artifact\" : 9 , \"total\" : 22 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/list/","text":"List # List Custom Fields . Query # GET /api/customField Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 401 403 [ { \"id\" : \"~28672\" , \"name\" : \"Number of Accounts\" , \"reference\" : \"Number of Accounts\" , \"description\" : \"Number of accounts leaked\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true }, { \"id\" : \"~53440\" , \"name\" : \"Nb of emails delivered\" , \"reference\" : \"Nb of emails delivered\" , \"description\" : \"Nb of emails delivered\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"List"},{"location":"thehive/api/custom-field/list/#list","text":"List Custom Fields .","title":"List"},{"location":"thehive/api/custom-field/list/#query","text":"GET /api/customField","title":"Query"},{"location":"thehive/api/custom-field/list/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/list/#responsebody-example","text":"200 401 403 [ { \"id\" : \"~28672\" , \"name\" : \"Number of Accounts\" , \"reference\" : \"Number of Accounts\" , \"description\" : \"Number of accounts leaked\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true }, { \"id\" : \"~53440\" , \"name\" : \"Nb of emails delivered\" , \"reference\" : \"Nb of emails delivered\" , \"description\" : \"Nb of emails delivered\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/update/","text":"Update # Update a Custom Field (requires manageCustomField permission). Query # PATCH /api/customField/{id} with: id : id or name of the custom field. Request Body Example # { \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"Sales\" , \"Marketing\" , \"VIP\" , \"Security\" , \"Sys admins\" , \"HR\" , \"Procurement\" , \"Legal\" ], \"mandatory\" : false } No fields are required. Response # Status codes # 200 : if Custom Fields is updated successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 401 403 { \"id\" : \"~32912\" , \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"HR\" , \"Legal\" , \"Marketing\" , \"Procurement\" , \"Sales\" , \"Security\" , \"Sys admins\" , \"VIP\" ], \"mandatory\" : false } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to update custom field, you haven't the permission manageCustomField\" }","title":"Update"},{"location":"thehive/api/custom-field/update/#update","text":"Update a Custom Field (requires manageCustomField permission).","title":"Update"},{"location":"thehive/api/custom-field/update/#query","text":"PATCH /api/customField/{id} with: id : id or name of the custom field.","title":"Query"},{"location":"thehive/api/custom-field/update/#request-body-example","text":"{ \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"Sales\" , \"Marketing\" , \"VIP\" , \"Security\" , \"Sys admins\" , \"HR\" , \"Procurement\" , \"Legal\" ], \"mandatory\" : false } No fields are required.","title":"Request Body Example"},{"location":"thehive/api/custom-field/update/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/update/#status-codes","text":"200 : if Custom Fields is updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/update/#responsebody-example","text":"201 401 403 { \"id\" : \"~32912\" , \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"HR\" , \"Legal\" , \"Marketing\" , \"Procurement\" , \"Sales\" , \"Security\" , \"Sys admins\" , \"VIP\" ], \"mandatory\" : false } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to update custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/dashboard/create/","text":"","title":"Create"},{"location":"thehive/api/dashboard/update/","text":"Update # Query # Request Body Example # ResponseBody Example #","title":"Update"},{"location":"thehive/api/dashboard/update/#update","text":"","title":"Update"},{"location":"thehive/api/dashboard/update/#query","text":"","title":"Query"},{"location":"thehive/api/dashboard/update/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/dashboard/update/#responsebody-example","text":"","title":"ResponseBody Example"},{"location":"thehive/api/observable/","text":"Observable APIs # List observables Create observable Update observable Delete observable Run analyzer in observable Run responder in observable","title":"Overview"},{"location":"thehive/api/observable/#observable-apis","text":"List observables Create observable Update observable Delete observable Run analyzer in observable Run responder in observable","title":"Observable APIs"},{"location":"thehive/api/observable/analyzer/","text":"Analyzer # You need to connect TheHive to a cortex server in order to enable analyzers. Attention Analyzer can only be run on an observable of a case. Run an analyzer on an observable # POST /api/connector/cortex/job Request example # { \"cortexId\" : \"Stable\" , \"artifactId\" : \"~816984288\" , \"analyzerId\" : \"Abuse_Finder_3_0\" } The following fields are required: - cortexId : name of the cortex server from the configuration - artifactId : id of the observable to analyze - analyzerId : id of the cortex analyzer to use Responseexample # { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Waiting\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660394582 , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } Get analyzer report # GET /api/connector/job/{jobId} jobId should be the id returned from the creation request Responseexample # Example { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Success\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660427845 , \"report\" : { \"success\" : true , \"full\" : { \"abuse_finder\" : { \"value\" : \"1.2.3.4\" , \"names\" : [ \"APNIC Debogon Project\" ], \"abuse\" : [ \"helpdesk@apnic.net\" ], \"raw\" : \"% [whois.apnic.net]\\n% Whois data copyright terms http://www.apnic.net/db/dbcopyright.html\\n\\n% Inf...\" } }, \"artifacts\" : [] }, \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } status can be one of: Waiting Success InProgress Failure Deleted List reports for an observable # POST /api/v1/query Query body example # Replace the value of idOrName by the id of your observable Example { \"query\" : [ { \"_name\" : \"getObservable\" , \"idOrName\" : \"~816984288\" }, { \"_name\" : \"jobs\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 200 } ] } Responseexample # Example [ { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Waiting\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660394582 , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } ]","title":"Analyzer"},{"location":"thehive/api/observable/analyzer/#analyzer","text":"You need to connect TheHive to a cortex server in order to enable analyzers. Attention Analyzer can only be run on an observable of a case.","title":"Analyzer"},{"location":"thehive/api/observable/analyzer/#run-an-analyzer-on-an-observable","text":"POST /api/connector/cortex/job","title":"Run an analyzer on an observable"},{"location":"thehive/api/observable/analyzer/#request-example","text":"{ \"cortexId\" : \"Stable\" , \"artifactId\" : \"~816984288\" , \"analyzerId\" : \"Abuse_Finder_3_0\" } The following fields are required: - cortexId : name of the cortex server from the configuration - artifactId : id of the observable to analyze - analyzerId : id of the cortex analyzer to use","title":"Request example"},{"location":"thehive/api/observable/analyzer/#responseexample","text":"{ \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Waiting\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660394582 , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" }","title":"Responseexample"},{"location":"thehive/api/observable/analyzer/#get-analyzer-report","text":"GET /api/connector/job/{jobId} jobId should be the id returned from the creation request","title":"Get analyzer report"},{"location":"thehive/api/observable/analyzer/#responseexample_1","text":"Example { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Success\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660427845 , \"report\" : { \"success\" : true , \"full\" : { \"abuse_finder\" : { \"value\" : \"1.2.3.4\" , \"names\" : [ \"APNIC Debogon Project\" ], \"abuse\" : [ \"helpdesk@apnic.net\" ], \"raw\" : \"% [whois.apnic.net]\\n% Whois data copyright terms http://www.apnic.net/db/dbcopyright.html\\n\\n% Inf...\" } }, \"artifacts\" : [] }, \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } status can be one of: Waiting Success InProgress Failure Deleted","title":"Responseexample"},{"location":"thehive/api/observable/analyzer/#list-reports-for-an-observable","text":"POST /api/v1/query","title":"List reports for an observable"},{"location":"thehive/api/observable/analyzer/#query-body-example","text":"Replace the value of idOrName by the id of your observable Example { \"query\" : [ { \"_name\" : \"getObservable\" , \"idOrName\" : \"~816984288\" }, { \"_name\" : \"jobs\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 200 } ] }","title":"Query body example"},{"location":"thehive/api/observable/analyzer/#responseexample_2","text":"Example [ { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Waiting\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660394582 , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } ]","title":"Responseexample"},{"location":"thehive/api/observable/create/","text":"Create # Creates an observable which can be linked to a case or an alert. Note (The name artifcat comes from TheHive v3) Query # Create an observable for a case POST /api/v0/case/{caseId}/artifact Create an observable for an alert POST /api/v0/alert/{alertId}/artifact Request Example # Observable with attachment Observables without atttachment If you want to upload an observable with a dataType of kind attachment, you need to use a multipart request curl -XPOST http://THEHIVE/api/v0/case/{caseId}/artifact -F attachment=@myFile -F _json=' { \"dataType\": \"file\", \"tlp\": 2, \"ioc\": true, \"sighted\": false, \"tags\": [], \"message\": \"foo\", \"data\": [], \"isZip\": false, \"zipPassword\": \"\" } ' To add an observable with no attachment, you can post a json body { \"dataType\" : \"hostname\" , \"tlp\" : 2 , \"ioc\" : true , \"sighted\" : true , \"tags\" : [], \"data\" : [ \"server.local\" ] } The following fields are required: dataType : (enum String, should be one registered observable type) One of data (Array of String) or attachment (File) Other optional fields: message : (String) description of the observable in the context of the case startDate : (Date) date of the observable creation default=now tlp : (Int) TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 tags : (Array of string) a list of tags default=[] ioc : (Boolean) indicates if the observable is an IOC default=false sighted : (Boolean) indicates if the observable was sighted default=false ignoreSimilarity : (Boolean) indicates if the observable should be used or not to calculate the similarity stats default=false ResponseBody Example # Observables with atttachment Observables without atttachment [ { \"_id\" : \"~4104\" , \"id\" : \"~4104\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630508511351 , \"_type\" : \"case_artifact\" , \"dataType\" : \"file\" , \"startDate\" : 1630508511351 , \"attachment\" : { \"name\" : \"server.log\" , \"hashes\" : [ \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" , \"a09531845b3b26d5707cdf50a8bb11aa507dd88c\" , \"1f08c024363568d6eb4e18ee97618acc\" ], \"size\" : 37165 , \"contentType\" : \"application/octet-stream\" , \"id\" : \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" }, \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"message\" : \"my message\" , \"reports\" : {}, \"stats\" : {} } ] [ { \"_id\" : \"~4104\" , \"id\" : \"~4104\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630508511351 , \"_type\" : \"case_artifact\" , \"ip\" : \"file\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1630508511351 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"message\" : \"my message\" , \"reports\" : {}, \"stats\" : {} } ]","title":"Create"},{"location":"thehive/api/observable/create/#create","text":"Creates an observable which can be linked to a case or an alert. Note (The name artifcat comes from TheHive v3)","title":"Create"},{"location":"thehive/api/observable/create/#query","text":"Create an observable for a case POST /api/v0/case/{caseId}/artifact Create an observable for an alert POST /api/v0/alert/{alertId}/artifact","title":"Query"},{"location":"thehive/api/observable/create/#request-example","text":"Observable with attachment Observables without atttachment If you want to upload an observable with a dataType of kind attachment, you need to use a multipart request curl -XPOST http://THEHIVE/api/v0/case/{caseId}/artifact -F attachment=@myFile -F _json=' { \"dataType\": \"file\", \"tlp\": 2, \"ioc\": true, \"sighted\": false, \"tags\": [], \"message\": \"foo\", \"data\": [], \"isZip\": false, \"zipPassword\": \"\" } ' To add an observable with no attachment, you can post a json body { \"dataType\" : \"hostname\" , \"tlp\" : 2 , \"ioc\" : true , \"sighted\" : true , \"tags\" : [], \"data\" : [ \"server.local\" ] } The following fields are required: dataType : (enum String, should be one registered observable type) One of data (Array of String) or attachment (File) Other optional fields: message : (String) description of the observable in the context of the case startDate : (Date) date of the observable creation default=now tlp : (Int) TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 tags : (Array of string) a list of tags default=[] ioc : (Boolean) indicates if the observable is an IOC default=false sighted : (Boolean) indicates if the observable was sighted default=false ignoreSimilarity : (Boolean) indicates if the observable should be used or not to calculate the similarity stats default=false","title":"Request Example"},{"location":"thehive/api/observable/create/#responsebody-example","text":"Observables with atttachment Observables without atttachment [ { \"_id\" : \"~4104\" , \"id\" : \"~4104\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630508511351 , \"_type\" : \"case_artifact\" , \"dataType\" : \"file\" , \"startDate\" : 1630508511351 , \"attachment\" : { \"name\" : \"server.log\" , \"hashes\" : [ \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" , \"a09531845b3b26d5707cdf50a8bb11aa507dd88c\" , \"1f08c024363568d6eb4e18ee97618acc\" ], \"size\" : 37165 , \"contentType\" : \"application/octet-stream\" , \"id\" : \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" }, \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"message\" : \"my message\" , \"reports\" : {}, \"stats\" : {} } ] [ { \"_id\" : \"~4104\" , \"id\" : \"~4104\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630508511351 , \"_type\" : \"case_artifact\" , \"ip\" : \"file\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1630508511351 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"message\" : \"my message\" , \"reports\" : {}, \"stats\" : {} } ]","title":"ResponseBody Example"},{"location":"thehive/api/observable/delete/","text":"Delete # Delete a case or alert Observable by its id Query # DELETE /api/v0/case/artifact/{observableId} DELETE /api/v0/alert/artifact/{observableId} Response # 204 No Content","title":"Delete"},{"location":"thehive/api/observable/delete/#delete","text":"Delete a case or alert Observable by its id","title":"Delete"},{"location":"thehive/api/observable/delete/#query","text":"DELETE /api/v0/case/artifact/{observableId} DELETE /api/v0/alert/artifact/{observableId}","title":"Query"},{"location":"thehive/api/observable/delete/#response","text":"204 No Content","title":"Response"},{"location":"thehive/api/observable/list/","text":"List / Search # Query # POST /api/v1/query Request Body Example # List last 30 observables for a case: { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{caseId}\" }, { \"_name\" : \"observables\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 30 } ] } ResponseExample # [ { \"_id\" : \"~122884120\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"foo@local.io\" , \"_updatedBy\" : \"foo@local.io\" , \"_createdAt\" : 1630509659446 , \"_updatedAt\" : 1630511666911 , \"dataType\" : \"hostname\" , \"data\" : \"server.local\" , \"startDate\" : 1630509659446 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"reports\" : {}, \"message\" : \"myMessage\" , \"extraData\" : {} }, { \"_id\" : \"~4104\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"foo@local.io\" , \"_createdAt\" : 1630508511351 , \"dataType\" : \"file\" , \"startDate\" : 1630508511351 , \"attachment\" : { \"_id\" : \"~40964280\" , \"_type\" : \"Attachment\" , \"_createdBy\" : \"foo@local.io\" , \"_createdAt\" : 1630508511313 , \"name\" : \"server.log\" , \"hashes\" : [ \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" , \"a09531845b3b26d5707cdf50a8bb11aa507dd88c\" , \"1f08c024363568d6eb4e18ee97618acc\" ], \"size\" : 37165 , \"contentType\" : \"application/octet-stream\" , \"id\" : \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" }, \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"reports\" : {}, \"message\" : \"foo\" , \"extraData\" : {} } ]","title":"List / Search"},{"location":"thehive/api/observable/list/#list-search","text":"","title":"List / Search"},{"location":"thehive/api/observable/list/#query","text":"POST /api/v1/query","title":"Query"},{"location":"thehive/api/observable/list/#request-body-example","text":"List last 30 observables for a case: { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{caseId}\" }, { \"_name\" : \"observables\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 30 } ] }","title":"Request Body Example"},{"location":"thehive/api/observable/list/#responseexample","text":"[ { \"_id\" : \"~122884120\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"foo@local.io\" , \"_updatedBy\" : \"foo@local.io\" , \"_createdAt\" : 1630509659446 , \"_updatedAt\" : 1630511666911 , \"dataType\" : \"hostname\" , \"data\" : \"server.local\" , \"startDate\" : 1630509659446 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"reports\" : {}, \"message\" : \"myMessage\" , \"extraData\" : {} }, { \"_id\" : \"~4104\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"foo@local.io\" , \"_createdAt\" : 1630508511351 , \"dataType\" : \"file\" , \"startDate\" : 1630508511351 , \"attachment\" : { \"_id\" : \"~40964280\" , \"_type\" : \"Attachment\" , \"_createdBy\" : \"foo@local.io\" , \"_createdAt\" : 1630508511313 , \"name\" : \"server.log\" , \"hashes\" : [ \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" , \"a09531845b3b26d5707cdf50a8bb11aa507dd88c\" , \"1f08c024363568d6eb4e18ee97618acc\" ], \"size\" : 37165 , \"contentType\" : \"application/octet-stream\" , \"id\" : \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" }, \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"reports\" : {}, \"message\" : \"foo\" , \"extraData\" : {} } ]","title":"ResponseExample"},{"location":"thehive/api/observable/responder/","text":"Responder # You need to connect TheHive to a cortex server in order to enable responders. Attention Responder can only be run on an observable of a case. Run a responder on an observable # POST /api/connector/cortex/action Request example # { \"cortexId\" : \"Stable\" , \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"objectType\" : \"case_artifact\" , \"objectId\" : \"~816984288\" } The following fields are required: cortexId : name of the cortex server from the configuration objectType : should be case_artifact here objectId : id of the observable to analyze responderId : id of the cortex responder to use Responseexample # { \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"responderName\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"responderDefinition\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"Bv0cq3sB8Pn57ilsUkFM\" , \"objectType\" : \"Observable\" , \"objectId\" : \"~816984288\" , \"status\" : \"Waiting\" , \"startDate\" : 1630663366136 , \"operations\" : \"[]\" , \"report\" : \"{}\" } List responder actions # GET /api/connector/cortex/action/case_artifact/{observableId} Responseexample # Example [ { \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"responderName\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"responderDefinition\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"Bv0cq3sB8Pn57ilsUkFM\" , \"objectType\" : \"Observable\" , \"objectId\" : \"~816984288\" , \"status\" : \"Failure\" , \"startDate\" : 1630663366136 , \"endDate\" : 1630663372393 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":false,\\\"artifacts\\\":[],\\\"operations\\\":[],...}\" } ] status can be one of: Waiting Success InProgress Failure Deleted report is a string that contains the output of the responder","title":"Responder"},{"location":"thehive/api/observable/responder/#responder","text":"You need to connect TheHive to a cortex server in order to enable responders. Attention Responder can only be run on an observable of a case.","title":"Responder"},{"location":"thehive/api/observable/responder/#run-a-responder-on-an-observable","text":"POST /api/connector/cortex/action","title":"Run a responder on an observable"},{"location":"thehive/api/observable/responder/#request-example","text":"{ \"cortexId\" : \"Stable\" , \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"objectType\" : \"case_artifact\" , \"objectId\" : \"~816984288\" } The following fields are required: cortexId : name of the cortex server from the configuration objectType : should be case_artifact here objectId : id of the observable to analyze responderId : id of the cortex responder to use","title":"Request example"},{"location":"thehive/api/observable/responder/#responseexample","text":"{ \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"responderName\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"responderDefinition\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"Bv0cq3sB8Pn57ilsUkFM\" , \"objectType\" : \"Observable\" , \"objectId\" : \"~816984288\" , \"status\" : \"Waiting\" , \"startDate\" : 1630663366136 , \"operations\" : \"[]\" , \"report\" : \"{}\" }","title":"Responseexample"},{"location":"thehive/api/observable/responder/#list-responder-actions","text":"GET /api/connector/cortex/action/case_artifact/{observableId}","title":"List responder actions"},{"location":"thehive/api/observable/responder/#responseexample_1","text":"Example [ { \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"responderName\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"responderDefinition\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"Bv0cq3sB8Pn57ilsUkFM\" , \"objectType\" : \"Observable\" , \"objectId\" : \"~816984288\" , \"status\" : \"Failure\" , \"startDate\" : 1630663366136 , \"endDate\" : 1630663372393 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":false,\\\"artifacts\\\":[],\\\"operations\\\":[],...}\" } ] status can be one of: Waiting Success InProgress Failure Deleted report is a string that contains the output of the responder","title":"Responseexample"},{"location":"thehive/api/observable/update/","text":"Update # Update a case or alert Observable by its id Query # PATCH /api/v0/case/artifact/{observableId} PATCH /api/v0/alert/artifact/{observableId} Request Body Example # { \"sighted\" : true , \"ioc\" : true , \"message\" : \"This observable was sighted\" } Fields that can be updated: ioc sighted ignoreSimilarity tags message tlp Once an observable is created, it is not possible to change its type or data ResponseBody Example # { \"_id\" : \"~122884120\" , \"id\" : \"~122884120\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"lydia@strangebee.com\" , \"createdAt\" : 1630509659446 , \"updatedAt\" : 1630511666911 , \"_type\" : \"case_artifact\" , \"dataType\" : \"hostname\" , \"data\" : \"server.local\" , \"startDate\" : 1630509659446 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : true , \"message\" : \"This observable was sighted\" , \"reports\" : {}, \"stats\" : {} }","title":"Update"},{"location":"thehive/api/observable/update/#update","text":"Update a case or alert Observable by its id","title":"Update"},{"location":"thehive/api/observable/update/#query","text":"PATCH /api/v0/case/artifact/{observableId} PATCH /api/v0/alert/artifact/{observableId}","title":"Query"},{"location":"thehive/api/observable/update/#request-body-example","text":"{ \"sighted\" : true , \"ioc\" : true , \"message\" : \"This observable was sighted\" } Fields that can be updated: ioc sighted ignoreSimilarity tags message tlp Once an observable is created, it is not possible to change its type or data","title":"Request Body Example"},{"location":"thehive/api/observable/update/#responsebody-example","text":"{ \"_id\" : \"~122884120\" , \"id\" : \"~122884120\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"lydia@strangebee.com\" , \"createdAt\" : 1630509659446 , \"updatedAt\" : 1630511666911 , \"_type\" : \"case_artifact\" , \"dataType\" : \"hostname\" , \"data\" : \"server.local\" , \"startDate\" : 1630509659446 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : true , \"message\" : \"This observable was sighted\" , \"reports\" : {}, \"stats\" : {} }","title":"ResponseBody Example"},{"location":"thehive/api/organisation/","text":"Organisation APIs # List organisations Create organisation Update organisation List organisation links Set organisation links","title":"Overview"},{"location":"thehive/api/organisation/#organisation-apis","text":"List organisations Create organisation Update organisation List organisation links Set organisation links","title":"Organisation APIs"},{"location":"thehive/api/organisation/create/","text":"Create # API to create a new TheHive organisation. Query # POST /api/v0/organisation Authorization # This API requires a super admin user with manageOrganisation permission Request # Request Body Example # { \"description\" : \"SOC team\" , \"name\" : \"soc\" } Fields # The following fields are required: name : (String) description : (String) Response # Status codes # 201 : if organisation creation completed successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 401 403 { \"_id\" : \"~204804296\" , \"_type\" : \"organisation\" , \"createdAt\" : 1630385478884 , \"createdBy\" : \"admin@thehive.local\" , \"description\" : \"SOC team\" , \"id\" : \"~204804296\" , \"links\" : [], \"name\" : \"soc\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Unauthorized action\" }","title":"Create"},{"location":"thehive/api/organisation/create/#create","text":"API to create a new TheHive organisation.","title":"Create"},{"location":"thehive/api/organisation/create/#query","text":"POST /api/v0/organisation","title":"Query"},{"location":"thehive/api/organisation/create/#authorization","text":"This API requires a super admin user with manageOrganisation permission","title":"Authorization"},{"location":"thehive/api/organisation/create/#request","text":"","title":"Request"},{"location":"thehive/api/organisation/create/#request-body-example","text":"{ \"description\" : \"SOC team\" , \"name\" : \"soc\" }","title":"Request Body Example"},{"location":"thehive/api/organisation/create/#fields","text":"The following fields are required: name : (String) description : (String)","title":"Fields"},{"location":"thehive/api/organisation/create/#response","text":"","title":"Response"},{"location":"thehive/api/organisation/create/#status-codes","text":"201 : if organisation creation completed successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/organisation/create/#responsebody-example","text":"200 401 403 { \"_id\" : \"~204804296\" , \"_type\" : \"organisation\" , \"createdAt\" : 1630385478884 , \"createdBy\" : \"admin@thehive.local\" , \"description\" : \"SOC team\" , \"id\" : \"~204804296\" , \"links\" : [], \"name\" : \"soc\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Unauthorized action\" }","title":"ResponseBody Example"},{"location":"thehive/api/organisation/list-links/","text":"List links # Query # GET /api/v0/organisation/{idOrName}/links with: idOrName id or name of the organisation Response # Status codes # 200 : if organisation exists 404 : if organisation doesn't exist","title":"List links"},{"location":"thehive/api/organisation/list-links/#list-links","text":"","title":"List links"},{"location":"thehive/api/organisation/list-links/#query","text":"GET /api/v0/organisation/{idOrName}/links with: idOrName id or name of the organisation","title":"Query"},{"location":"thehive/api/organisation/list-links/#response","text":"","title":"Response"},{"location":"thehive/api/organisation/list-links/#status-codes","text":"200 : if organisation exists 404 : if organisation doesn't exist","title":"Status codes"},{"location":"thehive/api/organisation/list/","text":"List/Search # List Organisations . Query # GET /api/v0/query Request # This is a Query API call, where: listOrganisation is the query name { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_fields\" : [ { \"updatedAt\" : \"desc\" } ], \"_name\" : \"sort\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # 200 401 [ { \"_createdAt\" : 1630385478884 , \"_createdBy\" : \"admin@thehive.local\" , \"_id\" : \"~204804296\" , \"_type\" : \"Organisation\" , \"_updatedAt\" : 1630415216098 , \"_updatedBy\" : \"admin@thehive.local\" , \"description\" : \"SOC level\" , \"links\" : [ \"cert\" ], \"name\" : \"soc-level1\" }, { \"_createdAt\" : 1606467059596 , \"_createdBy\" : \"admin@thehive.local\" , \"_id\" : \"~4144\" , \"_type\" : \"Organisation\" , \"description\" : \"CERT\" , \"links\" : [ \"soc-level1\" ], \"name\" : \"cert\" }, { \"_createdAt\" : 1606464802479 , \"_createdBy\" : \"system@thehive.local\" , \"_id\" : \"~8408\" , \"_type\" : \"Organisation\" , \"description\" : \"organisation for administration\" , \"links\" : [], \"name\" : \"admin\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List/Search"},{"location":"thehive/api/organisation/list/#listsearch","text":"List Organisations .","title":"List/Search"},{"location":"thehive/api/organisation/list/#query","text":"GET /api/v0/query","title":"Query"},{"location":"thehive/api/organisation/list/#request","text":"This is a Query API call, where: listOrganisation is the query name { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_fields\" : [ { \"updatedAt\" : \"desc\" } ], \"_name\" : \"sort\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Request"},{"location":"thehive/api/organisation/list/#response","text":"","title":"Response"},{"location":"thehive/api/organisation/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/organisation/list/#responsebody-example","text":"200 401 [ { \"_createdAt\" : 1630385478884 , \"_createdBy\" : \"admin@thehive.local\" , \"_id\" : \"~204804296\" , \"_type\" : \"Organisation\" , \"_updatedAt\" : 1630415216098 , \"_updatedBy\" : \"admin@thehive.local\" , \"description\" : \"SOC level\" , \"links\" : [ \"cert\" ], \"name\" : \"soc-level1\" }, { \"_createdAt\" : 1606467059596 , \"_createdBy\" : \"admin@thehive.local\" , \"_id\" : \"~4144\" , \"_type\" : \"Organisation\" , \"description\" : \"CERT\" , \"links\" : [ \"soc-level1\" ], \"name\" : \"cert\" }, { \"_createdAt\" : 1606464802479 , \"_createdBy\" : \"system@thehive.local\" , \"_id\" : \"~8408\" , \"_type\" : \"Organisation\" , \"description\" : \"organisation for administration\" , \"links\" : [], \"name\" : \"admin\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/organisation/update-links/","text":"Update links # Link orgnisation to one or many other organisations. It sets the list of organisation link to the list provided as input. It overrides the existing list of links. Query # PUT /api/v0/organisation/{idOrName}/links with: idOrName id or name of the organisation Request # Request Body Example # { \"organisations\" : [ \"cert\" , \"csirt\" ] } Fields # organisations ( required ): Array of organisation names Response # Status codes # 201 if the operation completed successfully","title":"Update links"},{"location":"thehive/api/organisation/update-links/#update-links","text":"Link orgnisation to one or many other organisations. It sets the list of organisation link to the list provided as input. It overrides the existing list of links.","title":"Update links"},{"location":"thehive/api/organisation/update-links/#query","text":"PUT /api/v0/organisation/{idOrName}/links with: idOrName id or name of the organisation","title":"Query"},{"location":"thehive/api/organisation/update-links/#request","text":"","title":"Request"},{"location":"thehive/api/organisation/update-links/#request-body-example","text":"{ \"organisations\" : [ \"cert\" , \"csirt\" ] }","title":"Request Body Example"},{"location":"thehive/api/organisation/update-links/#fields","text":"organisations ( required ): Array of organisation names","title":"Fields"},{"location":"thehive/api/organisation/update-links/#response","text":"","title":"Response"},{"location":"thehive/api/organisation/update-links/#status-codes","text":"201 if the operation completed successfully","title":"Status codes"},{"location":"thehive/api/organisation/update/","text":"Update # Query # PATCH /api/v0/organisation/{id} with: id : id or name of the organisation. Authorization # This API requires a super admin user with manageOrganisation permission Request Body Example # { \"description\" : \"SOC level 1 team\" , \"name\" : \"soc-level1\" } Fields # The following fields are editable: name (String) description (String) Response # 204 : if the organisation is updated successfully 401 : Authentication error 403 : Authorization error","title":"Update"},{"location":"thehive/api/organisation/update/#update","text":"","title":"Update"},{"location":"thehive/api/organisation/update/#query","text":"PATCH /api/v0/organisation/{id} with: id : id or name of the organisation.","title":"Query"},{"location":"thehive/api/organisation/update/#authorization","text":"This API requires a super admin user with manageOrganisation permission","title":"Authorization"},{"location":"thehive/api/organisation/update/#request-body-example","text":"{ \"description\" : \"SOC level 1 team\" , \"name\" : \"soc-level1\" }","title":"Request Body Example"},{"location":"thehive/api/organisation/update/#fields","text":"The following fields are editable: name (String) description (String)","title":"Fields"},{"location":"thehive/api/organisation/update/#response","text":"204 : if the organisation is updated successfully 401 : Authentication error 403 : Authorization error","title":"Response"},{"location":"thehive/api/search/","text":"Search APIs # Build queries","title":"Search APIs"},{"location":"thehive/api/search/#search-apis","text":"Build queries","title":"Search APIs"},{"location":"thehive/api/search/filters/","text":"","title":"Filters"},{"location":"thehive/api/search/pagination/","text":"","title":"Pagination"},{"location":"thehive/api/search/query/","text":"Query API # Overview # The Query API is the API used to search for objects with filtering and sorting capabilities. It's an API introduced by TheHive 4 and is optimized for the the new data model. TheHive comes with a list of predefined search Queries like: listOrganisation listUser listAlert listCase Query # POST /api/v0/query Request Body # The Query API request body should be an array of operations of different types: Selection: Required list of objects object by identifier Filtering: optional Sorting: optional Pagination: optional Formatting: optional Examples Simple List List with filters List with filters and sort List with pagination { \"query\" : [ { \"_name\" : \"listOrganisation\" } ] } List organisations called admin { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" } ] } List organisations called admin , sorted by ascendant updatedAt value { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" }, { \"_fields\" : [ { \"updatedAt\" : \"asc\" } ], \"_name\" : \"sort\" } ] } List organisations called admin , sorted by ascendant updatedAt value, paginated to display the first 15 items { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" }, { \"_fields\" : [ { \"updatedAt\" : \"asc\" } ], \"_name\" : \"sort\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Query API"},{"location":"thehive/api/search/query/#query-api","text":"","title":"Query API"},{"location":"thehive/api/search/query/#overview","text":"The Query API is the API used to search for objects with filtering and sorting capabilities. It's an API introduced by TheHive 4 and is optimized for the the new data model. TheHive comes with a list of predefined search Queries like: listOrganisation listUser listAlert listCase","title":"Overview"},{"location":"thehive/api/search/query/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/search/query/#request-body","text":"The Query API request body should be an array of operations of different types: Selection: Required list of objects object by identifier Filtering: optional Sorting: optional Pagination: optional Formatting: optional Examples Simple List List with filters List with filters and sort List with pagination { \"query\" : [ { \"_name\" : \"listOrganisation\" } ] } List organisations called admin { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" } ] } List organisations called admin , sorted by ascendant updatedAt value { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" }, { \"_fields\" : [ { \"updatedAt\" : \"asc\" } ], \"_name\" : \"sort\" } ] } List organisations called admin , sorted by ascendant updatedAt value, paginated to display the first 15 items { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" }, { \"_fields\" : [ { \"updatedAt\" : \"asc\" } ], \"_name\" : \"sort\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Request Body"},{"location":"thehive/api/search/sorting/","text":"","title":"Sorting"},{"location":"thehive/api/task/","text":"Case task APIs # Case task operations # List case tasks Create task Update task Get task details Run responder List responder jobs Case task log oprations # List task logs Create task log Delete task log Run responder on log List responder jobs on log Global task operations # List waiting tasks","title":"Overview"},{"location":"thehive/api/task/#case-task-apis","text":"","title":"Case task APIs"},{"location":"thehive/api/task/#case-task-operations","text":"List case tasks Create task Update task Get task details Run responder List responder jobs","title":"Case task operations"},{"location":"thehive/api/task/#case-task-log-oprations","text":"List task logs Create task log Delete task log Run responder on log List responder jobs on log","title":"Case task log oprations"},{"location":"thehive/api/task/#global-task-operations","text":"List waiting tasks","title":"Global task operations"},{"location":"thehive/api/task/create-log/","text":"Add log # Add a Log to an existing task (requires manageTask permission). Query # POST /api/case/task/{id}/log With: id : Task identifier Request Body Example # { \"message\" : \"The sandbox hasn't detected any suspicious activity\" , \"startDate\" : 1630683608000 , } The only required field is message . If you want to attach a file to the log, you need to use a multipart request curl -XPOST http://THEHIVE/api/v0/case/task/{taskId}/log -F attachment=@report.pdf -F _json=' { \"message\": \"The sandbox report\" } ' Response # Status codes # 201 : if Log is created successfully 401 : Authentication error 403 : Authorization error Response Body Example # 201 401 403 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\u00e7log\" , \"message\" : \"The sandbox hasn't detected any suspicious activity\" , \"startDate\" : 1630683608000 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create Log, you haven't the permission manageTask\" }","title":"Add log"},{"location":"thehive/api/task/create-log/#add-log","text":"Add a Log to an existing task (requires manageTask permission).","title":"Add log"},{"location":"thehive/api/task/create-log/#query","text":"POST /api/case/task/{id}/log With: id : Task identifier","title":"Query"},{"location":"thehive/api/task/create-log/#request-body-example","text":"{ \"message\" : \"The sandbox hasn't detected any suspicious activity\" , \"startDate\" : 1630683608000 , } The only required field is message . If you want to attach a file to the log, you need to use a multipart request curl -XPOST http://THEHIVE/api/v0/case/task/{taskId}/log -F attachment=@report.pdf -F _json=' { \"message\": \"The sandbox report\" } '","title":"Request Body Example"},{"location":"thehive/api/task/create-log/#response","text":"","title":"Response"},{"location":"thehive/api/task/create-log/#status-codes","text":"201 : if Log is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/task/create-log/#response-body-example","text":"201 401 403 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\u00e7log\" , \"message\" : \"The sandbox hasn't detected any suspicious activity\" , \"startDate\" : 1630683608000 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create Log, you haven't the permission manageTask\" }","title":"Response Body Example"},{"location":"thehive/api/task/create/","text":"Create # Create a Task (requires manageTask permission). Query # POST /api/case/{id}task With: id : Case identifier Request Body Example # { \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } The only required field is title . The status can be Waiting , InProgress , Completed or Cancel . Response # Status codes # 201 : if Tasks is created successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 401 403 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create Task, you haven't the permission manageTask\" }","title":"Create"},{"location":"thehive/api/task/create/#create","text":"Create a Task (requires manageTask permission).","title":"Create"},{"location":"thehive/api/task/create/#query","text":"POST /api/case/{id}task With: id : Case identifier","title":"Query"},{"location":"thehive/api/task/create/#request-body-example","text":"{ \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } The only required field is title . The status can be Waiting , InProgress , Completed or Cancel .","title":"Request Body Example"},{"location":"thehive/api/task/create/#response","text":"","title":"Response"},{"location":"thehive/api/task/create/#status-codes","text":"201 : if Tasks is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/task/create/#responsebody-example","text":"201 401 403 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create Task, you haven't the permission manageTask\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/delete-log/","text":"Delete log # Delete a Log of an existing task (requires manageTask permission). Query # DELETE /api/case/task/log/{id} With: id : Log identifier Response # Status codes # 204 : if Log is deleted successfully 401 : Authentication error 403 : Authorization error","title":"Delete log"},{"location":"thehive/api/task/delete-log/#delete-log","text":"Delete a Log of an existing task (requires manageTask permission).","title":"Delete log"},{"location":"thehive/api/task/delete-log/#query","text":"DELETE /api/case/task/log/{id} With: id : Log identifier","title":"Query"},{"location":"thehive/api/task/delete-log/#response","text":"","title":"Response"},{"location":"thehive/api/task/delete-log/#status-codes","text":"204 : if Log is deleted successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/task/get/","text":"Get case task # Get Task of a case. Query # GET /api/case/task/{id} with: id : id of the task. Response # Status codes # 200 : if query is run successfully 401 : Authentication error 404 : The Task is not found ResponseBody Example # 201 401 404 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Get case task"},{"location":"thehive/api/task/get/#get-case-task","text":"Get Task of a case.","title":"Get case task"},{"location":"thehive/api/task/get/#query","text":"GET /api/case/task/{id} with: id : id of the task.","title":"Query"},{"location":"thehive/api/task/get/#response","text":"","title":"Response"},{"location":"thehive/api/task/get/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 404 : The Task is not found","title":"Status codes"},{"location":"thehive/api/task/get/#responsebody-example","text":"201 401 404 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/list/","text":"List case tasks # List Task s of a case. Query # POST /api/v0/query Request Body Example # List 15 waiting tasks in case ~25485360. { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"~25485360\" }, { \"_name\" : \"tasks\" }, { \"_name\" : \"filter\" , \"status\" : \"Waiting\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # 201 401 [ { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 }, { \"id\" : \"~8360\" , \"_id\" : \"~8360\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630687291729 , \"updatedAt\" : 1630687323936 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List case tasks"},{"location":"thehive/api/task/list/#list-case-tasks","text":"List Task s of a case.","title":"List case tasks"},{"location":"thehive/api/task/list/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/task/list/#request-body-example","text":"List 15 waiting tasks in case ~25485360. { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"~25485360\" }, { \"_name\" : \"tasks\" }, { \"_name\" : \"filter\" , \"status\" : \"Waiting\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Request Body Example"},{"location":"thehive/api/task/list/#response","text":"","title":"Response"},{"location":"thehive/api/task/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/list/#responsebody-example","text":"201 401 [ { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 }, { \"id\" : \"~8360\" , \"_id\" : \"~8360\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630687291729 , \"updatedAt\" : 1630687323936 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/log-responder-jobs/","text":"List responder jobs on log # List actions run on a log. Query # GET /api/connector/cortex/action/case_task_log/{id} With: id : Log identifier Response # Status codes # 200 : if query is run successfully 401 : Authentication error Response Body Example # 200 401 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Log\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"jerome@strangebee.com\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List responder jobs on log"},{"location":"thehive/api/task/log-responder-jobs/#list-responder-jobs-on-log","text":"List actions run on a log.","title":"List responder jobs on log"},{"location":"thehive/api/task/log-responder-jobs/#query","text":"GET /api/connector/cortex/action/case_task_log/{id} With: id : Log identifier","title":"Query"},{"location":"thehive/api/task/log-responder-jobs/#response","text":"","title":"Response"},{"location":"thehive/api/task/log-responder-jobs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/log-responder-jobs/#response-body-example","text":"200 401 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Log\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"jerome@strangebee.com\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response Body Example"},{"location":"thehive/api/task/log-run-responder/","text":"Run responder # Run a responder on a Log (requires manageAction permission). Query # POST /api/connector/cortex/action Request Body Example # { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case_task_log\" , \"objectId\" : \"~11123\" } The required fields are responderId , objectType and objectId . Response # Status codes # 201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Log is not found Response Body Example # 201 401 403 404 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Log\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create action, you haven't the permission manageTask\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Log not found\" }","title":"Run responder"},{"location":"thehive/api/task/log-run-responder/#run-responder","text":"Run a responder on a Log (requires manageAction permission).","title":"Run responder"},{"location":"thehive/api/task/log-run-responder/#query","text":"POST /api/connector/cortex/action","title":"Query"},{"location":"thehive/api/task/log-run-responder/#request-body-example","text":"{ \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case_task_log\" , \"objectId\" : \"~11123\" } The required fields are responderId , objectType and objectId .","title":"Request Body Example"},{"location":"thehive/api/task/log-run-responder/#response","text":"","title":"Response"},{"location":"thehive/api/task/log-run-responder/#status-codes","text":"201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Log is not found","title":"Status codes"},{"location":"thehive/api/task/log-run-responder/#response-body-example","text":"201 401 403 404 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Log\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create action, you haven't the permission manageTask\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Log not found\" }","title":"Response Body Example"},{"location":"thehive/api/task/logs/","text":"List task logs # List Task log s of a Case . Query # POST /api/v1/query?name=case-task-logs Request Body Example # { \"query\" :[{ \"_name\" : \"getTask\" , \"idOrName\" : \"id\" }, { \"_name\" : \"logs\" }, { \"_name\" : \"sort\" , \"_fields\" :[{ \"date\" : \"desc\" }] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 10 , \"extraData\" :[ \"actionCount\" ] }] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # 200 401 [ { \"_id\" : \"~1421384\" , \"_type\" : \"Log\" , \"_createdBy\" : \"analyst@soc\" , \"_createdAt\" : 1637090593968 , \"message\" : \"42\" , \"date\" : 1637090593968 , \"owner\" : \"analyst@soc\" , \"extraData\" :{ \"actionCount\" : 0 } }, { \"_id\" : \"~1429680\" , \"_type\" : \"Log\" , \"_createdBy\" : \"analyst@soc\" , \"_createdAt\" : 1637090578809 , \"message\" : \"test sample\" , \"date\" : 1637090578809 , \"owner\" : \"analyst@soc\" , \"extraData\" :{ \"actionCount\" : 0 } } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List task logs"},{"location":"thehive/api/task/logs/#list-task-logs","text":"List Task log s of a Case .","title":"List task logs"},{"location":"thehive/api/task/logs/#query","text":"POST /api/v1/query?name=case-task-logs","title":"Query"},{"location":"thehive/api/task/logs/#request-body-example","text":"{ \"query\" :[{ \"_name\" : \"getTask\" , \"idOrName\" : \"id\" }, { \"_name\" : \"logs\" }, { \"_name\" : \"sort\" , \"_fields\" :[{ \"date\" : \"desc\" }] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 10 , \"extraData\" :[ \"actionCount\" ] }] }","title":"Request Body Example"},{"location":"thehive/api/task/logs/#response","text":"","title":"Response"},{"location":"thehive/api/task/logs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/logs/#responsebody-example","text":"200 401 [ { \"_id\" : \"~1421384\" , \"_type\" : \"Log\" , \"_createdBy\" : \"analyst@soc\" , \"_createdAt\" : 1637090593968 , \"message\" : \"42\" , \"date\" : 1637090593968 , \"owner\" : \"analyst@soc\" , \"extraData\" :{ \"actionCount\" : 0 } }, { \"_id\" : \"~1429680\" , \"_type\" : \"Log\" , \"_createdBy\" : \"analyst@soc\" , \"_createdAt\" : 1637090578809 , \"message\" : \"test sample\" , \"date\" : 1637090578809 , \"owner\" : \"analyst@soc\" , \"extraData\" :{ \"actionCount\" : 0 } } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/responder-jobs/","text":"List responder jobs # List actions run on a task. Query # GET /api/connector/cortex/action/case_task/{id} With: id : Task identifier Response # Status codes # 200 : if query is run successfully 401 : Authentication error Response Body Example # 200 401 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Task\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"jerome@strangebee.com\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List responder jobs"},{"location":"thehive/api/task/responder-jobs/#list-responder-jobs","text":"List actions run on a task.","title":"List responder jobs"},{"location":"thehive/api/task/responder-jobs/#query","text":"GET /api/connector/cortex/action/case_task/{id} With: id : Task identifier","title":"Query"},{"location":"thehive/api/task/responder-jobs/#response","text":"","title":"Response"},{"location":"thehive/api/task/responder-jobs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/responder-jobs/#response-body-example","text":"200 401 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Task\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"jerome@strangebee.com\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response Body Example"},{"location":"thehive/api/task/run-responder/","text":"Run responder # Run a responder on a Task (requires manageAction permission). Query # POST /api/connector/cortex/action Request Body Example # { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case_task\" , \"objectId\" : \"~11123\" } The required fields are responderId , objectType and objectId . Response # Status codes # 201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Task is not found Response Body Example # 201 401 403 404 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Task\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create action, you haven't the permission manageTask\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Run responder"},{"location":"thehive/api/task/run-responder/#run-responder","text":"Run a responder on a Task (requires manageAction permission).","title":"Run responder"},{"location":"thehive/api/task/run-responder/#query","text":"POST /api/connector/cortex/action","title":"Query"},{"location":"thehive/api/task/run-responder/#request-body-example","text":"{ \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case_task\" , \"objectId\" : \"~11123\" } The required fields are responderId , objectType and objectId .","title":"Request Body Example"},{"location":"thehive/api/task/run-responder/#response","text":"","title":"Response"},{"location":"thehive/api/task/run-responder/#status-codes","text":"201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Task is not found","title":"Status codes"},{"location":"thehive/api/task/run-responder/#response-body-example","text":"201 401 403 404 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Task\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create action, you haven't the permission manageTask\" } { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Response Body Example"},{"location":"thehive/api/task/update/","text":"Update # Update a Task (requires manageTask permission). Query # PATCH /api/case/task/{id} with: id : id of the task. Request Body Example # { \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 5 , \"dueDate\" : 1630694608000 } No fields are required. Response # Status codes # 200 : if Task is updated successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 401 403 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"updatedBy\" : \"jerome@strangebee.com\" , \"updatedAt\" : 1630685486000 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 5 , \"dueDate\" : 1630694608000 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to update Task, you haven't the permission manageTask\" }","title":"Update"},{"location":"thehive/api/task/update/#update","text":"Update a Task (requires manageTask permission).","title":"Update"},{"location":"thehive/api/task/update/#query","text":"PATCH /api/case/task/{id} with: id : id of the task.","title":"Query"},{"location":"thehive/api/task/update/#request-body-example","text":"{ \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 5 , \"dueDate\" : 1630694608000 } No fields are required.","title":"Request Body Example"},{"location":"thehive/api/task/update/#response","text":"","title":"Response"},{"location":"thehive/api/task/update/#status-codes","text":"200 : if Task is updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/task/update/#responsebody-example","text":"201 401 403 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"updatedBy\" : \"jerome@strangebee.com\" , \"updatedAt\" : 1630685486000 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 5 , \"dueDate\" : 1630694608000 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to update Task, you haven't the permission manageTask\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/waiting-tasks/","text":"List waiting tasks # List all waiting Task s. Query # POST /api/v0/query Request Body Example # List 15 waiting tasks, sorted by flag and startDate . { \"query\" : [ { \"_name\" : \"waitingTasks\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"flag\" : \"desc\" }, { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # 201 401 [ { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 }, { \"id\" : \"~8360\" , \"_id\" : \"~8360\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630687291729 , \"updatedAt\" : 1630687323936 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List waiting tasks"},{"location":"thehive/api/task/waiting-tasks/#list-waiting-tasks","text":"List all waiting Task s.","title":"List waiting tasks"},{"location":"thehive/api/task/waiting-tasks/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/task/waiting-tasks/#request-body-example","text":"List 15 waiting tasks, sorted by flag and startDate . { \"query\" : [ { \"_name\" : \"waitingTasks\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"flag\" : \"desc\" }, { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Request Body Example"},{"location":"thehive/api/task/waiting-tasks/#response","text":"","title":"Response"},{"location":"thehive/api/task/waiting-tasks/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/waiting-tasks/#responsebody-example","text":"201 401 [ { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 }, { \"id\" : \"~8360\" , \"_id\" : \"~8360\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630687291729 , \"updatedAt\" : 1630687323936 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/ttp/","text":"Tactic, Technique and Procedure APIs # List case TTPs Create TTP Update TTP Delete TTP","title":"Tactic, Technique and Procedure APIs"},{"location":"thehive/api/ttp/#tactic-technique-and-procedure-apis","text":"List case TTPs Create TTP Update TTP Delete TTP","title":"Tactic, Technique and Procedure APIs"},{"location":"thehive/api/ttp/create/","text":"Create # Query # Request Body Example # ResponseBody Example #","title":"Create"},{"location":"thehive/api/ttp/create/#create","text":"","title":"Create"},{"location":"thehive/api/ttp/create/#query","text":"","title":"Query"},{"location":"thehive/api/ttp/create/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/ttp/create/#responsebody-example","text":"","title":"ResponseBody Example"},{"location":"thehive/api/ttp/delete/","text":"","title":"Delete"},{"location":"thehive/api/ttp/list/","text":"","title":"List"},{"location":"thehive/api/ttp/update/","text":"Update # Query # Request Body Example # ResponseBody Example #","title":"Update"},{"location":"thehive/api/ttp/update/#update","text":"","title":"Update"},{"location":"thehive/api/ttp/update/#query","text":"","title":"Query"},{"location":"thehive/api/ttp/update/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/ttp/update/#responsebody-example","text":"","title":"ResponseBody Example"},{"location":"thehive/api/user/","text":"User APIs # List users Create a user Update a user Delete a user Lock user Generate API key Get API key Revoke API key Set password","title":"Overview"},{"location":"thehive/api/user/#user-apis","text":"List users Create a user Update a user Delete a user Lock user Generate API key Get API key Revoke API key Set password","title":"User APIs"},{"location":"thehive/api/user/create/","text":"Create # Create an User . Query # POST /api/v1/user Request Body Example # { \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"organisation\" : \"StrangeBee\" , \"profile\" : \"org-admin\" , \"email\" : \"jerome@strangebee.com\" , \"password\" : \"my-secret-password\" } The following fields are required: login : (String - email address) name : (String) organisation : (String) profile : [admin|org-admin|analyst|read-only|any customed profile] Response # Status codes # 201 : if User is created successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # { \"_id\" : \"~947527808\" , \"_createdBy\" : \"admin@thehive.local\" , \"_createdAt\" : 1630411433091 , \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"hasKey\" : false , \"hasPassword\" : false , \"hasMFA\" : false , \"locked\" : false , \"profile\" : \"analyst\" , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCase\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"organisation\" : \"StrangeBee\" , \"organisations\" : [], \"email\" : \"jerome@strangebee.com\" }","title":"Create"},{"location":"thehive/api/user/create/#create","text":"Create an User .","title":"Create"},{"location":"thehive/api/user/create/#query","text":"POST /api/v1/user","title":"Query"},{"location":"thehive/api/user/create/#request-body-example","text":"{ \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"organisation\" : \"StrangeBee\" , \"profile\" : \"org-admin\" , \"email\" : \"jerome@strangebee.com\" , \"password\" : \"my-secret-password\" } The following fields are required: login : (String - email address) name : (String) organisation : (String) profile : [admin|org-admin|analyst|read-only|any customed profile]","title":"Request Body Example"},{"location":"thehive/api/user/create/#response","text":"","title":"Response"},{"location":"thehive/api/user/create/#status-codes","text":"201 : if User is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/create/#responsebody-example","text":"{ \"_id\" : \"~947527808\" , \"_createdBy\" : \"admin@thehive.local\" , \"_createdAt\" : 1630411433091 , \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"hasKey\" : false , \"hasPassword\" : false , \"hasMFA\" : false , \"locked\" : false , \"profile\" : \"analyst\" , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCase\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"organisation\" : \"StrangeBee\" , \"organisations\" : [], \"email\" : \"jerome@strangebee.com\" }","title":"ResponseBody Example"},{"location":"thehive/api/user/delete/","text":"Delete # Delete a User . Query # DELETE /api/v1/user/{id}/force?organisation={ORG_NAME} with: id : id or login of the user ORG_NAME : the organisation name from which the user is to be removed Response # Status codes # 204 : if User is successfully deleted 401 : Authentication error 403 : Authorization error","title":"Delete"},{"location":"thehive/api/user/delete/#delete","text":"Delete a User .","title":"Delete"},{"location":"thehive/api/user/delete/#query","text":"DELETE /api/v1/user/{id}/force?organisation={ORG_NAME} with: id : id or login of the user ORG_NAME : the organisation name from which the user is to be removed","title":"Query"},{"location":"thehive/api/user/delete/#response","text":"","title":"Response"},{"location":"thehive/api/user/delete/#status-codes","text":"204 : if User is successfully deleted 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/generate-api-key/","text":"Generate API key # Generate an API key for a user. Query # POST /api/v1/user/{id}/key/renew with: id : id or login of the user Request Body Example # The body is empty. Response # Status codes # 200 : if the API key have succesfully been generated 401 : Authentication error 403 : Authorization error ResponseBody Example # The key in plain text. BOXTE+Cq0qrZcHhTK4j0LpT/TVW5auOz","title":"Generate API key"},{"location":"thehive/api/user/generate-api-key/#generate-api-key","text":"Generate an API key for a user.","title":"Generate API key"},{"location":"thehive/api/user/generate-api-key/#query","text":"POST /api/v1/user/{id}/key/renew with: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/generate-api-key/#request-body-example","text":"The body is empty.","title":"Request Body Example"},{"location":"thehive/api/user/generate-api-key/#response","text":"","title":"Response"},{"location":"thehive/api/user/generate-api-key/#status-codes","text":"200 : if the API key have succesfully been generated 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/generate-api-key/#responsebody-example","text":"The key in plain text. BOXTE+Cq0qrZcHhTK4j0LpT/TVW5auOz","title":"ResponseBody Example"},{"location":"thehive/api/user/get-api-key/","text":"Get API key # Get the API key of a user. Query # GET /api/v1/user/{id}/key with: id : id or login of the user Response # Status codes # 200 : if the API key have succesfully been generated 401 : Authentication error 403 : Authorization error ResponseBody Example # BOXTE+Cq0qrZcHhTK4j0LpT/TVW5auOz","title":"Get API key"},{"location":"thehive/api/user/get-api-key/#get-api-key","text":"Get the API key of a user.","title":"Get API key"},{"location":"thehive/api/user/get-api-key/#query","text":"GET /api/v1/user/{id}/key with: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/get-api-key/#response","text":"","title":"Response"},{"location":"thehive/api/user/get-api-key/#status-codes","text":"200 : if the API key have succesfully been generated 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/get-api-key/#responsebody-example","text":"BOXTE+Cq0qrZcHhTK4j0LpT/TVW5auOz","title":"ResponseBody Example"},{"location":"thehive/api/user/list/","text":"List # List users. Query # POST /api/v1/query Request Body Example # List last 15 users created. { \"query\" : [ { \"_name\" : \"getOrganisation\" , \"idOrName\" : \"StrangeBee\" }, { \"_name\" : \"users\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"login\" : \"asc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"organisation\" : \"StrangeBee\" } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # [ { \"_id\" : \"~947527808\" , \"_createdBy\" : \"admin@thehive.local\" , \"_createdAt\" : 1630411433091 , \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"hasKey\" : false , \"hasPassword\" : false , \"hasMFA\" : false , \"locked\" : false , \"profile\" : \"analyst\" , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCase\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"organisation\" : \"StrangeBee\" , \"organisations\" : [] } ]","title":"List"},{"location":"thehive/api/user/list/#list","text":"List users.","title":"List"},{"location":"thehive/api/user/list/#query","text":"POST /api/v1/query","title":"Query"},{"location":"thehive/api/user/list/#request-body-example","text":"List last 15 users created. { \"query\" : [ { \"_name\" : \"getOrganisation\" , \"idOrName\" : \"StrangeBee\" }, { \"_name\" : \"users\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"login\" : \"asc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"organisation\" : \"StrangeBee\" } ] }","title":"Request Body Example"},{"location":"thehive/api/user/list/#response","text":"","title":"Response"},{"location":"thehive/api/user/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/list/#responsebody-example","text":"[ { \"_id\" : \"~947527808\" , \"_createdBy\" : \"admin@thehive.local\" , \"_createdAt\" : 1630411433091 , \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"hasKey\" : false , \"hasPassword\" : false , \"hasMFA\" : false , \"locked\" : false , \"profile\" : \"analyst\" , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCase\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"organisation\" : \"StrangeBee\" , \"organisations\" : [] } ]","title":"ResponseBody Example"},{"location":"thehive/api/user/lock/","text":"Lock / Unlock # Lock a User . Query # PATCH /api/v1/user/{id} With: id : id or login of the user Request Body Example # Lock Unlock { \"locked\" : true } { \"locked\" : false } The following fields are required: locked : (Boolean) Response # Status codes # 204 : if User is locked successfully 401 : Authentication error 403 : Authorization error","title":"Lock / Unlock"},{"location":"thehive/api/user/lock/#lock-unlock","text":"Lock a User .","title":"Lock / Unlock"},{"location":"thehive/api/user/lock/#query","text":"PATCH /api/v1/user/{id} With: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/lock/#request-body-example","text":"Lock Unlock { \"locked\" : true } { \"locked\" : false } The following fields are required: locked : (Boolean)","title":"Request Body Example"},{"location":"thehive/api/user/lock/#response","text":"","title":"Response"},{"location":"thehive/api/user/lock/#status-codes","text":"204 : if User is locked successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/revoke-api-key/","text":"Revoke API key # Revoke the API key of a user Query # DELETE /api/v1/user/{id}/key with: id : id or login of the user Response # Status codes # 204 : if API key is successfully revoked 401 : Authentication error 403 : Authorization error","title":"Revoke API key"},{"location":"thehive/api/user/revoke-api-key/#revoke-api-key","text":"Revoke the API key of a user","title":"Revoke API key"},{"location":"thehive/api/user/revoke-api-key/#query","text":"DELETE /api/v1/user/{id}/key with: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/revoke-api-key/#response","text":"","title":"Response"},{"location":"thehive/api/user/revoke-api-key/#status-codes","text":"204 : if API key is successfully revoked 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/set-password/","text":"Set password # Set a User 's password. The user making the query needs to be an admin of the platform Query # POST /api/v1/user/{id}/password/set with: id : id of the user Request Body Example # { \"password\" : \"thehive1234\" } The following fields are required: password : (String) Response # Status codes # 204 : if password is set successfully 401 : Authentication error 403 : Authorization error","title":"Set password"},{"location":"thehive/api/user/set-password/#set-password","text":"Set a User 's password. The user making the query needs to be an admin of the platform","title":"Set password"},{"location":"thehive/api/user/set-password/#query","text":"POST /api/v1/user/{id}/password/set with: id : id of the user","title":"Query"},{"location":"thehive/api/user/set-password/#request-body-example","text":"{ \"password\" : \"thehive1234\" } The following fields are required: password : (String)","title":"Request Body Example"},{"location":"thehive/api/user/set-password/#response","text":"","title":"Response"},{"location":"thehive/api/user/set-password/#status-codes","text":"204 : if password is set successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/update/","text":"Update # Update User 's information. Query # PATCH /api/v1/user/{id} With: id : id or login of the user Request Body Example # { \"name\" : \"Jerome\" , \"profile\" : \"org-admin\" , \"organisation\" : \"StrangeBee\" , \"locked\" : false } The field organisation is used if the profile is updated (the profile of an user depends on the organisation). If not specified, the current organisation is used. No fields are required. Response # Status codes # 204 : if User is updated successfully 401 : Authentication error 403 : Authorization error","title":"Update"},{"location":"thehive/api/user/update/#update","text":"Update User 's information.","title":"Update"},{"location":"thehive/api/user/update/#query","text":"PATCH /api/v1/user/{id} With: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/update/#request-body-example","text":"{ \"name\" : \"Jerome\" , \"profile\" : \"org-admin\" , \"organisation\" : \"StrangeBee\" , \"locked\" : false } The field organisation is used if the profile is updated (the profile of an user depends on the organisation). If not specified, the current organisation is used. No fields are required.","title":"Request Body Example"},{"location":"thehive/api/user/update/#response","text":"","title":"Response"},{"location":"thehive/api/user/update/#status-codes","text":"204 : if User is updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/installation-and-configuration/","text":"Installation & configuration guides # Overview # The scalability of TheHive allows it to be set up as a standalone server or as nodes inside a cluster. Any number of nodes can rely on a database and a file system also setup as standalone servers or a cluster. Before starting installing and configuring, you need to identify and define the targetted architecture. Choose a setup # The modular architecture makes it support several types of database, file storage system and indexing system. The initial choices made with the target architecture and the setup are crucial, especially for the database. If high availability and fault tolerance are necessary, implementing a cluster might be the choice, and this choice determines the database, the file storage and indexing system to install. Hardware Pre-requisites Hardware requirements depends on the number of concurrent users and how they use the system. The following table give some information to choose the hardware. Number of users CPU RAM < 3 2 4-8 < 10 4 8-16 < 20 8 16-32 Choose a database # Once the target setup is identified, the first choice to make is the database. Even of local Berkeley DB and Cassandra database are supported, we recommend using Apache Cassandra , which is a scalable and high available Database, even for standalone servers. Berkeley DB can be enough for testing purposes. Upgradability This choice is decisive as migration from Berkeley DB to Cassandra is not possible . Choose a file storage system # Like for databases, several options exist regarding file system. Basically, for standalone setups, using the local filesystem is the easiest solution. If installing a cluster, there are several options: Using a share NFS folder Using Apache Hadoop , a distributed file system Using a S3-compatible storage service ; for example with Min.IO Upgradability Starting with a standalone server and a local file storage and upgrading to a cluster with S3 of Hadoop is possible. Existing files can be moved to the targetted solutions. Choose an index system # Introduced with TheHive 4.1 to increase performances, TheHive relies on a dedicated indexing process. With a standalone setup, using a local index with Lucene is sufficient. In the case of a cluster, all nodes have to connect to the same index: an instance of Elasticsearch is then required. Upgradability Starting with a standalone server and Lucene and upgrading to a cluster with Elasticsearch is possible. Indices can be rebuilt. However, it can takes some time. Installation Guide # The following Guide let you prepare , install and configure TheHive and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages. If you want to build TheHive from sources, you can follow this guide . Configuration Guides # The configuration of TheHive is in files stored in the /etc/thehive folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/thehive \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by Debian or RPM packages. This file should contain a secret that should be used by one instance. Various aspects can configured in the application.conf file: database and indexing File storage Akka Authentication Connectors Cortex: connecting to one or more organisation MISP: connecting to one or more organisation Webhooks Other service parameters Uses Cases # Basic stand alone server # Follow the installation guides for you prefered operating system. Cluster with 3 TheHive nodes # The folling guide details all the installation and configuration steps to get a cluster with 3 nodes working. The cluster is composed of: 3 TheHive servers 3 Cassandra servers 3 Min.IO servers","title":"Overview"},{"location":"thehive/installation-and-configuration/#installation-configuration-guides","text":"","title":"Installation &amp; configuration guides"},{"location":"thehive/installation-and-configuration/#overview","text":"The scalability of TheHive allows it to be set up as a standalone server or as nodes inside a cluster. Any number of nodes can rely on a database and a file system also setup as standalone servers or a cluster. Before starting installing and configuring, you need to identify and define the targetted architecture.","title":"Overview"},{"location":"thehive/installation-and-configuration/#choose-a-setup","text":"The modular architecture makes it support several types of database, file storage system and indexing system. The initial choices made with the target architecture and the setup are crucial, especially for the database. If high availability and fault tolerance are necessary, implementing a cluster might be the choice, and this choice determines the database, the file storage and indexing system to install. Hardware Pre-requisites Hardware requirements depends on the number of concurrent users and how they use the system. The following table give some information to choose the hardware. Number of users CPU RAM < 3 2 4-8 < 10 4 8-16 < 20 8 16-32","title":"Choose a setup"},{"location":"thehive/installation-and-configuration/#choose-a-database","text":"Once the target setup is identified, the first choice to make is the database. Even of local Berkeley DB and Cassandra database are supported, we recommend using Apache Cassandra , which is a scalable and high available Database, even for standalone servers. Berkeley DB can be enough for testing purposes. Upgradability This choice is decisive as migration from Berkeley DB to Cassandra is not possible .","title":"Choose a database"},{"location":"thehive/installation-and-configuration/#choose-a-file-storage-system","text":"Like for databases, several options exist regarding file system. Basically, for standalone setups, using the local filesystem is the easiest solution. If installing a cluster, there are several options: Using a share NFS folder Using Apache Hadoop , a distributed file system Using a S3-compatible storage service ; for example with Min.IO Upgradability Starting with a standalone server and a local file storage and upgrading to a cluster with S3 of Hadoop is possible. Existing files can be moved to the targetted solutions.","title":"Choose a file storage system"},{"location":"thehive/installation-and-configuration/#choose-an-index-system","text":"Introduced with TheHive 4.1 to increase performances, TheHive relies on a dedicated indexing process. With a standalone setup, using a local index with Lucene is sufficient. In the case of a cluster, all nodes have to connect to the same index: an instance of Elasticsearch is then required. Upgradability Starting with a standalone server and Lucene and upgrading to a cluster with Elasticsearch is possible. Indices can be rebuilt. However, it can takes some time.","title":"Choose an index system"},{"location":"thehive/installation-and-configuration/#installation-guide","text":"The following Guide let you prepare , install and configure TheHive and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages. If you want to build TheHive from sources, you can follow this guide .","title":"Installation Guide"},{"location":"thehive/installation-and-configuration/#configuration-guides","text":"The configuration of TheHive is in files stored in the /etc/thehive folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/thehive \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by Debian or RPM packages. This file should contain a secret that should be used by one instance. Various aspects can configured in the application.conf file: database and indexing File storage Akka Authentication Connectors Cortex: connecting to one or more organisation MISP: connecting to one or more organisation Webhooks Other service parameters","title":"Configuration Guides"},{"location":"thehive/installation-and-configuration/#uses-cases","text":"","title":"Uses Cases"},{"location":"thehive/installation-and-configuration/#basic-stand-alone-server","text":"Follow the installation guides for you prefered operating system.","title":"Basic stand alone server"},{"location":"thehive/installation-and-configuration/#cluster-with-3-thehive-nodes","text":"The folling guide details all the installation and configuration steps to get a cluster with 3 nodes working. The cluster is composed of: 3 TheHive servers 3 Cassandra servers 3 Min.IO servers","title":"Cluster with 3 TheHive nodes"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/","text":"Use TheHive as a cluster # This guide provides configuration examples for TheHive, Cassandra and MinIO to build a fault-tolerant cluster of 3 active nodes. Prerequisite # 3 servers with TheHive and Cassandra installed. TheHive # In this guide, we are considering the node 1 to be the master node. Start by configuring akka component by editing the /etc/thehive/application.conf file of each node like this: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] } Cassandra # We are considering setting up a cluster of 3 active nodes of Cassandra with a replication factor of 3. That means that all nodes are active and the data is present on each node. This setup is tolerant to a 1 node failure. For the rest of this part, we are considering that all nodes sit on the same network. Configuration # Nodes configuration # For each node, update configuration files with the following parameters: /etc/cassandra/cassandra.yml cluster_name: 'thp' num_tokens: 256 authenticator: PasswordAuthenticator authorizer: CassandraAuthorizer role_manager: CassandraRoleManager data_file_directories: - /var/lib/cassandra/data commitlog_directory: /var/lib/cassandra/commitlog saved_caches_directory: /var/lib/cassandra/saved_caches seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"<ip node 1>, <ip node 2>, <ip node 3>\" listen_interface : eth0 rpc_interface: eth0 endpoint_snitch: SimpleSnitch Ensure to setup the right interface name. delete file /etc/cassandra/cassandra-topology.properties rm /etc/cassandra/cassandra-topology.properties Start nodes # On each node, start the service: service cassandra start Ensure that all nodes are up and running: root@cassandra:/# nodetool status Datacenter: dc1 =============== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN <ip node 1> 776.53 KiB 256 100.0% a79c9a8c-c99b-4d74-8e78-6b0c252abd86 rack1 UN <ip node 2> 671.72 KiB 256 100.0% 8fda2906-2097-4d62-91f8-005e33d3e839 rack1 UN <ip node 3> 611.54 KiB 256 100.0% 201ab99c-8e16-49b1-9b66-5444044fb1cd rack1 Initialise the database # On one node run (default password for cassandra account is cassandra ): cqlsh <ip node X> -u cassandra Start by changing the password of superadmin named cassandra : ALTER USER cassandra WITH PASSWORD 'NEWPASSWORD' ; exit and reconnect . Ensure user accounts are duplicated on all nodes ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Create keyspace named thehive CREATE KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : '3' } AND durable_writes = 'true' ; Create role thehive and grant permissions on thehive keyspace (choose a password) CREATE ROLE thehive WITH LOGIN = true AND PASSWORD = 'PASSWORD' ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO 'thehive' ; TheHive associated configuration # Update the configuration of thehive accordingly in /etc/thehive/application.conf : ## Database configuration db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"<ip node 1>\", \"<ip node 2>\", \"<ip node 3>\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"PASSWORD\" cql { cluster-name: thp keyspace: thehive } } Troubleshooting # InvalidRequest: code=2200 [Invalid query] message=\u201dorg.apache.cassandra.auth.CassandraRoleManager doesn\u2019t support PASSWORD\u201d. set the value authenticator: PasswordAuthenticator in cassandra.yaml Caused by: java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.UnauthorizedException: Unable to perform authorization of permissions: Unable to perform authorization of super-user permission: Cannot achieve consistency level LOCAL_ONE ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; MinIO # MinIO distributed mode requires fresh directories. Here is an example of implementation of MinIO with TheHive. The following procedure should be performed on all servers belonging the the cluster. We are considering the setup where the cluster is composed of 3 servers named minio1, minio2 & minio3. Create a dedicated system account # Create a dedicated user with /opt/minio as homedir. adduser minio Create at least 2 data volumes on each server # Create 2 folders on each server: mkdir -p /srv/minio/{1,2} chown -R minio:minio /srv/minio Setup hosts files # Edit /etc/hosts of all servers ip-minio-1 minio1 ip-minio-2 minio2 ip-minio-3 minio3 installation # cd /opt/minio mkdir /opt/minio/{bin,etc} wget -O /opt/minio/bin https://dl.minio.io/server/minio/release/linux-amd64/minio chown -R minio:minio /opt/minio Configuration # Create or edit file `/opt/minio/etc/minio.conf MINIO_OPTS=\"server --address :9100 http://minio{1...3}/srv/minio/{1...2}\" MINIO_ACCESS_KEY=\"<ACCESS_KEY>\" MINIO_SECRET_KEY=\"<SECRET_KEY>\" Create a service file named /usr/lib/systemd/system/minio.service [Unit] Description=minio Documentation=https://docs.min.io Wants=network-online.target After=network-online.target AssertFileIsExecutable=/opt/minio/bin/minio [Service] WorkingDirectory=/opt/minio User=minio Group=minio EnvironmentFile=/opt/minio/etc/minio.conf ExecStart=/opt/minio/bin/minio $MINIO_OPTS Restart=always LimitNOFILE=65536 TimeoutStopSec=0 SendSIGKILL=no [Install] WantedBy=multi-user.target Enable and start the service # systemctl daemon-reload systemctl enable minio systemctl start minio.service Prepare the service for TheHive # Following operations should be performed once all servers are up and running. A new server CAN NOT be added afterward. Connect using the access key and secret key to one server with your browser on port 9100: http://minio:9100 Create a bucket named thehive The bucket should be created and available on all your servers. TheHive associated configuration # For each TheHive node of the cluster, add the relevant storage configuration. Example for the first node thehive1: storage { provider: s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Each TheHive server can connect to one MinIO server.","title":"Use TheHive as a cluster"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#use-thehive-as-a-cluster","text":"This guide provides configuration examples for TheHive, Cassandra and MinIO to build a fault-tolerant cluster of 3 active nodes.","title":"Use TheHive as a cluster"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#prerequisite","text":"3 servers with TheHive and Cassandra installed.","title":"Prerequisite"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#thehive","text":"In this guide, we are considering the node 1 to be the master node. Start by configuring akka component by editing the /etc/thehive/application.conf file of each node like this: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] }","title":"TheHive"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#cassandra","text":"We are considering setting up a cluster of 3 active nodes of Cassandra with a replication factor of 3. That means that all nodes are active and the data is present on each node. This setup is tolerant to a 1 node failure. For the rest of this part, we are considering that all nodes sit on the same network.","title":"Cassandra"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#configuration","text":"","title":"Configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#nodes-configuration","text":"For each node, update configuration files with the following parameters: /etc/cassandra/cassandra.yml cluster_name: 'thp' num_tokens: 256 authenticator: PasswordAuthenticator authorizer: CassandraAuthorizer role_manager: CassandraRoleManager data_file_directories: - /var/lib/cassandra/data commitlog_directory: /var/lib/cassandra/commitlog saved_caches_directory: /var/lib/cassandra/saved_caches seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"<ip node 1>, <ip node 2>, <ip node 3>\" listen_interface : eth0 rpc_interface: eth0 endpoint_snitch: SimpleSnitch Ensure to setup the right interface name. delete file /etc/cassandra/cassandra-topology.properties rm /etc/cassandra/cassandra-topology.properties","title":"Nodes configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#start-nodes","text":"On each node, start the service: service cassandra start Ensure that all nodes are up and running: root@cassandra:/# nodetool status Datacenter: dc1 =============== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN <ip node 1> 776.53 KiB 256 100.0% a79c9a8c-c99b-4d74-8e78-6b0c252abd86 rack1 UN <ip node 2> 671.72 KiB 256 100.0% 8fda2906-2097-4d62-91f8-005e33d3e839 rack1 UN <ip node 3> 611.54 KiB 256 100.0% 201ab99c-8e16-49b1-9b66-5444044fb1cd rack1","title":"Start nodes"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#initialise-the-database","text":"On one node run (default password for cassandra account is cassandra ): cqlsh <ip node X> -u cassandra Start by changing the password of superadmin named cassandra : ALTER USER cassandra WITH PASSWORD 'NEWPASSWORD' ; exit and reconnect . Ensure user accounts are duplicated on all nodes ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Create keyspace named thehive CREATE KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : '3' } AND durable_writes = 'true' ; Create role thehive and grant permissions on thehive keyspace (choose a password) CREATE ROLE thehive WITH LOGIN = true AND PASSWORD = 'PASSWORD' ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO 'thehive' ;","title":"Initialise the database"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#thehive-associated-configuration","text":"Update the configuration of thehive accordingly in /etc/thehive/application.conf : ## Database configuration db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"<ip node 1>\", \"<ip node 2>\", \"<ip node 3>\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"PASSWORD\" cql { cluster-name: thp keyspace: thehive } }","title":"TheHive associated configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#troubleshooting","text":"InvalidRequest: code=2200 [Invalid query] message=\u201dorg.apache.cassandra.auth.CassandraRoleManager doesn\u2019t support PASSWORD\u201d. set the value authenticator: PasswordAuthenticator in cassandra.yaml Caused by: java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.UnauthorizedException: Unable to perform authorization of permissions: Unable to perform authorization of super-user permission: Cannot achieve consistency level LOCAL_ONE ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ;","title":"Troubleshooting"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#minio","text":"MinIO distributed mode requires fresh directories. Here is an example of implementation of MinIO with TheHive. The following procedure should be performed on all servers belonging the the cluster. We are considering the setup where the cluster is composed of 3 servers named minio1, minio2 & minio3.","title":"MinIO"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#create-a-dedicated-system-account","text":"Create a dedicated user with /opt/minio as homedir. adduser minio","title":"Create a dedicated system account"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#create-at-least-2-data-volumes-on-each-server","text":"Create 2 folders on each server: mkdir -p /srv/minio/{1,2} chown -R minio:minio /srv/minio","title":"Create at least 2 data volumes on each server"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#setup-hosts-files","text":"Edit /etc/hosts of all servers ip-minio-1 minio1 ip-minio-2 minio2 ip-minio-3 minio3","title":"Setup hosts files"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#installation","text":"cd /opt/minio mkdir /opt/minio/{bin,etc} wget -O /opt/minio/bin https://dl.minio.io/server/minio/release/linux-amd64/minio chown -R minio:minio /opt/minio","title":"installation"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#configuration_1","text":"Create or edit file `/opt/minio/etc/minio.conf MINIO_OPTS=\"server --address :9100 http://minio{1...3}/srv/minio/{1...2}\" MINIO_ACCESS_KEY=\"<ACCESS_KEY>\" MINIO_SECRET_KEY=\"<SECRET_KEY>\" Create a service file named /usr/lib/systemd/system/minio.service [Unit] Description=minio Documentation=https://docs.min.io Wants=network-online.target After=network-online.target AssertFileIsExecutable=/opt/minio/bin/minio [Service] WorkingDirectory=/opt/minio User=minio Group=minio EnvironmentFile=/opt/minio/etc/minio.conf ExecStart=/opt/minio/bin/minio $MINIO_OPTS Restart=always LimitNOFILE=65536 TimeoutStopSec=0 SendSIGKILL=no [Install] WantedBy=multi-user.target","title":"Configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#enable-and-start-the-service","text":"systemctl daemon-reload systemctl enable minio systemctl start minio.service","title":"Enable and start the service"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#prepare-the-service-for-thehive","text":"Following operations should be performed once all servers are up and running. A new server CAN NOT be added afterward. Connect using the access key and secret key to one server with your browser on port 9100: http://minio:9100 Create a bucket named thehive The bucket should be created and available on all your servers.","title":"Prepare the service for TheHive"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#thehive-associated-configuration_1","text":"For each TheHive node of the cluster, add the relevant storage configuration. Example for the first node thehive1: storage { provider: s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Each TheHive server can connect to one MinIO server.","title":"TheHive associated configuration"},{"location":"thehive/installation-and-configuration/configuration/akka/","text":"Cluster # Quote Akka is a toolkit for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala -- https://akka.io/ Akka is used to make several nodes of TheHive work together and offer a smooth user experience. A good cluster setup requires at least 3 nodes of THeHive applications. For each node, Akka must be configured like this: ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } with: remote.artery.hostname containing the hostname or IP address of the node, cluster.seed-nodes containing the list of akka nodes and beeing the same on all nodes Configuration of a Cluster with 3 nodes Node 1 Node 2 Node 3 Akka configuration for Node 1: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.1\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Akka configuration for Node 2: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.2\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Akka configuration for Node 3: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.3\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } SSL/TLS # Akka supports SSL/TLS to encrypt communications between nodes. A typical configuration with SSL support : ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } ssl.config-ssl-engine { key-store = \"<PATH TO KEYSTORE>\" trust-store = \"<PATH TO TRUSTSTORE>\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } Note Note that akka.remote.artery.transport has changed and akka.ssl.config-ssl-engine needs to be configured. Reference : https://doc.akka.io/docs/akka/current/remoting-artery.html#remote-security About Certificates Use your own internal PKI, or keytool commands to generate your certificates. Reference : https://lightbend.github.io/ssl-config/CertificateGeneration.html#using-keytool Your server certificates should contain various KeyUsage and ExtendedkeyUsage extensions to make everything work properly: KeyUsage extensions nonRepudiation dataEncipherment digitalSignature keyEncipherment ExtendedkeyUsage extensions serverAuth clientAuth Akka configuration with SSL for Node 1 ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"10.1.2.1\" port = 2551 } ssl.config-ssl-engine { key-store = \"/etc/thehive/application.conf.d/certs/10.1.2.1.jks\" trust-store = \"/etc/thehive/application.conf.d/certs/internal_ca.jks\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Apply the same principle for the other nodes, and restart all services.","title":"Cluster"},{"location":"thehive/installation-and-configuration/configuration/akka/#cluster","text":"Quote Akka is a toolkit for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala -- https://akka.io/ Akka is used to make several nodes of TheHive work together and offer a smooth user experience. A good cluster setup requires at least 3 nodes of THeHive applications. For each node, Akka must be configured like this: ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } with: remote.artery.hostname containing the hostname or IP address of the node, cluster.seed-nodes containing the list of akka nodes and beeing the same on all nodes Configuration of a Cluster with 3 nodes Node 1 Node 2 Node 3 Akka configuration for Node 1: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.1\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Akka configuration for Node 2: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.2\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Akka configuration for Node 3: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.3\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] }","title":"Cluster"},{"location":"thehive/installation-and-configuration/configuration/akka/#ssltls","text":"Akka supports SSL/TLS to encrypt communications between nodes. A typical configuration with SSL support : ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } ssl.config-ssl-engine { key-store = \"<PATH TO KEYSTORE>\" trust-store = \"<PATH TO TRUSTSTORE>\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } Note Note that akka.remote.artery.transport has changed and akka.ssl.config-ssl-engine needs to be configured. Reference : https://doc.akka.io/docs/akka/current/remoting-artery.html#remote-security About Certificates Use your own internal PKI, or keytool commands to generate your certificates. Reference : https://lightbend.github.io/ssl-config/CertificateGeneration.html#using-keytool Your server certificates should contain various KeyUsage and ExtendedkeyUsage extensions to make everything work properly: KeyUsage extensions nonRepudiation dataEncipherment digitalSignature keyEncipherment ExtendedkeyUsage extensions serverAuth clientAuth Akka configuration with SSL for Node 1 ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"10.1.2.1\" port = 2551 } ssl.config-ssl-engine { key-store = \"/etc/thehive/application.conf.d/certs/10.1.2.1.jks\" trust-store = \"/etc/thehive/application.conf.d/certs/internal_ca.jks\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Apply the same principle for the other nodes, and restart all services.","title":"SSL/TLS"},{"location":"thehive/installation-and-configuration/configuration/authentication/","text":"Authentication # Authentication consists of a set of module. Each one tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. The default configuration for authentication is: auth { providers = [ {name : session} {name : basic, realm : thehive} {name : local} {name : key} ] } Below are the available authentication modules: session # Authenticates HTTP requests using a cookie. This module manage the cookie creation and expiration. It accepts the following configuration parameters: Parameter Type Description inactivity duration the maximum time of user inactivity before the session is closed warning duration the time before the expiration TheHive returns a warning message Example auth { providers = [ { name : session inactivity : 600 minutes warning : 10 minutes } ] } local # Create a session if the provided login and password, or API key is correct according to the local user database. key # Authenticates HTTP requests using API key provided in the authorization header. The format is Authorization: Bearer xxx (xxx is replaced by the API key). The key is searched using other authentication modules (currently, only local authentication module can validate the key). basic # Authenticates HTTP requests using the login and password provided in authorization header using basic authentication format (Base64). Password is checked from the local user database. Parameter Type Description realm string name of the realm. Without this parameter, the browser doesn't ask to authenticate header # Authenticates HTTP requests using a HTTP header containing the user login. This is used to delegate authentication in a reverse proxy. This module accepts the configuration: Parameter Type Description userHeader string the name of the header that contain the user login ad # Use Microsoft ActiveDirectory to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the domain controllers. If missing, the dnsDomain is used winDomain string the Windows domain name ( MYDOMAIN ) dnsDomain string the Windows domain name in DNS format ( mydomain.local ) useSSL boolean indicate if SSL must be used to connect to domain controller. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ad , hosts : [ \"dc.mydomain.local\" ], dnsDomain : \"mydomain.local\" , winDomain : \"MYDOMAIN\" , } ] } ldap # Use LDAP directory server to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the LDAP servers bindDN string DN of the service account in LDAP. This account is used to search the user bindPW string password of the service account baseDN string DN where the users are located in filter string filter used to search the user. \"{0}\" is replaced by the user login. A valid filter is: (&(uid={0})(objectClass=posixAccount)) useSSL boolean indicate if SSL must be used to connect to LDAP server. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ldap hosts : [ ldap1.mydomain.local , ldap2.mydomain.local ] bindDN : \"cn=thehive,ou=services,dc=mydomain,dc=local\" bindPW : \"SuperSecretPassword\" baseDN : \"ou=users,dc=mydomain,dc=local\" filter : \"(cn={0})\" useSSL : true } ] } oauth2 # Authenticate the user using an external OAuth2 authenticator server. It accepts the following configuration parameters: Parameter Type Description clientId string client ID in the OAuth2 server clientSecret string client secret in the OAuth2 server redirectUri string the url of TheHive AOuth2 page ( xxx/api/ssoLogin ) responseType string type of the response. Currently only \"code\" is accepted grantType string type of the grant. Currently only \"authorization_code\" is accepted authorizationUrl string the url of the OAuth2 server tokenUrl string the token url of the OAuth2 server userUrl string the url to get user information in OAuth2 server scope list of string list of scope userIdField string the field that contains the id of the user in user info organisationField string (optional) the field that contains the organisation name in user info defaultOrganisation string (optional) the default organisation used to login if not present on user info authorizationHeader string prefix of the authorization header to get user info: Bearer, token, ... Example Keycloak Okta Github Microsoft 365 Google ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/auth\" authorizationHeader : \"Bearer\" tokenUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/token\" userUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"https://OKTA/oauth2/v1/authorize\" authorizationHeader : \"Bearer\" tokenUrl : \"http://OKTA/oauth2/v1/token\" userUrl : \"http://OKTA/oauth2/v1/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://github.com/login/oauth/authorize\" authorizationHeader : \"token\" tokenUrl : \"https://github.com/login/oauth/access_token\" userUrl : \"https://api.github.com/user\" scope : [ \"user\" ] userIdField : \"email\" #userOrganisation : \"\" } ] } Note CLIENT_ID and CLIENT_SECRET are created in the OAuth Apps section at https://github.com/settings/developers . this configuration requires that users set the Public email in their Public Profile on https://github.com/settings/profile . ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/authorize\" authorizationHeader : \"Bearer \" tokenUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/token\" userUrl : \"https://graph.microsoft.com/v1.0/me\" scope : [ \"User.Read\" ] userIdField : \"mail\" #userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note To create CLIENT_ID , CLIENT_SECRET and TENANT , register a new app at https://aad.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps . ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://accounts.google.com/o/oauth2/v2/auth\" authorizationHeader : \"Bearer \" tokenUrl : \"https://oauth2.googleapis.com/token\" userUrl : \"https://openidconnect.googleapis.com/v1/userinfo\" scope : [ \"email\" , \"profile\" , \"openid\" ] userIdField : \"email\" # userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note CLIENT_ID and CLIENT_SECRET are created in the _APIs & Services_ > _Credentials_ section of the GCP Console Instructions on how to create Oauth2 credentials at https://support.google.com/cloud/answer/6158849 For the latest reference for Google auth URLs please check Google's .well-known/openid-configuration User autocreation # To allow users to login without previously creating them, you can enable autocreation by adding user.autoCreateOnSso=true to the top level of your configuration. Example user.autoCreateOnSso : true user.profileFieldName : profile user.organisationFieldName : organisation user.defaults.profile : analyst user.defaults.organisation : cert Multi-Factor Authentication # Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page (top-Right corner button > Settings). User administrators can: See which users have activated MFA Reset MFA settings of any user This feature can be **disabled** by setting a config property to false : Example auth.multifactor.enabled = false","title":"Authentication"},{"location":"thehive/installation-and-configuration/configuration/authentication/#authentication","text":"Authentication consists of a set of module. Each one tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. The default configuration for authentication is: auth { providers = [ {name : session} {name : basic, realm : thehive} {name : local} {name : key} ] } Below are the available authentication modules:","title":"Authentication"},{"location":"thehive/installation-and-configuration/configuration/authentication/#session","text":"Authenticates HTTP requests using a cookie. This module manage the cookie creation and expiration. It accepts the following configuration parameters: Parameter Type Description inactivity duration the maximum time of user inactivity before the session is closed warning duration the time before the expiration TheHive returns a warning message Example auth { providers = [ { name : session inactivity : 600 minutes warning : 10 minutes } ] }","title":"session"},{"location":"thehive/installation-and-configuration/configuration/authentication/#local","text":"Create a session if the provided login and password, or API key is correct according to the local user database.","title":"local"},{"location":"thehive/installation-and-configuration/configuration/authentication/#key","text":"Authenticates HTTP requests using API key provided in the authorization header. The format is Authorization: Bearer xxx (xxx is replaced by the API key). The key is searched using other authentication modules (currently, only local authentication module can validate the key).","title":"key"},{"location":"thehive/installation-and-configuration/configuration/authentication/#basic","text":"Authenticates HTTP requests using the login and password provided in authorization header using basic authentication format (Base64). Password is checked from the local user database. Parameter Type Description realm string name of the realm. Without this parameter, the browser doesn't ask to authenticate","title":"basic"},{"location":"thehive/installation-and-configuration/configuration/authentication/#header","text":"Authenticates HTTP requests using a HTTP header containing the user login. This is used to delegate authentication in a reverse proxy. This module accepts the configuration: Parameter Type Description userHeader string the name of the header that contain the user login","title":"header"},{"location":"thehive/installation-and-configuration/configuration/authentication/#ad","text":"Use Microsoft ActiveDirectory to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the domain controllers. If missing, the dnsDomain is used winDomain string the Windows domain name ( MYDOMAIN ) dnsDomain string the Windows domain name in DNS format ( mydomain.local ) useSSL boolean indicate if SSL must be used to connect to domain controller. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ad , hosts : [ \"dc.mydomain.local\" ], dnsDomain : \"mydomain.local\" , winDomain : \"MYDOMAIN\" , } ] }","title":"ad"},{"location":"thehive/installation-and-configuration/configuration/authentication/#ldap","text":"Use LDAP directory server to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the LDAP servers bindDN string DN of the service account in LDAP. This account is used to search the user bindPW string password of the service account baseDN string DN where the users are located in filter string filter used to search the user. \"{0}\" is replaced by the user login. A valid filter is: (&(uid={0})(objectClass=posixAccount)) useSSL boolean indicate if SSL must be used to connect to LDAP server. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ldap hosts : [ ldap1.mydomain.local , ldap2.mydomain.local ] bindDN : \"cn=thehive,ou=services,dc=mydomain,dc=local\" bindPW : \"SuperSecretPassword\" baseDN : \"ou=users,dc=mydomain,dc=local\" filter : \"(cn={0})\" useSSL : true } ] }","title":"ldap"},{"location":"thehive/installation-and-configuration/configuration/authentication/#oauth2","text":"Authenticate the user using an external OAuth2 authenticator server. It accepts the following configuration parameters: Parameter Type Description clientId string client ID in the OAuth2 server clientSecret string client secret in the OAuth2 server redirectUri string the url of TheHive AOuth2 page ( xxx/api/ssoLogin ) responseType string type of the response. Currently only \"code\" is accepted grantType string type of the grant. Currently only \"authorization_code\" is accepted authorizationUrl string the url of the OAuth2 server tokenUrl string the token url of the OAuth2 server userUrl string the url to get user information in OAuth2 server scope list of string list of scope userIdField string the field that contains the id of the user in user info organisationField string (optional) the field that contains the organisation name in user info defaultOrganisation string (optional) the default organisation used to login if not present on user info authorizationHeader string prefix of the authorization header to get user info: Bearer, token, ... Example Keycloak Okta Github Microsoft 365 Google ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/auth\" authorizationHeader : \"Bearer\" tokenUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/token\" userUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"https://OKTA/oauth2/v1/authorize\" authorizationHeader : \"Bearer\" tokenUrl : \"http://OKTA/oauth2/v1/token\" userUrl : \"http://OKTA/oauth2/v1/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://github.com/login/oauth/authorize\" authorizationHeader : \"token\" tokenUrl : \"https://github.com/login/oauth/access_token\" userUrl : \"https://api.github.com/user\" scope : [ \"user\" ] userIdField : \"email\" #userOrganisation : \"\" } ] } Note CLIENT_ID and CLIENT_SECRET are created in the OAuth Apps section at https://github.com/settings/developers . this configuration requires that users set the Public email in their Public Profile on https://github.com/settings/profile . ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/authorize\" authorizationHeader : \"Bearer \" tokenUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/token\" userUrl : \"https://graph.microsoft.com/v1.0/me\" scope : [ \"User.Read\" ] userIdField : \"mail\" #userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note To create CLIENT_ID , CLIENT_SECRET and TENANT , register a new app at https://aad.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps . ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://accounts.google.com/o/oauth2/v2/auth\" authorizationHeader : \"Bearer \" tokenUrl : \"https://oauth2.googleapis.com/token\" userUrl : \"https://openidconnect.googleapis.com/v1/userinfo\" scope : [ \"email\" , \"profile\" , \"openid\" ] userIdField : \"email\" # userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note CLIENT_ID and CLIENT_SECRET are created in the _APIs & Services_ > _Credentials_ section of the GCP Console Instructions on how to create Oauth2 credentials at https://support.google.com/cloud/answer/6158849 For the latest reference for Google auth URLs please check Google's .well-known/openid-configuration","title":"oauth2"},{"location":"thehive/installation-and-configuration/configuration/authentication/#user-autocreation","text":"To allow users to login without previously creating them, you can enable autocreation by adding user.autoCreateOnSso=true to the top level of your configuration. Example user.autoCreateOnSso : true user.profileFieldName : profile user.organisationFieldName : organisation user.defaults.profile : analyst user.defaults.organisation : cert","title":"User autocreation"},{"location":"thehive/installation-and-configuration/configuration/authentication/#multi-factor-authentication","text":"Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page (top-Right corner button > Settings). User administrators can: See which users have activated MFA Reset MFA settings of any user This feature can be **disabled** by setting a config property to false : Example auth.multifactor.enabled = false","title":"Multi-Factor Authentication"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/","text":"TheHive connector: Cortex # Enable the connector # The Cortex connector module needs to be enabled to allow TheHive work with Cortex. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule Configure one connection # TheHive is able to connect more than one Cortex organisation. Several parameters can be configured for one server : Parameter Type Description name string name given to the Cortex instance (eg: Cortex-Internal ) url string url to connect to the Cortex instance auth dict method used to authenticate on the server ( bearer if using API keys) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy refreshDelay duration frequency of job updates checks (default: 5 seconds ) maxRetryOnError integer maximum number of successive errors before give up (default: 3 ) statusCheckInterval duration check remote Cortex status time interval (default: 1 minute ) includedTheHiveOrganisations list of string list of TheHive organisations which can use this Cortex server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this Cortex server (default: None ( [] ) ) This configuration has to be added to TheHive conf/application.conf file. ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = local url = \"http://localhost:9001\" auth { type = \"bearer\" key = \"[REDACTED]\" } wsConfig {} includedTheHiveOrganisations = [\"*\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } Note By default, adding a Cortex server in TheHive configuration make it available for all organisations added on the instance. Example 1 server more servers Configuration with one Cortex connection: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } Configuration with 2 Cortex connections: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex1 url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig {} includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } { name = Cortex2 url = \"http://cortex2:9001\" auth { type = \"bearer\" key = \"lSDkjDGGGHtipueroBHOroNJKLbpi\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG2\", \"ORG3\"] excludedTheHiveOrganisations = [\"ORG1\"] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 5 minutes }","title":"Cortex connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/#thehive-connector-cortex","text":"","title":"TheHive connector: Cortex"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/#enable-the-connector","text":"The Cortex connector module needs to be enabled to allow TheHive work with Cortex. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule","title":"Enable the connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/#configure-one-connection","text":"TheHive is able to connect more than one Cortex organisation. Several parameters can be configured for one server : Parameter Type Description name string name given to the Cortex instance (eg: Cortex-Internal ) url string url to connect to the Cortex instance auth dict method used to authenticate on the server ( bearer if using API keys) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy refreshDelay duration frequency of job updates checks (default: 5 seconds ) maxRetryOnError integer maximum number of successive errors before give up (default: 3 ) statusCheckInterval duration check remote Cortex status time interval (default: 1 minute ) includedTheHiveOrganisations list of string list of TheHive organisations which can use this Cortex server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this Cortex server (default: None ( [] ) ) This configuration has to be added to TheHive conf/application.conf file. ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = local url = \"http://localhost:9001\" auth { type = \"bearer\" key = \"[REDACTED]\" } wsConfig {} includedTheHiveOrganisations = [\"*\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } Note By default, adding a Cortex server in TheHive configuration make it available for all organisations added on the instance. Example 1 server more servers Configuration with one Cortex connection: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } Configuration with 2 Cortex connections: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex1 url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig {} includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } { name = Cortex2 url = \"http://cortex2:9001\" auth { type = \"bearer\" key = \"lSDkjDGGGHtipueroBHOroNJKLbpi\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG2\", \"ORG3\"] excludedTheHiveOrganisations = [\"ORG1\"] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 5 minutes }","title":"Configure one connection"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/","text":"TheHive connector: MISP # Enable MISP connector # The MISP connector module needs to be enabled to allow TheHive to interact with MISP. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.misp.MispModule Configuration # TheHive is able to connect to more than one MISP server for pulling, pushing or both. Several parameters can be configured for one server : Parameter Type Description interval duration delay between to pull/push events to remote MISP servers. This is a common parameter for all configured server name string name given to the MISP instance (eg: MISP-MyOrg ) url string url to connect to the MISP instance auth dict method used to authenticate on the server ( key if using API keys) purpose string define the purpose of the server MISP: ImportOnly , ExportOnly or ImportAndExport (default: ImportAndExport ) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy caseTemplate string case template used by default in TheHive to import events as Alerts tags list of string tags to be added to events imported as Alerts in TheHive exportCaseTags boolean indicate if the tags of the case should be exported to MISP event (default: false) Optional parameters can be added to filter out some events coming into TheHive: Parameter Type Description exclusion.organisations list of string list of MISP organisation from which event will not be imported exclusion.tags list of string don't import MISP events which have one of these tags whitelist.organisations list of string import only events from these MISP organisations whitelist.tags list of string import only MISP events which have one of these tags max-age duration maximum age of the last publish date of event to be imported in TheHive includedTheHiveOrganisations list of string list of TheHive organisations which can use this MISP server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this MISP server (default: None ( [] ) ) Additionally, some organisations or tags from MISP can be defined to exclude events. Note By default, adding a MISP server in TheHive configuration make it available for all organisations added on the instance. This configuration has to be added to TheHive conf/application.conf file: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"local\" url = \"http : //localhost/\" auth { type = key key = \"***\" } wsConfig {} caseTemplate = \"<Template_Name_goes_here>\" tags = [ \"misp-server-id\" ] max-age = 7 days exclusion { organisations = [ \"bad organisation\" , \"other orga\" ] tags = [ \"tag1\" , \"tag2\" ] } whitelist { tags = [ \"tag1\" , \"tag2\" ] } includedTheHiveOrganisations = [ \"*\" ] excludedTheHiveOrganisations = [] } ] } Example Connection with 1 MISP server: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"MISP Server\" url = \"https : //misp.server\" auth { type = key key = \"XhtropikjthiuGIORWUHHlLhlfeerljta\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } tags = [ \"tag1\" , \"tag2\" , \"tag3\" ] caseTemplate = \"misp\" includedTheHiveOrganisations = [ \"ORG1\" , \"ORG2\" ] } ] }","title":"MISP connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/#thehive-connector-misp","text":"","title":"TheHive connector: MISP"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/#enable-misp-connector","text":"The MISP connector module needs to be enabled to allow TheHive to interact with MISP. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.misp.MispModule","title":"Enable MISP connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/#configuration","text":"TheHive is able to connect to more than one MISP server for pulling, pushing or both. Several parameters can be configured for one server : Parameter Type Description interval duration delay between to pull/push events to remote MISP servers. This is a common parameter for all configured server name string name given to the MISP instance (eg: MISP-MyOrg ) url string url to connect to the MISP instance auth dict method used to authenticate on the server ( key if using API keys) purpose string define the purpose of the server MISP: ImportOnly , ExportOnly or ImportAndExport (default: ImportAndExport ) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy caseTemplate string case template used by default in TheHive to import events as Alerts tags list of string tags to be added to events imported as Alerts in TheHive exportCaseTags boolean indicate if the tags of the case should be exported to MISP event (default: false) Optional parameters can be added to filter out some events coming into TheHive: Parameter Type Description exclusion.organisations list of string list of MISP organisation from which event will not be imported exclusion.tags list of string don't import MISP events which have one of these tags whitelist.organisations list of string import only events from these MISP organisations whitelist.tags list of string import only MISP events which have one of these tags max-age duration maximum age of the last publish date of event to be imported in TheHive includedTheHiveOrganisations list of string list of TheHive organisations which can use this MISP server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this MISP server (default: None ( [] ) ) Additionally, some organisations or tags from MISP can be defined to exclude events. Note By default, adding a MISP server in TheHive configuration make it available for all organisations added on the instance. This configuration has to be added to TheHive conf/application.conf file: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"local\" url = \"http : //localhost/\" auth { type = key key = \"***\" } wsConfig {} caseTemplate = \"<Template_Name_goes_here>\" tags = [ \"misp-server-id\" ] max-age = 7 days exclusion { organisations = [ \"bad organisation\" , \"other orga\" ] tags = [ \"tag1\" , \"tag2\" ] } whitelist { tags = [ \"tag1\" , \"tag2\" ] } includedTheHiveOrganisations = [ \"*\" ] excludedTheHiveOrganisations = [] } ] } Example Connection with 1 MISP server: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"MISP Server\" url = \"https : //misp.server\" auth { type = key key = \"XhtropikjthiuGIORWUHHlLhlfeerljta\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } tags = [ \"tag1\" , \"tag2\" , \"tag3\" ] caseTemplate = \"misp\" includedTheHiveOrganisations = [ \"ORG1\" , \"ORG2\" ] } ] }","title":"Configuration"},{"location":"thehive/installation-and-configuration/configuration/database/","text":"Database and index configuration # TheHive can be configured to connect to local Berkeley database or Cassandra database. Tip Using Cassandra is strongly recommended for production use while Berkeley DB can be prefered for testing and training purpose. Starting with TheHive 4.1.0, indexes are managed by a dedicated engine. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch . Configuation # A typical database configuration for TheHive looks like this: ## Database configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"IP_ADDRESS\"] cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : lucene directory: /path/to/index/folder } } } } List of possible parameters # Parameter Type Description provider string provider name. Default: janusgraph storage dict storage configuration storage.backend string storage type. Can be cql or berkeleyje storage.hostname list of string list of IP addresses or hostnames when using cql backend storage.directory string local path for data when using berkeleyje backend storage.username string account username with cql backend if Cassandra auth is configured storage.password string account password with cql backend if Cassandra auth is configured storage.port integer port number with cql backend ( 9042 by default). Change this if using an alternate port or a dedicated port number when using SSL with Cassandra storage.cql dict configuration for cql backend if used storage.cql.cluster-name string name of the cluster name used in the configuration of Apache Cassandra storage.cql.keyspace string Keyspace name used to store TheHive data in Apache Cassandra storage.cql.ssl.enabled boolean false by default. set it to true if SSL is used with Cassandra storage.cql.ssl.truststore.location string path the the truststore. Specify it when using SSL with Cassandra storage.cql.ssl.password string password to access the truststore storage.cql.ssl.client-authentication-enabled boolean Enables use of a client key to authenticate with Cassandra storage.cql.ssl.keystore.location string path the the keystore. Specify it when using SSL and client auth. with Cassandra storage.cql.ssl.keystore.keypassword string password to access the key in the keystore storage.cql.ssl.truststore.storepassword string password the access the keystore index.search dict configuration for indexes index.search.backend string index engine. Default: lucene provided with TheHive. Can also be elasticsearch index.search.directory string path to folder where indexes should be stored, when using lucene engine index.search.hostname list of string list of IP addresses or hostnames when using elasticsearch engine index.search.index-name string name of index, when using elasticseach engine index.search.elasticsearch.http.auth.type: basic string basic is the only possible value index.search.elasticsearch.http.auth.basic.username string Username account on Elasticsearch index.search.elasticsearch.http.auth.basic.password string Password of the account on Elasticsearch index.search.elasticsearch.ssl.enabled boolean Enable SSL true/false index.search.elasticsearch.ssl.truststore.location string Location of the truststore index.search.elasticsearch.ssl.truststore.password string Password of the truststore index.search.elasticsearch.ssl.keystore.location string Location of the keystore for client authentication index.search.elasticsearch.ssl.keystore.storepassword string Password of the keystore index.search.elasticsearch.ssl.keystore.keypassword string Password of the client certificate index.search.elasticsearch.ssl.disable-hostname-verification boolean Disable SSL verification true/false index.search.elasticsearch.ssl.allow-self-signed-certificates boolean Allow self signe certificates true/false Warning Using Elasticsearch to manage indexes is required if you are setting up TheHive as a cluster. The initial start , or first start after configuring indexes might take some time if the database contains a large amount of data. This time is due to the indexes creation More information on configuration for Elasticsearch connection: https://docs.janusgraph.org/index-backend/elasticsearch/ . Use cases # Database and index engine can be different, depending on the use case and target setup: Example Testing server Standalone server with Cassandra Cluster with Cassandra & Elasticsearch For such use cases, local database and indexes are adequate: Create a dedicated folder for data and for indexes. These folders should belong to the user thehive:thehive . mkdir /opt/thp/thehive/ { data, index } chown -R thehive:thehive /opt/thp/thehive/ { data, index } Configure TheHive accordingly: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: berkeleyje directory: /opt/thp/thehive/database } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Install a Cassandra server locally Create a dedicated folder for indexes. This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Install a cluster of Cassandra servers Get access to an Elasticsearch server Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive elasticsearch { http { auth { type: basic basic { username: httpuser password: httppassword } } } ssl { enabled: true truststore { location: /path/to/your/truststore.jks password: truststorepwd } } } } } } } Warning In this configuration, all TheHive nodes should have the same configuration. Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored","title":"Database & indexes"},{"location":"thehive/installation-and-configuration/configuration/database/#database-and-index-configuration","text":"TheHive can be configured to connect to local Berkeley database or Cassandra database. Tip Using Cassandra is strongly recommended for production use while Berkeley DB can be prefered for testing and training purpose. Starting with TheHive 4.1.0, indexes are managed by a dedicated engine. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch .","title":"Database and index configuration"},{"location":"thehive/installation-and-configuration/configuration/database/#configuation","text":"A typical database configuration for TheHive looks like this: ## Database configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"IP_ADDRESS\"] cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : lucene directory: /path/to/index/folder } } } }","title":"Configuation"},{"location":"thehive/installation-and-configuration/configuration/database/#list-of-possible-parameters","text":"Parameter Type Description provider string provider name. Default: janusgraph storage dict storage configuration storage.backend string storage type. Can be cql or berkeleyje storage.hostname list of string list of IP addresses or hostnames when using cql backend storage.directory string local path for data when using berkeleyje backend storage.username string account username with cql backend if Cassandra auth is configured storage.password string account password with cql backend if Cassandra auth is configured storage.port integer port number with cql backend ( 9042 by default). Change this if using an alternate port or a dedicated port number when using SSL with Cassandra storage.cql dict configuration for cql backend if used storage.cql.cluster-name string name of the cluster name used in the configuration of Apache Cassandra storage.cql.keyspace string Keyspace name used to store TheHive data in Apache Cassandra storage.cql.ssl.enabled boolean false by default. set it to true if SSL is used with Cassandra storage.cql.ssl.truststore.location string path the the truststore. Specify it when using SSL with Cassandra storage.cql.ssl.password string password to access the truststore storage.cql.ssl.client-authentication-enabled boolean Enables use of a client key to authenticate with Cassandra storage.cql.ssl.keystore.location string path the the keystore. Specify it when using SSL and client auth. with Cassandra storage.cql.ssl.keystore.keypassword string password to access the key in the keystore storage.cql.ssl.truststore.storepassword string password the access the keystore index.search dict configuration for indexes index.search.backend string index engine. Default: lucene provided with TheHive. Can also be elasticsearch index.search.directory string path to folder where indexes should be stored, when using lucene engine index.search.hostname list of string list of IP addresses or hostnames when using elasticsearch engine index.search.index-name string name of index, when using elasticseach engine index.search.elasticsearch.http.auth.type: basic string basic is the only possible value index.search.elasticsearch.http.auth.basic.username string Username account on Elasticsearch index.search.elasticsearch.http.auth.basic.password string Password of the account on Elasticsearch index.search.elasticsearch.ssl.enabled boolean Enable SSL true/false index.search.elasticsearch.ssl.truststore.location string Location of the truststore index.search.elasticsearch.ssl.truststore.password string Password of the truststore index.search.elasticsearch.ssl.keystore.location string Location of the keystore for client authentication index.search.elasticsearch.ssl.keystore.storepassword string Password of the keystore index.search.elasticsearch.ssl.keystore.keypassword string Password of the client certificate index.search.elasticsearch.ssl.disable-hostname-verification boolean Disable SSL verification true/false index.search.elasticsearch.ssl.allow-self-signed-certificates boolean Allow self signe certificates true/false Warning Using Elasticsearch to manage indexes is required if you are setting up TheHive as a cluster. The initial start , or first start after configuring indexes might take some time if the database contains a large amount of data. This time is due to the indexes creation More information on configuration for Elasticsearch connection: https://docs.janusgraph.org/index-backend/elasticsearch/ .","title":"List of possible parameters"},{"location":"thehive/installation-and-configuration/configuration/database/#use-cases","text":"Database and index engine can be different, depending on the use case and target setup: Example Testing server Standalone server with Cassandra Cluster with Cassandra & Elasticsearch For such use cases, local database and indexes are adequate: Create a dedicated folder for data and for indexes. These folders should belong to the user thehive:thehive . mkdir /opt/thp/thehive/ { data, index } chown -R thehive:thehive /opt/thp/thehive/ { data, index } Configure TheHive accordingly: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: berkeleyje directory: /opt/thp/thehive/database } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Install a Cassandra server locally Create a dedicated folder for indexes. This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Install a cluster of Cassandra servers Get access to an Elasticsearch server Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive elasticsearch { http { auth { type: basic basic { username: httpuser password: httppassword } } } ssl { enabled: true truststore { location: /path/to/your/truststore.jks password: truststorepwd } } } } } } } Warning In this configuration, all TheHive nodes should have the same configuration. Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored","title":"Use cases"},{"location":"thehive/installation-and-configuration/configuration/file-storage/","text":"File storage configuration # TheHive can be configured to use local or distributed filesystems. Example Local or NFS Min.IO Apache Hadoop Create dedicated folder ; it should belong to user and group thehive:thehive . mkdir /opt/thp/thehive/files chown thehive:thehive /opt/thp/thehive/files Configure TheHive accordingly: ## Attachment storage configuration storage { ## Local filesystem provider : localfs localfs { location : /opt/thp/thehive/files } } Install a Min.IO cluster Configure each node of TheHive accordingly: ## Attachment storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://10.1.2.4:9100\" accessKey = \"thehive\" secretKey = \"minio_password\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Install an Apache Hadoop server Configure each node of TheHive accordingly ( /etc/thehive/application.conf ): ## Attachment storage configuration ## Hadoop filesystem (HDFS) provider : hdfs hdfs { root : \"hdfs://10.1.2.4:10000\" # namenode server hostname location : \"/thehive\" # location inside HDFS username : thehive # file owner } }","title":"File Storage"},{"location":"thehive/installation-and-configuration/configuration/file-storage/#file-storage-configuration","text":"TheHive can be configured to use local or distributed filesystems. Example Local or NFS Min.IO Apache Hadoop Create dedicated folder ; it should belong to user and group thehive:thehive . mkdir /opt/thp/thehive/files chown thehive:thehive /opt/thp/thehive/files Configure TheHive accordingly: ## Attachment storage configuration storage { ## Local filesystem provider : localfs localfs { location : /opt/thp/thehive/files } } Install a Min.IO cluster Configure each node of TheHive accordingly: ## Attachment storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://10.1.2.4:9100\" accessKey = \"thehive\" secretKey = \"minio_password\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Install an Apache Hadoop server Configure each node of TheHive accordingly ( /etc/thehive/application.conf ): ## Attachment storage configuration ## Hadoop filesystem (HDFS) provider : hdfs hdfs { root : \"hdfs://10.1.2.4:10000\" # namenode server hostname location : \"/thehive\" # location inside HDFS username : thehive # file owner } }","title":"File storage configuration"},{"location":"thehive/installation-and-configuration/configuration/logs/","text":"","title":"Logs"},{"location":"thehive/installation-and-configuration/configuration/manage-configuration/","text":"Manage configuration files # TheHive uses HOCON as configuration file format. This format gives enough flexibility to structure and organise the configuration of TheHive. TheHive is delivered with following files, in the folder /etc/thehive : logback.xml containing the log policy secret.conf containing a secret key used to create sessions. This key should be unique per instance (in the case of a cluster, this key should be the same for all nodes of this cluster) application.conf HOCON file format let you organise the configuration to have separate files for each purpose. It is the possible to create a /etc/thehive/application.conf.d folder and have several files inside that will be included in the main file /etc/thehive/application.conf . At the end, the following configuration structure is possible: /etc/thehive |-- application.conf |-- application.conf.d | |-- secret.conf | |-- service.conf | |-- ssl.conf | |-- proxy.conf | |-- database.conf | |-- storage.conf | |-- cluster.conf | |-- authentication.conf | |-- cortex.conf | |-- misp.conf | `-- webhooks.conf `-- logback.xml And the content of /etc/thehive/application.conf : ### ## Documentation is available at https://docs.thehive-project.org ### ## Include Play secret key # More information on secret key at https://www.playframework.com/documentation/2.8.x/ApplicationSecret include \"/etc/thehive/application.conf.d/secret.conf\" ## Service include \"/etc/thehive/application.conf.d/service.conf\" ## SSL settings include \"/etc/thehive/application.conf.d/ssl.conf\" ## PROXY settings include \"/etc/thehive/application.conf.d/proxy.conf\" ## Database include \"/etc/thehive/application.conf.d/database.conf\" ## Storage include \"/etc/thehive/application.conf.d/storage.conf\" ## Cluster include \"/etc/thehive/application.conf.d/cluster.conf\" ## Authentication include \"/etc/thehive/application.conf.d/authentication.conf\" ## Cortex include \"/etc/thehive/application.conf.d/cortex.conf\" ## MISP include \"/etc/thehive/application.conf.d/misp.conf\" ## Webhooks include \"/etc/thehive/application.conf.d/webhooks.conf\"","title":"Manage Configuration"},{"location":"thehive/installation-and-configuration/configuration/manage-configuration/#manage-configuration-files","text":"TheHive uses HOCON as configuration file format. This format gives enough flexibility to structure and organise the configuration of TheHive. TheHive is delivered with following files, in the folder /etc/thehive : logback.xml containing the log policy secret.conf containing a secret key used to create sessions. This key should be unique per instance (in the case of a cluster, this key should be the same for all nodes of this cluster) application.conf HOCON file format let you organise the configuration to have separate files for each purpose. It is the possible to create a /etc/thehive/application.conf.d folder and have several files inside that will be included in the main file /etc/thehive/application.conf . At the end, the following configuration structure is possible: /etc/thehive |-- application.conf |-- application.conf.d | |-- secret.conf | |-- service.conf | |-- ssl.conf | |-- proxy.conf | |-- database.conf | |-- storage.conf | |-- cluster.conf | |-- authentication.conf | |-- cortex.conf | |-- misp.conf | `-- webhooks.conf `-- logback.xml And the content of /etc/thehive/application.conf : ### ## Documentation is available at https://docs.thehive-project.org ### ## Include Play secret key # More information on secret key at https://www.playframework.com/documentation/2.8.x/ApplicationSecret include \"/etc/thehive/application.conf.d/secret.conf\" ## Service include \"/etc/thehive/application.conf.d/service.conf\" ## SSL settings include \"/etc/thehive/application.conf.d/ssl.conf\" ## PROXY settings include \"/etc/thehive/application.conf.d/proxy.conf\" ## Database include \"/etc/thehive/application.conf.d/database.conf\" ## Storage include \"/etc/thehive/application.conf.d/storage.conf\" ## Cluster include \"/etc/thehive/application.conf.d/cluster.conf\" ## Authentication include \"/etc/thehive/application.conf.d/authentication.conf\" ## Cortex include \"/etc/thehive/application.conf.d/cortex.conf\" ## MISP include \"/etc/thehive/application.conf.d/misp.conf\" ## Webhooks include \"/etc/thehive/application.conf.d/webhooks.conf\"","title":"Manage configuration files"},{"location":"thehive/installation-and-configuration/configuration/proxy/","text":"Proxy settings # Proxy for connectors # Refer to Cortex or MISP configuration to setup specific proxy configuration for these remote services. Proxy for global application # Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. Parameter Type Description wsConfig.proxy.host string The hostname of the proxy server wsConfig.proxy.port integer The port of the proxy server wsConfig.proxy.protocol string The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified wsConfig.proxy.user string The username of the credentials for the proxy server wsConfig.proxy.password string The password for the credentials for the proxy server wsConfig.proxy.ntlmDomain string The NTLM domain wsConfig.proxy.encoding string The realm's charset wsConfig.proxy.nonProxyHosts list The list of hosts on which proxy must not be used","title":"Proxy"},{"location":"thehive/installation-and-configuration/configuration/proxy/#proxy-settings","text":"","title":"Proxy settings"},{"location":"thehive/installation-and-configuration/configuration/proxy/#proxy-for-connectors","text":"Refer to Cortex or MISP configuration to setup specific proxy configuration for these remote services.","title":"Proxy for connectors"},{"location":"thehive/installation-and-configuration/configuration/proxy/#proxy-for-global-application","text":"Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. Parameter Type Description wsConfig.proxy.host string The hostname of the proxy server wsConfig.proxy.port integer The port of the proxy server wsConfig.proxy.protocol string The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified wsConfig.proxy.user string The username of the credentials for the proxy server wsConfig.proxy.password string The password for the credentials for the proxy server wsConfig.proxy.ntlmDomain string The NTLM domain wsConfig.proxy.encoding string The realm's charset wsConfig.proxy.nonProxyHosts list The list of hosts on which proxy must not be used","title":"Proxy for global application"},{"location":"thehive/installation-and-configuration/configuration/secret/","text":"secret.conf file # This file contains a secret that is used to define cookies used to manage the users session. As a result, one instance of TheHive should use a unique secret key. Example ## Play secret key play.http.secret.key=\"dgngu325mbnbc39cxas4l5kb24503836y2vsvsg465989fbsvop9d09ds6df6\" Warning In the case of a cluster of TheHive nodes, all nodes should have the same secret.conf file with the same secret key. The secret is used to generate user sessions.","title":"Secret key"},{"location":"thehive/installation-and-configuration/configuration/secret/#secretconf-file","text":"This file contains a secret that is used to define cookies used to manage the users session. As a result, one instance of TheHive should use a unique secret key. Example ## Play secret key play.http.secret.key=\"dgngu325mbnbc39cxas4l5kb24503836y2vsvsg465989fbsvop9d09ds6df6\" Warning In the case of a cluster of TheHive nodes, all nodes should have the same secret.conf file with the same secret key. The secret is used to generate user sessions.","title":"secret.conf file"},{"location":"thehive/installation-and-configuration/configuration/service/","text":"Service # Listen address & port # By default the application listens on all interfaces and port 9000 . This is possible to specify listen address and ports with following parameters in the application.conf file: http.address=127.0.0.1 http.port=9000 Context # If you are using a reverse proxy, and you want to specify a location (ex: /thehive ), updating the configuration of TheHive is also required Example play.http.context: \"/thehive\" Specific configuration for streams # If you are using a reverse proxy like Nginx, you might receive error popups with the following message: StreamSrv 504 Gateway Time-Out . You need to change default setting for long polling refresh, Set stream.longPolling.refresh accordingly. Example stream.longPolling.refresh: 45 seconds Manage content length # Content length of text and files managed by the application are limited by default. Before TheHive v4.1.1 , the Play framework sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. Since TheHive v4.1.1 , these values are set with default parameters: # Max file size play.http.parser.maxDiskBuffer : 128MB # Max textual content length play.http.parser.maxMemoryBuffer : 256kB If you feel that these should be updated, edit /etc/thehive/application.conf file and update these parameters accordingly. Tip if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"Service"},{"location":"thehive/installation-and-configuration/configuration/service/#service","text":"","title":"Service"},{"location":"thehive/installation-and-configuration/configuration/service/#listen-address-port","text":"By default the application listens on all interfaces and port 9000 . This is possible to specify listen address and ports with following parameters in the application.conf file: http.address=127.0.0.1 http.port=9000","title":"Listen address &amp; port"},{"location":"thehive/installation-and-configuration/configuration/service/#context","text":"If you are using a reverse proxy, and you want to specify a location (ex: /thehive ), updating the configuration of TheHive is also required Example play.http.context: \"/thehive\"","title":"Context"},{"location":"thehive/installation-and-configuration/configuration/service/#specific-configuration-for-streams","text":"If you are using a reverse proxy like Nginx, you might receive error popups with the following message: StreamSrv 504 Gateway Time-Out . You need to change default setting for long polling refresh, Set stream.longPolling.refresh accordingly. Example stream.longPolling.refresh: 45 seconds","title":"Specific configuration for streams"},{"location":"thehive/installation-and-configuration/configuration/service/#manage-content-length","text":"Content length of text and files managed by the application are limited by default. Before TheHive v4.1.1 , the Play framework sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. Since TheHive v4.1.1 , these values are set with default parameters: # Max file size play.http.parser.maxDiskBuffer : 128MB # Max textual content length play.http.parser.maxMemoryBuffer : 256kB If you feel that these should be updated, edit /etc/thehive/application.conf file and update these parameters accordingly. Tip if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"Manage content length"},{"location":"thehive/installation-and-configuration/configuration/ssl/","text":"SSL # Server configuration # We recommend using a reverse proxy to manage SSL layer. Connectors # Refer to Cortex or MISP configuration to setup specific SSL configuration of these remote services. Client configuration # SSL configuration might be requis required to connect remote services. Following parameters can be defined: Parameter Type Description wsConfig.ssl.keyManager.stores list Stores client certificates (see #certificate-manager ) wsConfig.ssl.trustManager.stores list Stored custom Certificate Authorities (see #certificate-manager wsConfig.ssl.protocol string Defines a different default protocol (see #protocols ) wsConfig.ssl.enabledProtocols list List of enabled protocols (see #protocols ) wsConfig.ssl.enabledCipherSuites list List of enabled cipher suites (see #ciphers ) wsConfig.ssl.loose.acceptAnyCertificate boolean Accept any certificates true / false Certificate manager # Certificate manager is used to store client certificates and certificate authorities. Custom Certificate Authority # Global configuration # If setting up a custom Certificate Authority (to connect web proxies, remote services ...) is required globally in the application, the better solution consists of installing it on the OS and restarting TheHive. Debian apt-get install -y ca-certificates-java mkdir /usr/share/ca-certificates/extra cp mysctomcert.crt /usr/share/ca-certificates/extra dpkg-reconfigure ca-certificates service thehive restart Use dedicated trust stores # the other way, is to use the trustManager key in TheHive configuration. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. wsConfig.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } Client certificates # keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) wsConfig.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] } Protocols # If you want to define a different default protocol, you can set it specifically in the client: wsConfig.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: wsConfig.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] Ciphers # Cipher suites can be configured using wsConfig.ssl.enabledCipherSuites : wsConfig.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ] Debugging # To debug the key manager / trust manager, set the following flags: wsConfig.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true }","title":"SSL"},{"location":"thehive/installation-and-configuration/configuration/ssl/#ssl","text":"","title":"SSL"},{"location":"thehive/installation-and-configuration/configuration/ssl/#server-configuration","text":"We recommend using a reverse proxy to manage SSL layer.","title":"Server configuration"},{"location":"thehive/installation-and-configuration/configuration/ssl/#connectors","text":"Refer to Cortex or MISP configuration to setup specific SSL configuration of these remote services.","title":"Connectors"},{"location":"thehive/installation-and-configuration/configuration/ssl/#client-configuration","text":"SSL configuration might be requis required to connect remote services. Following parameters can be defined: Parameter Type Description wsConfig.ssl.keyManager.stores list Stores client certificates (see #certificate-manager ) wsConfig.ssl.trustManager.stores list Stored custom Certificate Authorities (see #certificate-manager wsConfig.ssl.protocol string Defines a different default protocol (see #protocols ) wsConfig.ssl.enabledProtocols list List of enabled protocols (see #protocols ) wsConfig.ssl.enabledCipherSuites list List of enabled cipher suites (see #ciphers ) wsConfig.ssl.loose.acceptAnyCertificate boolean Accept any certificates true / false","title":"Client configuration"},{"location":"thehive/installation-and-configuration/configuration/ssl/#certificate-manager","text":"Certificate manager is used to store client certificates and certificate authorities.","title":"Certificate manager"},{"location":"thehive/installation-and-configuration/configuration/ssl/#custom-certificate-authority","text":"","title":"Custom Certificate Authority"},{"location":"thehive/installation-and-configuration/configuration/ssl/#global-configuration","text":"If setting up a custom Certificate Authority (to connect web proxies, remote services ...) is required globally in the application, the better solution consists of installing it on the OS and restarting TheHive. Debian apt-get install -y ca-certificates-java mkdir /usr/share/ca-certificates/extra cp mysctomcert.crt /usr/share/ca-certificates/extra dpkg-reconfigure ca-certificates service thehive restart","title":"Global configuration"},{"location":"thehive/installation-and-configuration/configuration/ssl/#use-dedicated-trust-stores","text":"the other way, is to use the trustManager key in TheHive configuration. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. wsConfig.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] }","title":"Use dedicated trust stores"},{"location":"thehive/installation-and-configuration/configuration/ssl/#client-certificates","text":"keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) wsConfig.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] }","title":"Client certificates"},{"location":"thehive/installation-and-configuration/configuration/ssl/#protocols","text":"If you want to define a different default protocol, you can set it specifically in the client: wsConfig.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: wsConfig.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"]","title":"Protocols"},{"location":"thehive/installation-and-configuration/configuration/ssl/#ciphers","text":"Cipher suites can be configured using wsConfig.ssl.enabledCipherSuites : wsConfig.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ]","title":"Ciphers"},{"location":"thehive/installation-and-configuration/configuration/ssl/#debugging","text":"To debug the key manager / trust manager, set the following flags: wsConfig.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true }","title":"Debugging"},{"location":"thehive/installation-and-configuration/configuration/webhooks/","text":"TheHive webhooks # TheHive can notify external system of modification events (case creation, alert update, task assignment, ...). To use webhooks notifications, 2 steps are required: configure a notification, and activate it. 1. Define webhook endpoints # The configuration can accept following parameters: Parameter Type Description name string the identifier of the endpoint. It is used when the webhook is setup for an organisation version integer defines the format of the message. If version is 0 , TheHive will send messages with the same format as TheHive3. Currently TheHive only supports version 0. wsConfig dict the configuration of HTTP client. It contains proxy, SSL and timeout configuration. auth dict the configuration of authenticationI. It contains type, and additional options. includedTheHiveOrganisations list of string list of TheHive organisations which can use this endpoint (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this endpoint (default: None ( [] ) ) The following section should be added in application.conf : ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig : {} auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ] Use a proxy # Wehbook call can go through a proxy, in which case, Webhooks configuration requires a wsConfig config notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ] Use an authentication method # Webhook endpoints can be authenticated, in this case, Webhook configuration requires a auth setting. Supported methods are: No Auth (Default) Basic Auth Beared Auth Key Auth auth : { type : \"none\" } auth : { type : \"basic\" , username : \"foo\" , password : \"bar\" } auth : { type : \"bearer\" , key : \"foobar\" } auth : { type : \"key\" , key : \"foobar\" } Warning In 4.1.0 release, the auth config is REQUIRED . Examples # Example ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"bearer\" , key : \"API_KEY\" } includedTheHiveOrganisations : [ \"ORG1\" , \"ORG2\" ] excludedTheHiveOrganisations : [ \"ORG3\" ] } ] 2. Activate webhooks # This action must be done by an organisation admin (with permission manageConfig ) and requires to run a curl command: read -p 'Enter the URL of TheHive: ' thehive_url read -p 'Enter your login: ' thehive_user read -s -p 'Enter your password: ' thehive_password curl -XPUT -u $thehive_user : $thehive_password -H 'Content-type: application/json' $thehive_url /api/config/organisation/notification -d ' { \"value\": [ { \"delegate\": false, \"trigger\": { \"name\": \"AnyEvent\"}, \"notifier\": { \"name\": \"webhook\", \"endpoint\": \"local\" } } ] }'","title":"Webhooks"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#thehive-webhooks","text":"TheHive can notify external system of modification events (case creation, alert update, task assignment, ...). To use webhooks notifications, 2 steps are required: configure a notification, and activate it.","title":"TheHive webhooks"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#1-define-webhook-endpoints","text":"The configuration can accept following parameters: Parameter Type Description name string the identifier of the endpoint. It is used when the webhook is setup for an organisation version integer defines the format of the message. If version is 0 , TheHive will send messages with the same format as TheHive3. Currently TheHive only supports version 0. wsConfig dict the configuration of HTTP client. It contains proxy, SSL and timeout configuration. auth dict the configuration of authenticationI. It contains type, and additional options. includedTheHiveOrganisations list of string list of TheHive organisations which can use this endpoint (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this endpoint (default: None ( [] ) ) The following section should be added in application.conf : ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig : {} auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ]","title":"1. Define webhook endpoints"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#use-a-proxy","text":"Wehbook call can go through a proxy, in which case, Webhooks configuration requires a wsConfig config notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ]","title":"Use a proxy"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#use-an-authentication-method","text":"Webhook endpoints can be authenticated, in this case, Webhook configuration requires a auth setting. Supported methods are: No Auth (Default) Basic Auth Beared Auth Key Auth auth : { type : \"none\" } auth : { type : \"basic\" , username : \"foo\" , password : \"bar\" } auth : { type : \"bearer\" , key : \"foobar\" } auth : { type : \"key\" , key : \"foobar\" } Warning In 4.1.0 release, the auth config is REQUIRED .","title":"Use an authentication method"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#examples","text":"Example ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"bearer\" , key : \"API_KEY\" } includedTheHiveOrganisations : [ \"ORG1\" , \"ORG2\" ] excludedTheHiveOrganisations : [ \"ORG3\" ] } ]","title":"Examples"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#2-activate-webhooks","text":"This action must be done by an organisation admin (with permission manageConfig ) and requires to run a curl command: read -p 'Enter the URL of TheHive: ' thehive_url read -p 'Enter your login: ' thehive_user read -s -p 'Enter your password: ' thehive_password curl -XPUT -u $thehive_user : $thehive_password -H 'Content-type: application/json' $thehive_url /api/config/organisation/notification -d ' { \"value\": [ { \"delegate\": false, \"trigger\": { \"name\": \"AnyEvent\"}, \"notifier\": { \"name\": \"webhook\", \"endpoint\": \"local\" } } ] }'","title":"2. Activate webhooks"},{"location":"thehive/installation-and-configuration/installation/build-sources/","text":"Installing and running from sources # Dependencies # System packages # apt-get install apt-transport-https NPM # curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.0/install.sh | bash Bower and Grunt # nvm install --lts npm install -g bower grunt Build # The backend cd /opt git clone https://github.com/TheHive-Project/TheHive.git cd TheHive git checkout scalligraph git submodule init git submodule update ./sbt stage The UI cd /opt/TheHive/frontend npm install bower install grunt build","title":"Build sources"},{"location":"thehive/installation-and-configuration/installation/build-sources/#installing-and-running-from-sources","text":"","title":"Installing and running from sources"},{"location":"thehive/installation-and-configuration/installation/build-sources/#dependencies","text":"","title":"Dependencies"},{"location":"thehive/installation-and-configuration/installation/build-sources/#system-packages","text":"apt-get install apt-transport-https","title":"System packages"},{"location":"thehive/installation-and-configuration/installation/build-sources/#npm","text":"curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.0/install.sh | bash","title":"NPM"},{"location":"thehive/installation-and-configuration/installation/build-sources/#bower-and-grunt","text":"nvm install --lts npm install -g bower grunt","title":"Bower and Grunt"},{"location":"thehive/installation-and-configuration/installation/build-sources/#build","text":"The backend cd /opt git clone https://github.com/TheHive-Project/TheHive.git cd TheHive git checkout scalligraph git submodule init git submodule update ./sbt stage The UI cd /opt/TheHive/frontend npm install bower install grunt build","title":"Build"},{"location":"thehive/installation-and-configuration/installation/hadoop/","text":"Hadoop: installation and configuration # This guide proposes an example of installation and configuration of Apache Hadoop . Installation # Download hadoop distribution from https://hadoop.apache.org/releases.html and uncompress. cd /tmp wget https://downloads.apache.org/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz cd /opt tar zxf /tmp/hadoop-3.1.3.tar.gz ln -s hadoop-3.1.3 hadoop Create a user and update permissions useradd hadoop chown hadoop:root -R /opt/hadoop* Create a datastore and set permissions mkdir /opt/thp/thehive/hdfs chown hadoop:root -R /opt/thp/thehive/hdfs Create ssh keys for hadoop user: su - hadoop ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 0600 ~/.ssh/authorized_keys Update .bashrc file for hadoop user. Add following lines: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export HADOOP_HOME=/opt/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME Note : Apache has a well detailed documentation for more advanced configuration with Hadoop. Configuration the Hadoop Master # Configuration files are located in etc/hadoop ( /opt/hadoop/etc/hadoop ). They must be identical in all nodes. Notes : The configuration described there is for a single node server. This node is the master node, namenode and datanode (refer to Hadoop documentation for more information). After validating this node is running successfully, refer to the related guide to add nodes; Ensure you update the port value to something different than 9000 as it is already reserved for TheHive application service; Edit the file core-site.xml : <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://thehive1:10000 </value> </property> <property> <name> hadoop.tmp.dir </name> <value> /opt/thp/thehive/hdfs/temp </value> </property> <property> <name> dfs.client.block.write.replace-datanode-on-failure.best-effort </name> <value> true </value> </property> </configuration> Edit the file hdfs-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> dfs.replication </name> <value> 2 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/thp/thehive/hdfs/namenode/data </value> </property> <property> <name> dfs.datanode.name.dir </name> <value> /opt/thp/thehive/hdfs/datanode/data </value> </property> <property> <name> dfs.namenode.checkpoint.dir </name> <value> /opt/thp/thehive/hdfs/checkpoint </value> </property> <property> <name> dfs.namenode.http-address </name> <value> 0.0.0.0:9870 </value> </property> <!-- <property> <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name> <value>true</value> </property> --> <property> <name> dfs.client.block.write.replace-datanode-on-failure.policy </name> <value> NEVER </value> </property> </configuration> Format the volume and start services # Format the volume su - hadoop cd /opt/hadoop bin/hdfs namenode -format Run it as a service # Create the /etc/systemd/system/hadoop.service file with the following content: [Unit] Description=Hadoop Documentation=https://hadoop.apache.org/docs/current/index.html Wants=network-online.target After=network-online.target [Service] WorkingDirectory=/opt/hadoop Type=forking User=hadoop Group=hadoop Environment=JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 Environment=HADOOP_HOME=/opt/hadoop Environment=YARN_HOME=/opt/hadoop Environment=HADOOP_COMMON_HOME=/opt/hadoop Environment=HADOOP_HDFS_HOME=/opt/hadoop Environment=HADOOP_MAPRED_HOME=/opt/hadoop Restart=on-failure TimeoutStartSec=2min ExecStart=/opt/hadoop/sbin/start-all.sh ExecStop=/opt/hadoop/sbin/stop-all.sh StandardOutput=null StandardError=null # Specifies the maximum file descriptor number that can be opened by this process LimitNOFILE=65536 # Disable timeout logic and wait until process is stopped TimeoutStopSec=0 # SIGTERM signal is used to stop the Java process KillSignal=SIGTERM # Java process is never killed SendSIGKILL=no [Install] WantedBy=multi-user.target Start the service # service hadoop start You can check cluster status in http://thehive1:9870 Add nodes # To add Hadoop nodes, refer the the related guide .","title":"Hadoop: installation and configuration"},{"location":"thehive/installation-and-configuration/installation/hadoop/#hadoop-installation-and-configuration","text":"This guide proposes an example of installation and configuration of Apache Hadoop .","title":"Hadoop: installation and configuration"},{"location":"thehive/installation-and-configuration/installation/hadoop/#installation","text":"Download hadoop distribution from https://hadoop.apache.org/releases.html and uncompress. cd /tmp wget https://downloads.apache.org/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz cd /opt tar zxf /tmp/hadoop-3.1.3.tar.gz ln -s hadoop-3.1.3 hadoop Create a user and update permissions useradd hadoop chown hadoop:root -R /opt/hadoop* Create a datastore and set permissions mkdir /opt/thp/thehive/hdfs chown hadoop:root -R /opt/thp/thehive/hdfs Create ssh keys for hadoop user: su - hadoop ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 0600 ~/.ssh/authorized_keys Update .bashrc file for hadoop user. Add following lines: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export HADOOP_HOME=/opt/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME Note : Apache has a well detailed documentation for more advanced configuration with Hadoop.","title":"Installation"},{"location":"thehive/installation-and-configuration/installation/hadoop/#configuration-the-hadoop-master","text":"Configuration files are located in etc/hadoop ( /opt/hadoop/etc/hadoop ). They must be identical in all nodes. Notes : The configuration described there is for a single node server. This node is the master node, namenode and datanode (refer to Hadoop documentation for more information). After validating this node is running successfully, refer to the related guide to add nodes; Ensure you update the port value to something different than 9000 as it is already reserved for TheHive application service; Edit the file core-site.xml : <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://thehive1:10000 </value> </property> <property> <name> hadoop.tmp.dir </name> <value> /opt/thp/thehive/hdfs/temp </value> </property> <property> <name> dfs.client.block.write.replace-datanode-on-failure.best-effort </name> <value> true </value> </property> </configuration> Edit the file hdfs-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> dfs.replication </name> <value> 2 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/thp/thehive/hdfs/namenode/data </value> </property> <property> <name> dfs.datanode.name.dir </name> <value> /opt/thp/thehive/hdfs/datanode/data </value> </property> <property> <name> dfs.namenode.checkpoint.dir </name> <value> /opt/thp/thehive/hdfs/checkpoint </value> </property> <property> <name> dfs.namenode.http-address </name> <value> 0.0.0.0:9870 </value> </property> <!-- <property> <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name> <value>true</value> </property> --> <property> <name> dfs.client.block.write.replace-datanode-on-failure.policy </name> <value> NEVER </value> </property> </configuration>","title":"Configuration the Hadoop Master"},{"location":"thehive/installation-and-configuration/installation/hadoop/#format-the-volume-and-start-services","text":"Format the volume su - hadoop cd /opt/hadoop bin/hdfs namenode -format","title":"Format the volume and start services"},{"location":"thehive/installation-and-configuration/installation/hadoop/#run-it-as-a-service","text":"Create the /etc/systemd/system/hadoop.service file with the following content: [Unit] Description=Hadoop Documentation=https://hadoop.apache.org/docs/current/index.html Wants=network-online.target After=network-online.target [Service] WorkingDirectory=/opt/hadoop Type=forking User=hadoop Group=hadoop Environment=JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 Environment=HADOOP_HOME=/opt/hadoop Environment=YARN_HOME=/opt/hadoop Environment=HADOOP_COMMON_HOME=/opt/hadoop Environment=HADOOP_HDFS_HOME=/opt/hadoop Environment=HADOOP_MAPRED_HOME=/opt/hadoop Restart=on-failure TimeoutStartSec=2min ExecStart=/opt/hadoop/sbin/start-all.sh ExecStop=/opt/hadoop/sbin/stop-all.sh StandardOutput=null StandardError=null # Specifies the maximum file descriptor number that can be opened by this process LimitNOFILE=65536 # Disable timeout logic and wait until process is stopped TimeoutStopSec=0 # SIGTERM signal is used to stop the Java process KillSignal=SIGTERM # Java process is never killed SendSIGKILL=no [Install] WantedBy=multi-user.target","title":"Run it as a service"},{"location":"thehive/installation-and-configuration/installation/hadoop/#start-the-service","text":"service hadoop start You can check cluster status in http://thehive1:9870","title":"Start the service"},{"location":"thehive/installation-and-configuration/installation/hadoop/#add-nodes","text":"To add Hadoop nodes, refer the the related guide .","title":"Add nodes"},{"location":"thehive/installation-and-configuration/installation/minio/","text":"","title":"Minio"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/","text":"Step-by-Step guide # This page is a step by step installation and configuration guide to get an TheHive 4 instance up and running. This guide is illustrated with examples for Debian and RPM packages based systems and for installation from binary packages. Java Virtual Machine # Debian RPM Other apt-get install -y openjdk-8-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" yum install -y java-1.8.0-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" The installation requires Java 8, so refer to your system documentation to install it. Note TheHive can be loaded by Java 11, but not the stable version of Cassandra, which still requires Java 8. If you set up a cluster for the database distinct from TheHive servers: Cassandra nodes can be loaded by Java 8 TheHive nodes can be loaded by Java 11 For standalone servers, with TheHive and Cassandra on the same OS, we recommend having only Java 8 installed for both applications. Cassandra database # Apache Cassandra is a scalable and high available database. TheHive supports the latest stable version 3.11.x of Cassandra. Install from repository # Debian RPM Other Add Apache repository references curl -fsSL https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list Install the package sudo apt update sudo apt install cassandra Add the Apache repository of Cassandra to /etc/yum.repos.d/cassandra.repo [ cassandra ] name = Apache Cassandra baseurl = https://downloads.apache.org/cassandra/redhat/311x/ gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://downloads.apache.org/cassandra/KEYS Install the package yum install -y cassandra Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice. By default, data is stored in /var/lib/cassandra . Configuration # Start by changing the cluster_name with thp . Run the command cqlsh : cqlsh localhost 9042 cqlsh > UPDATE system . local SET cluster_name = 'thp' where key = 'local' ; Exit and then run: nodetool flush Configure Cassandra by editing /etc/cassandra/cassandra.yaml file. # content from /etc/cassandra/cassandra.yaml cluster_name: 'thp' listen_address: 'xx.xx.xx.xx' # address for nodes rpc_address: 'xx.xx.xx.xx' # address for clients seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: # Ex: \"<ip1>,<ip2>,<ip3>\" - seeds: 'xx.xx.xx.xx' # self for the first node data_file_directories: - '/var/lib/cassandra/data' commitlog_directory: '/var/lib/cassandra/commitlog' saved_caches_directory: '/var/lib/cassandra/saved_caches' hints_directory: - '/var/lib/cassandra/hints' Then restart the service: Debian RPM service cassandra restart Run the service and ensure it restart after a reboot: systemctl daemon-reload service cassandra start chkconfig cassandra on Warning Cassandra service does not start well with the new systemd version. There is an existing issue and a fix on Apache website: https://issues.apache.org/jira/browse/CASSANDRA-15273 By default Cassandra listens on 7000/tcp (inter-node), 9042/tcp (client). Additional configuration # For additional configuration options, refer to: Cassandra documentation page Datastax documentation page Security # To add security measures in Cassandra , refer the the related administration guide . Add nodes # To add Cassandra nodes, refer the the related administration guide . Indexing engine # Starting from TheHive 4.1.0, a solution to store data indexes is required. These indexes should be unique and the same for all nodes of TheHive cluster. TheHive embed a Lucene engine you can use for standalone server For clusters setups, an instance of Elasticsearch is required Local lucene engine Elasticsearch Create a folder dedicated to host indexes for TheHive: mkdir /opt/thp/thehive/index chown thehive:thehive -R /opt/thp/thehive/index Use an existing Elasticsearch instance or install a new one. This instance should be reachable by all nodes of a cluster. Warning Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored Note Indexes will be created at the first start of TheHive. It can take a certain amount of time, depending the size of the database Like data and files, indexes should be part of the backup policy Indexes can removed and created again File storage # Files uploaded in TheHive (in task logs or in observables ) can be stores in localsystem, in a Hadoop filesystem (recommended) or in the graph database. For standalone production and test servers , we recommends using local filesystem. If you think about building a cluster with TheHive, you have several possible solutions: using Hadoop or S3 services ; see the related guide for more details and an example with MinIO servers. Local Filesystem S3 with Min.io HDFS with Hadoop Warning This option is perfect for standalone servers . If you intend to build a cluster for your instance of TheHive 4 we recommend: using a NFS share, common to all nodes having a look at storage solutions implementing S3 or HDFS. To store files on the local filesystem, start by choosing the dedicated folder: mkdir -p /opt/thp/thehive/files This path will be used in the configuration of TheHive. Later, after having installed TheHive, ensure the user thehive owns the path chosen for storing files: chown -R thehive:thehive /opt/thp/thehive/files An example of installing, configuring and use Min.IO is detailed in this documentation . An example of installing, configuring and use Apache Hadoop is detailed in this documentation . TheHive # This part contains instructions to install TheHive and then configure it. Warning TheHive4 can't be installed on the same server than older versions. We recommend installing it on a new server, especially if a migration is foreseen. Installation # All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . Debian RPM curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY We also release stable and beta version of the applications. Stable versions # Install TheHive 4.x package of the stable version by using the following commands: Debian RPM Other echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-latest.zip unzip thehive4-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service Beta versions # To install beta versions of TheHive4, use the following setup: Debian RPM Other echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-beta-latest.zip unzip thehive4-beta-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service Warning We recommend using or playing with Beta version for testing purpose only . Configuration # Following configurations are required to start TheHive successfully: Secret key configuration Database configuration File storage configuration Secret key configuration # Debian RPM Other The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. Setup a secret key in the /etc/thehive/secret.conf file by running the following command: cat > /etc/thehive/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ Database # To use Cassandra database, TheHive configuration file ( /etc/thehive/application.conf ) has to be edited and updated with following lines: db { provider : janusgraph janusgraph { storage { backend : cql hostname : [ \"127.0.0.1\" ] # seed node ip addresses #username: \"<cassandra_username>\" # login to connect to database (if configured in Cassandra) #password: \"<cassandra_passowrd\" cql { cluster-name : thp # cluster name keyspace : thehive # name of the keyspace local-datacenter : datacenter1 # name of the datacenter where TheHive runs (relevant only on multi datacenter setup) # replication-factor: 2 # number of replica read-consistency-level : ONE write-consistency-level : ONE } } } } Indexes # Update db.storage configuration part in /etc/thehive/application.conf accordingly to your setup. Lucene Elasticsearch If your setup is a standalone server or you are using a common NFS share, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : lucene directory : /opt/thp/thehive/index } } } If you decided to have access to a centralised index with Elasticsearch, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : elasticsearch hostname : [ \"10.1.2.20\" ] index-name : thehive } } } Filesystem # Local filesystem S3 HDFS If you chose to store files on the local filesystem: Ensure permission of the folder chown -R thehive:thehive /opt/thp/thehive/files add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider = localfs localfs.location = /opt/thp/thehive/files } If you chose MinIO and a S3 object storage system to store files in a filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_ADDRESS>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" } } If you chose Apache Hadoop and a HDFS filesystem to store files in a distrubuted filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : hdfs hdfs { root : \"hdfs://thehive1:10000\" # namenode server location : \"/thehive\" username : thehive } } Run # Save configuration file and run the service: service thehive start Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . The default admin user is admin@thehive.local with password secret . It is recommended to change the default password. Advanced configuration # For additional configuration options, please refer to the Configuration Guides .","title":"Step by step guide"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#step-by-step-guide","text":"This page is a step by step installation and configuration guide to get an TheHive 4 instance up and running. This guide is illustrated with examples for Debian and RPM packages based systems and for installation from binary packages.","title":"Step-by-Step guide"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#java-virtual-machine","text":"Debian RPM Other apt-get install -y openjdk-8-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" yum install -y java-1.8.0-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" The installation requires Java 8, so refer to your system documentation to install it. Note TheHive can be loaded by Java 11, but not the stable version of Cassandra, which still requires Java 8. If you set up a cluster for the database distinct from TheHive servers: Cassandra nodes can be loaded by Java 8 TheHive nodes can be loaded by Java 11 For standalone servers, with TheHive and Cassandra on the same OS, we recommend having only Java 8 installed for both applications.","title":"Java Virtual Machine"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#cassandra-database","text":"Apache Cassandra is a scalable and high available database. TheHive supports the latest stable version 3.11.x of Cassandra.","title":"Cassandra database"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#install-from-repository","text":"Debian RPM Other Add Apache repository references curl -fsSL https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list Install the package sudo apt update sudo apt install cassandra Add the Apache repository of Cassandra to /etc/yum.repos.d/cassandra.repo [ cassandra ] name = Apache Cassandra baseurl = https://downloads.apache.org/cassandra/redhat/311x/ gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://downloads.apache.org/cassandra/KEYS Install the package yum install -y cassandra Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice. By default, data is stored in /var/lib/cassandra .","title":"Install from repository"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#configuration","text":"Start by changing the cluster_name with thp . Run the command cqlsh : cqlsh localhost 9042 cqlsh > UPDATE system . local SET cluster_name = 'thp' where key = 'local' ; Exit and then run: nodetool flush Configure Cassandra by editing /etc/cassandra/cassandra.yaml file. # content from /etc/cassandra/cassandra.yaml cluster_name: 'thp' listen_address: 'xx.xx.xx.xx' # address for nodes rpc_address: 'xx.xx.xx.xx' # address for clients seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: # Ex: \"<ip1>,<ip2>,<ip3>\" - seeds: 'xx.xx.xx.xx' # self for the first node data_file_directories: - '/var/lib/cassandra/data' commitlog_directory: '/var/lib/cassandra/commitlog' saved_caches_directory: '/var/lib/cassandra/saved_caches' hints_directory: - '/var/lib/cassandra/hints' Then restart the service: Debian RPM service cassandra restart Run the service and ensure it restart after a reboot: systemctl daemon-reload service cassandra start chkconfig cassandra on Warning Cassandra service does not start well with the new systemd version. There is an existing issue and a fix on Apache website: https://issues.apache.org/jira/browse/CASSANDRA-15273 By default Cassandra listens on 7000/tcp (inter-node), 9042/tcp (client).","title":"Configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#additional-configuration","text":"For additional configuration options, refer to: Cassandra documentation page Datastax documentation page","title":"Additional configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#security","text":"To add security measures in Cassandra , refer the the related administration guide .","title":"Security"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#add-nodes","text":"To add Cassandra nodes, refer the the related administration guide .","title":"Add nodes"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#indexing-engine","text":"Starting from TheHive 4.1.0, a solution to store data indexes is required. These indexes should be unique and the same for all nodes of TheHive cluster. TheHive embed a Lucene engine you can use for standalone server For clusters setups, an instance of Elasticsearch is required Local lucene engine Elasticsearch Create a folder dedicated to host indexes for TheHive: mkdir /opt/thp/thehive/index chown thehive:thehive -R /opt/thp/thehive/index Use an existing Elasticsearch instance or install a new one. This instance should be reachable by all nodes of a cluster. Warning Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored Note Indexes will be created at the first start of TheHive. It can take a certain amount of time, depending the size of the database Like data and files, indexes should be part of the backup policy Indexes can removed and created again","title":"Indexing engine"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#file-storage","text":"Files uploaded in TheHive (in task logs or in observables ) can be stores in localsystem, in a Hadoop filesystem (recommended) or in the graph database. For standalone production and test servers , we recommends using local filesystem. If you think about building a cluster with TheHive, you have several possible solutions: using Hadoop or S3 services ; see the related guide for more details and an example with MinIO servers. Local Filesystem S3 with Min.io HDFS with Hadoop Warning This option is perfect for standalone servers . If you intend to build a cluster for your instance of TheHive 4 we recommend: using a NFS share, common to all nodes having a look at storage solutions implementing S3 or HDFS. To store files on the local filesystem, start by choosing the dedicated folder: mkdir -p /opt/thp/thehive/files This path will be used in the configuration of TheHive. Later, after having installed TheHive, ensure the user thehive owns the path chosen for storing files: chown -R thehive:thehive /opt/thp/thehive/files An example of installing, configuring and use Min.IO is detailed in this documentation . An example of installing, configuring and use Apache Hadoop is detailed in this documentation .","title":"File storage"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#thehive","text":"This part contains instructions to install TheHive and then configure it. Warning TheHive4 can't be installed on the same server than older versions. We recommend installing it on a new server, especially if a migration is foreseen.","title":"TheHive"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#installation","text":"All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . Debian RPM curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY We also release stable and beta version of the applications.","title":"Installation"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#stable-versions","text":"Install TheHive 4.x package of the stable version by using the following commands: Debian RPM Other echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-latest.zip unzip thehive4-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service","title":"Stable versions"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#beta-versions","text":"To install beta versions of TheHive4, use the following setup: Debian RPM Other echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-beta-latest.zip unzip thehive4-beta-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service Warning We recommend using or playing with Beta version for testing purpose only .","title":"Beta versions"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#configuration_1","text":"Following configurations are required to start TheHive successfully: Secret key configuration Database configuration File storage configuration","title":"Configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#secret-key-configuration","text":"Debian RPM Other The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. Setup a secret key in the /etc/thehive/secret.conf file by running the following command: cat > /etc/thehive/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_","title":"Secret key configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#database","text":"To use Cassandra database, TheHive configuration file ( /etc/thehive/application.conf ) has to be edited and updated with following lines: db { provider : janusgraph janusgraph { storage { backend : cql hostname : [ \"127.0.0.1\" ] # seed node ip addresses #username: \"<cassandra_username>\" # login to connect to database (if configured in Cassandra) #password: \"<cassandra_passowrd\" cql { cluster-name : thp # cluster name keyspace : thehive # name of the keyspace local-datacenter : datacenter1 # name of the datacenter where TheHive runs (relevant only on multi datacenter setup) # replication-factor: 2 # number of replica read-consistency-level : ONE write-consistency-level : ONE } } } }","title":"Database"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#indexes","text":"Update db.storage configuration part in /etc/thehive/application.conf accordingly to your setup. Lucene Elasticsearch If your setup is a standalone server or you are using a common NFS share, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : lucene directory : /opt/thp/thehive/index } } } If you decided to have access to a centralised index with Elasticsearch, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : elasticsearch hostname : [ \"10.1.2.20\" ] index-name : thehive } } }","title":"Indexes"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#filesystem","text":"Local filesystem S3 HDFS If you chose to store files on the local filesystem: Ensure permission of the folder chown -R thehive:thehive /opt/thp/thehive/files add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider = localfs localfs.location = /opt/thp/thehive/files } If you chose MinIO and a S3 object storage system to store files in a filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_ADDRESS>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" } } If you chose Apache Hadoop and a HDFS filesystem to store files in a distrubuted filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : hdfs hdfs { root : \"hdfs://thehive1:10000\" # namenode server location : \"/thehive\" username : thehive } }","title":"Filesystem"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#run","text":"Save configuration file and run the service: service thehive start Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . The default admin user is admin@thehive.local with password secret . It is recommended to change the default password.","title":"Run"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#advanced-configuration","text":"For additional configuration options, please refer to the Configuration Guides .","title":"Advanced configuration"},{"location":"thehive/legacy/thehive3/","text":"TheHive is a scalable 4-in-1 open source and free security incident response platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. Thanks to Cortex , our powerful free and open source analysis engine, you can analyze (and triage) observables at scale using more than 100 analyzers. Additionally and starting from TheHive 3.1.0, you can actively respond to threats and interact with your constituency and other parties thanks to Cortex responders. Last but not least, TheHive is highly integrated with MISP , the de facto standard of threat sharing, as it can pull events from several MISP instances and export investigation cases back to one or several ones. It also has additional features such as MISP extended events and health checking. This is TheHive's documentation repository. If you are looking for its source code, please visit https://github.com/TheHive-Project/TheHive/ . Hardware Pre-requisites # TheHive uses ElasticSearch to store data. Both software use a Java VM. We recommend using a virtual machine with 8vCPU, 8 GB of RAM and 60 GB of disk. You can also use a physical machine with similar specifications. Guides # Installation Guide Administration Guide Configuration Guide Webhooks Cluster Configuration Updating Backup & Restore Migration Guide API Documentation (incomplete) Miscellaneous Information # Feature Set (In Progress) Changelog Training Material Additional Resources Single Sign-On on TheHive with X.509 Certificates (Experimental Feature) License # TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run. Updates # Information, news and updates are regularly posted on TheHive Project Twitter account and on the blog . Contributing # We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing. Support # Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Important Note : If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository . Community Discussions # We have set up a Google forum at https://groups.google.com/a/thehive-project.org/d/forum/users . To request access, you need a Google account. You may create one using a Gmail address or without it . Website # https://thehive-project.org/","title":"Index"},{"location":"thehive/legacy/thehive3/#hardware-pre-requisites","text":"TheHive uses ElasticSearch to store data. Both software use a Java VM. We recommend using a virtual machine with 8vCPU, 8 GB of RAM and 60 GB of disk. You can also use a physical machine with similar specifications.","title":"Hardware Pre-requisites"},{"location":"thehive/legacy/thehive3/#guides","text":"Installation Guide Administration Guide Configuration Guide Webhooks Cluster Configuration Updating Backup & Restore Migration Guide API Documentation (incomplete)","title":"Guides"},{"location":"thehive/legacy/thehive3/#miscellaneous-information","text":"Feature Set (In Progress) Changelog Training Material Additional Resources Single Sign-On on TheHive with X.509 Certificates (Experimental Feature)","title":"Miscellaneous Information"},{"location":"thehive/legacy/thehive3/#license","text":"TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run.","title":"License"},{"location":"thehive/legacy/thehive3/#updates","text":"Information, news and updates are regularly posted on TheHive Project Twitter account and on the blog .","title":"Updates"},{"location":"thehive/legacy/thehive3/#contributing","text":"We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing.","title":"Contributing"},{"location":"thehive/legacy/thehive3/#support","text":"Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Important Note : If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository .","title":"Support"},{"location":"thehive/legacy/thehive3/#community-discussions","text":"We have set up a Google forum at https://groups.google.com/a/thehive-project.org/d/forum/users . To request access, you need a Google account. You may create one using a Gmail address or without it .","title":"Community Discussions"},{"location":"thehive/legacy/thehive3/#website","text":"https://thehive-project.org/","title":"Website"},{"location":"thehive/legacy/thehive3/feature-set/","text":"Feature set # This document lists the features provided by TheHive in either the UI or APIs and Webhooks. TheHive comes with the native support of integrating: one or more Cortex instances one or more MISP instances Authentication # TheHive supports multiple authentication methods: Local authentication using a local user collection AD authentication LDAP authentication SSO authentication X.509 certificates authentication Case Management # List and filter cases Create new cases from scratch or using case templates Add custom fields to cases Add metrics to cases Find linked cases to a given case based shared observables Add tasks and task groups to cases Assign tasks to a given user Add logs to tasks, including attachment to task logs Add observables to a case Execute Cortex responders against cases tasks task logs Delete cases by administrators only Alert Management # Alerts are a sort of incident not yet qualified as a Case. The Alerts sections allows: Listing and searching for alerts Marking alerts as read Ignoring alert updates Previewing alert details Display alert details and editable custom fields Display alerts observables Display similar cases Importing an alert as an emtpty case or using a case template Merging an alert into an existing case MISP Integration # MISP is natively integrated to TheHive allowing: The declaration of one or more MISP instances Each instance can be used to Import and/or Export events from MISP or cases to MISP Imported MISP events are made available as Alerts Imporing is configurable using filters (configuration files) Feeders # Feeders are external tools designed to send alerts to TheHive leveraging the REST APIs Thehive offers Feeders can be written and any programming language as long as it is compatible with TheHive APIs Feeders can be written in Python and use TheHive4Py Search capabilities # The search section provided by TheHive allows searching for the following objects using dynamic forms: cases tasks observables logs alerts Dashboarding # The dashboards section allows: creating private dashboards per user creating shared dashboads visible by all users adding widgets to dashboards using a drag & drop capabilities creating widgets that target cases, tasks, observables, alerts, jobs configuring widgets in a granular manner Administration # Case templates # Create case templates Add tasks to templates Add metrics to templates Add custom fields to templates Define default values for custom fields, metrics and tasks Export case template definitions Import case template definitions Metrics # List and Create metrics Custom fields # Create custom fields Update custom fields Users # List users Create/Edit users Set a user password Set a user API key Revoke a user's API key Lock a user Analyzer report templates # Report templates are used to display the raw reports from Cortex in a text format. This section allows: Importing short and long reports Customize short and long reports for each analyzer Cortex integration # TheHive uses Cortex to have access to analyzers and responsders Analyzers can be launched against observables to get more details about a given observable Responders can be launched against case, tasks, observables, logs, and alerts to execute an action One or more Cortex instances can be connected to TheHive Database migration # TheHive provides a mechanism to upgrade the Elasticsearch database by copying the index and making transformations on it.","title":"Feature set"},{"location":"thehive/legacy/thehive3/feature-set/#feature-set","text":"This document lists the features provided by TheHive in either the UI or APIs and Webhooks. TheHive comes with the native support of integrating: one or more Cortex instances one or more MISP instances","title":"Feature set"},{"location":"thehive/legacy/thehive3/feature-set/#authentication","text":"TheHive supports multiple authentication methods: Local authentication using a local user collection AD authentication LDAP authentication SSO authentication X.509 certificates authentication","title":"Authentication"},{"location":"thehive/legacy/thehive3/feature-set/#case-management","text":"List and filter cases Create new cases from scratch or using case templates Add custom fields to cases Add metrics to cases Find linked cases to a given case based shared observables Add tasks and task groups to cases Assign tasks to a given user Add logs to tasks, including attachment to task logs Add observables to a case Execute Cortex responders against cases tasks task logs Delete cases by administrators only","title":"Case Management"},{"location":"thehive/legacy/thehive3/feature-set/#alert-management","text":"Alerts are a sort of incident not yet qualified as a Case. The Alerts sections allows: Listing and searching for alerts Marking alerts as read Ignoring alert updates Previewing alert details Display alert details and editable custom fields Display alerts observables Display similar cases Importing an alert as an emtpty case or using a case template Merging an alert into an existing case","title":"Alert Management"},{"location":"thehive/legacy/thehive3/feature-set/#misp-integration","text":"MISP is natively integrated to TheHive allowing: The declaration of one or more MISP instances Each instance can be used to Import and/or Export events from MISP or cases to MISP Imported MISP events are made available as Alerts Imporing is configurable using filters (configuration files)","title":"MISP Integration"},{"location":"thehive/legacy/thehive3/feature-set/#feeders","text":"Feeders are external tools designed to send alerts to TheHive leveraging the REST APIs Thehive offers Feeders can be written and any programming language as long as it is compatible with TheHive APIs Feeders can be written in Python and use TheHive4Py","title":"Feeders"},{"location":"thehive/legacy/thehive3/feature-set/#search-capabilities","text":"The search section provided by TheHive allows searching for the following objects using dynamic forms: cases tasks observables logs alerts","title":"Search capabilities"},{"location":"thehive/legacy/thehive3/feature-set/#dashboarding","text":"The dashboards section allows: creating private dashboards per user creating shared dashboads visible by all users adding widgets to dashboards using a drag & drop capabilities creating widgets that target cases, tasks, observables, alerts, jobs configuring widgets in a granular manner","title":"Dashboarding"},{"location":"thehive/legacy/thehive3/feature-set/#administration","text":"","title":"Administration"},{"location":"thehive/legacy/thehive3/feature-set/#case-templates","text":"Create case templates Add tasks to templates Add metrics to templates Add custom fields to templates Define default values for custom fields, metrics and tasks Export case template definitions Import case template definitions","title":"Case templates"},{"location":"thehive/legacy/thehive3/feature-set/#metrics","text":"List and Create metrics","title":"Metrics"},{"location":"thehive/legacy/thehive3/feature-set/#custom-fields","text":"Create custom fields Update custom fields","title":"Custom fields"},{"location":"thehive/legacy/thehive3/feature-set/#users","text":"List users Create/Edit users Set a user password Set a user API key Revoke a user's API key Lock a user","title":"Users"},{"location":"thehive/legacy/thehive3/feature-set/#analyzer-report-templates","text":"Report templates are used to display the raw reports from Cortex in a text format. This section allows: Importing short and long reports Customize short and long reports for each analyzer","title":"Analyzer report templates"},{"location":"thehive/legacy/thehive3/feature-set/#cortex-integration","text":"TheHive uses Cortex to have access to analyzers and responsders Analyzers can be launched against observables to get more details about a given observable Responders can be launched against case, tasks, observables, logs, and alerts to execute an action One or more Cortex instances can be connected to TheHive","title":"Cortex integration"},{"location":"thehive/legacy/thehive3/feature-set/#database-migration","text":"TheHive provides a mechanism to upgrade the Elasticsearch database by copying the index and making transformations on it.","title":"Database migration"},{"location":"thehive/legacy/thehive3/migration-guide/","text":"Migration guide # From 3.4.x to 3.5.0 # Taking into account the EoL of version 6.x. of Elasticsearch, TheHive 3.5.0 is the first version to support Elasticsearch 7.x. This version introduce breaking changes. This time, we had no choice, we were not able to make TheHive support smoothly the ES upgrade. TheHive 3.5.0 supports Elasticsearch 7.x ONLY . This first steps before starting the upgrade process are: Identify the version of Elasticsearch which created your index Stop TheHive service Stop Elasticsearch service How to identify the version of Elasticsearch which created your database index ? # The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Run the following command : curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created_string' if the output is similar to \"5.x\" then your database index has been created with Elasticsearch 5.x reindexing is required, you should follow a dedicated process to upgrade . If it is \"6.x\" then your database has been created with Elasticsearch 6. Your database was created with Elasticsearch 5.x or earlier # This is where things might be complicated. This upgrade progress requires handling the database index by updating parameters, and reindex before updating Elasticsearch, and updating TheHive. Read carefully the dedicated documentation . It should help you run this specific actions on your Elasticsearch database, and also install or update application whether you are using DEB, RPM or binary packages, and even docker images. Your database was created with Elasticsearch 6.x # If you started using TheHive with Elasticsearch 6.x, then you just need to update the configuration of Elasticsearch to reflect this one: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Following parameters * are not accepted anymore by Elasticsearch 7: thread_pool.index.queue_size thread_pool.bulk.queue_size With TheHive service stopped, ensure the new version of Elasticsearch starts. If everything is ok, then TheHive 3.5.0 can be installed. To run this operation successfully, you need to update your repository configuration if you are using DEB and RPM packages, or specify the right version to install if using docker. Read carefully the installation guide . From 3.3.x to 3.4.0 # Starting from version 3.4.0-RC1, TheHive supports Elasticsearch 6 and will continue to work with Elasticsearch 5.x. TheHive 3.4.0-RC1 and later versions communicate with Elasticsearch using its HTTP service (9200/tcp by default) instead of its legacy binary protocol (9300/tcp by default). If you have a firewall between TheHive and Elasticsearch, you probably need to update its rules to change to the new port number. The configuration file ( application.conf ) needs some modifications to reflect the protocol change: The setting search.host is replaced by search.uri The general format of the URI is: http(s)://host:port,host:port(/prefix)?querystring . Multiple host:port combinations can be specified, separated by commas. Options can be specified using a standard URI query string syntax, eg. cluster.name=hive . The search.cluster setting is no longer used. Authentication can be configured with the search.user and search.password settings. When SSL/TLS is enabled, you can set a truststore and a keystore. The truststore contains the certificate authorities used to validate remote certificates. The keystore contains the certificate and the private key used to connect to the Elasticsearch cluster. The configuration is: search { keyStore { path: \"/path/to/keystore/file\" type: \"JKS\" # or PKCS12 password: \"secret.password.of.keystore\" } trustStore { path: \"/path/to/truststore/file\" type: \"JKS\" password: \"secret.password.of.truststore\" } } The Elasticsearch client also accepts the following settings: - circularRedirectsAllowed ( true / false ) - connectionRequestTimeout (number of seconds) - connectTimeout - contentCompressionEnabled.foreach(requestConfigBuilder.setContentCompressionEnabled) - search.cookieSpec (??) - expectContinueEnabled ( true / false ) - maxRedirects (number) - proxy -- not yet supported - proxyPreferredAuthSchemes -- not yet supported - redirectsEnabled ( true / false ) - relativeRedirectsAllowed ( true / false ) - socketTimeout (number of seconds) - targetPreferredAuthSchemes (??) The configuration items keepalive , pageSize , nbshards and nbreplicas are still valid. For practical details, you can have a look here for an example of migration of TheHive and Elasticsearch. From 3.0.x to 3.0.4 # TheHive 3.0.4 (Cerana 0.4) comes with new MISP settings to filter events that will be imported as alerts. Please refer to MISP event filters configuration section. The maximum number of custom fields and metrics in a case is 50 by default. If you try to put more, ElasticSearch will raise an error. You can now increase the limit by adding in your application.conf: index { settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } } The data schema has been changed in Cerana to support some dashboard features. At the first connection, TheHive will ask you to migrate the data. A new index, called the_hive_13 by default, will be created then. See Updating . From 2.13.x to 3.0.0 # The schema of data has been changed in Cerana to integrate dashboard. At the first request, TheHive will ask you to migrate the data. A new index, called the_hive_12 by default, will be created then. See Updating . From 2.13.0 or 2.13.1 to 2.13.2 # At the first connection to TheHive 2.13.2, a migration of the database will be asked. This will create a new ElasticSearch index ( the_hive_11 by default). See Updating . From 2.12.x to 2.13.x # Configuration updates # play.crypto.secret is deprecated, use play.http.secret.key instead. auth.type is deprecated, use auth.provider instead. Basic authentication is disabled by default. We strongly recommand to update the clients that rely on the API to interact with TheHive to use the new API key authentication method . This feature has been added in this release. If you need to enable basic authentication, use auth.method.basic=true in application.conf Note that the TheHive4Py 1.3.0 Python library also adds API key authentication support. Alert role # A new role \"alert\" has been added. Only users with this role can create an alert. If you have tool that uses TheHive API to create alerts, you must give the ability to do it in user administration. ElasticSearch # TheHive 2.13 uses ElasticSearch 5.x. Our tests have been done on ElasticSearch 5.5. So we recommend to use this specific version, even if TheHive should work perfectly with ElasticSearch 5.6 that doesn't introduce breaking changes. Data structure migration # Before upgrading ElasticSearch, backup all your indices . Then remove all indices except the last index of TheHive (most probably the_hive_10). You can list all indices with the following command: curl http://127.0.0.1:9200/_cat/indices ElasticSearch has changed the structure of its data directory (please refer to Path to data on disk ). The node name in the path where data are stored (DATA_DIR) must be removed. Stop ElasticSearch and execute the following lines to change the directory structure: echo -n 'Enter the path of ElasticSearch data: ' read DATA_DIR echo -n 'Enter the name of your cluster [hive]: ' read CLUSTER_NAME mv ${DATA_DIR}/${CLUSTER_NAME:=hive}/* ${DATA_DIR} rmdir ${DATA_DIR}/${CLUSTER_NAME} System requirements # ElasticSearch 5.x requires at least 262144 memory map areas (vm.max_map_count). Run sysctl -w vm.max_map_count=262144. To make this setting persistent after a server restart, add vm.max_map_count = 262144 in /etc/sysctl.conf (or to /etc/sysctl.d/80-elasticsearch.conf ) Configuration # The configuration of ElasticSearch should contain the following settings: http.host: 127.0.0.1 transport.host: 127.0.0.1 cluster.name: hive script.inline: true thread_pool.index.queue_size: 100000 thread_pool.search.queue_size: 100000 thread_pool.bulk.queue_size: 100000 Adapt http.host and transport.host to your environment. Docker # The default ElasticSearch image has been deprecated. It is recommended to use the docker image from Elastic.co . The new image doesn't use the same user ID so you need to change the owner of the data files. You can simply run chown -R 1000.1000 $DATA_DIR (DATA_DIR is the folder which contains ElasticSearch data). Then you can use the following script: docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --publish 127.0.0.1:9300:9300 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"transport.host=0.0.0.0\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:5.5.2 Note : TheHive doesn't support X-Pack. Don't enable it . Warnings You Can Safely Ignore with ES 5.5 # ElasticSearch 5.5 will output the following warnings: - unexpected docvalues type NONE for field '_parent' (expected one of [SORTED, SORTED_SET]). Re-index with correct docvalues type. You can safely ignore this message. For more information see issues #25849 and #26341 - License [will expire] on [***]. If you have a new license, please update it. Ignore this warning as TheHive doesn't use Elasticsearch's commercial features. Note : ElasticSearch 5.6 fixes those warnings. From 2.11.x to 2.12.x # Database migration # At the first connection to TheHive 2.12, a migration of the database will be asked. This will create a new ElasticSearch index (the_hive_10). See Updating . From 2.10.x to 2.11.x # Database migration # At the first connection to TheHive 2.11, a migration of the database will be asked. This will create a new ElastciSearch index (the_hive_9). See Updating . MISP to alert # MISP synchronization is now done using alerting framework. MISP events are seen like other alert. You can use TheHive4py to create your own alert. Configuration changes # MISP certificate authority deprecated # Specifying certificate authority in MISP configuration using \"cert\" key is now deprecated. You must replace it by - before: misp { [...] cert = \"/path/to/truststore.jks\" } - after: misp { [...] ws.ssl.trustManager.stores = [ { type: \"JKS\" path: \"/path/to/truststore.jks\" } ] } ws key can be placed in MISP server section or in global MISP section. In the latter, ws configuration will be applied on all MISP instances. Cortex and MISP HTTP client options # HTTP client used by Cortex and MISP is more configurable. Proxy can be configured, with or without authentication. Refer to configuration for all possible options. Packages # New RPM and DEB packages # RPM and DEB packages are now available. This makes the installation easier than using a binary package (ZIP). See the Installation Guide for reference. Docker # All-in-One docker (containing TheHive and Cortex) is not provided any longer. New TheHive docker image doesn't contain ElasticSearch. We recommend to use docker-compose to link TheHive, ElasticSearch and Cortex dockers. For more information, see the Installation Guide for reference. TheHive configuration is located in /etc/thehive/application.conf for all packages. If you use docker package you must update its location (previously was /opt/docker/conf/application.conf ).","title":"Migration guide"},{"location":"thehive/legacy/thehive3/migration-guide/#migration-guide","text":"","title":"Migration guide"},{"location":"thehive/legacy/thehive3/migration-guide/#from-34x-to-350","text":"Taking into account the EoL of version 6.x. of Elasticsearch, TheHive 3.5.0 is the first version to support Elasticsearch 7.x. This version introduce breaking changes. This time, we had no choice, we were not able to make TheHive support smoothly the ES upgrade. TheHive 3.5.0 supports Elasticsearch 7.x ONLY . This first steps before starting the upgrade process are: Identify the version of Elasticsearch which created your index Stop TheHive service Stop Elasticsearch service","title":"From 3.4.x to 3.5.0"},{"location":"thehive/legacy/thehive3/migration-guide/#how-to-identify-the-version-of-elasticsearch-which-created-your-database-index","text":"The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Run the following command : curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created_string' if the output is similar to \"5.x\" then your database index has been created with Elasticsearch 5.x reindexing is required, you should follow a dedicated process to upgrade . If it is \"6.x\" then your database has been created with Elasticsearch 6.","title":"How to identify the version of Elasticsearch which created your database index ?"},{"location":"thehive/legacy/thehive3/migration-guide/#your-database-was-created-with-elasticsearch-5x-or-earlier","text":"This is where things might be complicated. This upgrade progress requires handling the database index by updating parameters, and reindex before updating Elasticsearch, and updating TheHive. Read carefully the dedicated documentation . It should help you run this specific actions on your Elasticsearch database, and also install or update application whether you are using DEB, RPM or binary packages, and even docker images.","title":"Your database was created with Elasticsearch 5.x or earlier"},{"location":"thehive/legacy/thehive3/migration-guide/#your-database-was-created-with-elasticsearch-6x","text":"If you started using TheHive with Elasticsearch 6.x, then you just need to update the configuration of Elasticsearch to reflect this one: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Following parameters * are not accepted anymore by Elasticsearch 7: thread_pool.index.queue_size thread_pool.bulk.queue_size With TheHive service stopped, ensure the new version of Elasticsearch starts. If everything is ok, then TheHive 3.5.0 can be installed. To run this operation successfully, you need to update your repository configuration if you are using DEB and RPM packages, or specify the right version to install if using docker. Read carefully the installation guide .","title":"Your database was created with Elasticsearch 6.x"},{"location":"thehive/legacy/thehive3/migration-guide/#from-33x-to-340","text":"Starting from version 3.4.0-RC1, TheHive supports Elasticsearch 6 and will continue to work with Elasticsearch 5.x. TheHive 3.4.0-RC1 and later versions communicate with Elasticsearch using its HTTP service (9200/tcp by default) instead of its legacy binary protocol (9300/tcp by default). If you have a firewall between TheHive and Elasticsearch, you probably need to update its rules to change to the new port number. The configuration file ( application.conf ) needs some modifications to reflect the protocol change: The setting search.host is replaced by search.uri The general format of the URI is: http(s)://host:port,host:port(/prefix)?querystring . Multiple host:port combinations can be specified, separated by commas. Options can be specified using a standard URI query string syntax, eg. cluster.name=hive . The search.cluster setting is no longer used. Authentication can be configured with the search.user and search.password settings. When SSL/TLS is enabled, you can set a truststore and a keystore. The truststore contains the certificate authorities used to validate remote certificates. The keystore contains the certificate and the private key used to connect to the Elasticsearch cluster. The configuration is: search { keyStore { path: \"/path/to/keystore/file\" type: \"JKS\" # or PKCS12 password: \"secret.password.of.keystore\" } trustStore { path: \"/path/to/truststore/file\" type: \"JKS\" password: \"secret.password.of.truststore\" } } The Elasticsearch client also accepts the following settings: - circularRedirectsAllowed ( true / false ) - connectionRequestTimeout (number of seconds) - connectTimeout - contentCompressionEnabled.foreach(requestConfigBuilder.setContentCompressionEnabled) - search.cookieSpec (??) - expectContinueEnabled ( true / false ) - maxRedirects (number) - proxy -- not yet supported - proxyPreferredAuthSchemes -- not yet supported - redirectsEnabled ( true / false ) - relativeRedirectsAllowed ( true / false ) - socketTimeout (number of seconds) - targetPreferredAuthSchemes (??) The configuration items keepalive , pageSize , nbshards and nbreplicas are still valid. For practical details, you can have a look here for an example of migration of TheHive and Elasticsearch.","title":"From 3.3.x to 3.4.0"},{"location":"thehive/legacy/thehive3/migration-guide/#from-30x-to-304","text":"TheHive 3.0.4 (Cerana 0.4) comes with new MISP settings to filter events that will be imported as alerts. Please refer to MISP event filters configuration section. The maximum number of custom fields and metrics in a case is 50 by default. If you try to put more, ElasticSearch will raise an error. You can now increase the limit by adding in your application.conf: index { settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } } The data schema has been changed in Cerana to support some dashboard features. At the first connection, TheHive will ask you to migrate the data. A new index, called the_hive_13 by default, will be created then. See Updating .","title":"From 3.0.x to 3.0.4"},{"location":"thehive/legacy/thehive3/migration-guide/#from-213x-to-300","text":"The schema of data has been changed in Cerana to integrate dashboard. At the first request, TheHive will ask you to migrate the data. A new index, called the_hive_12 by default, will be created then. See Updating .","title":"From 2.13.x to 3.0.0"},{"location":"thehive/legacy/thehive3/migration-guide/#from-2130-or-2131-to-2132","text":"At the first connection to TheHive 2.13.2, a migration of the database will be asked. This will create a new ElasticSearch index ( the_hive_11 by default). See Updating .","title":"From 2.13.0 or 2.13.1 to 2.13.2"},{"location":"thehive/legacy/thehive3/migration-guide/#from-212x-to-213x","text":"","title":"From 2.12.x to 2.13.x"},{"location":"thehive/legacy/thehive3/migration-guide/#configuration-updates","text":"play.crypto.secret is deprecated, use play.http.secret.key instead. auth.type is deprecated, use auth.provider instead. Basic authentication is disabled by default. We strongly recommand to update the clients that rely on the API to interact with TheHive to use the new API key authentication method . This feature has been added in this release. If you need to enable basic authentication, use auth.method.basic=true in application.conf Note that the TheHive4Py 1.3.0 Python library also adds API key authentication support.","title":"Configuration updates"},{"location":"thehive/legacy/thehive3/migration-guide/#alert-role","text":"A new role \"alert\" has been added. Only users with this role can create an alert. If you have tool that uses TheHive API to create alerts, you must give the ability to do it in user administration.","title":"Alert role"},{"location":"thehive/legacy/thehive3/migration-guide/#elasticsearch","text":"TheHive 2.13 uses ElasticSearch 5.x. Our tests have been done on ElasticSearch 5.5. So we recommend to use this specific version, even if TheHive should work perfectly with ElasticSearch 5.6 that doesn't introduce breaking changes.","title":"ElasticSearch"},{"location":"thehive/legacy/thehive3/migration-guide/#data-structure-migration","text":"Before upgrading ElasticSearch, backup all your indices . Then remove all indices except the last index of TheHive (most probably the_hive_10). You can list all indices with the following command: curl http://127.0.0.1:9200/_cat/indices ElasticSearch has changed the structure of its data directory (please refer to Path to data on disk ). The node name in the path where data are stored (DATA_DIR) must be removed. Stop ElasticSearch and execute the following lines to change the directory structure: echo -n 'Enter the path of ElasticSearch data: ' read DATA_DIR echo -n 'Enter the name of your cluster [hive]: ' read CLUSTER_NAME mv ${DATA_DIR}/${CLUSTER_NAME:=hive}/* ${DATA_DIR} rmdir ${DATA_DIR}/${CLUSTER_NAME}","title":"Data structure migration"},{"location":"thehive/legacy/thehive3/migration-guide/#system-requirements","text":"ElasticSearch 5.x requires at least 262144 memory map areas (vm.max_map_count). Run sysctl -w vm.max_map_count=262144. To make this setting persistent after a server restart, add vm.max_map_count = 262144 in /etc/sysctl.conf (or to /etc/sysctl.d/80-elasticsearch.conf )","title":"System requirements"},{"location":"thehive/legacy/thehive3/migration-guide/#configuration","text":"The configuration of ElasticSearch should contain the following settings: http.host: 127.0.0.1 transport.host: 127.0.0.1 cluster.name: hive script.inline: true thread_pool.index.queue_size: 100000 thread_pool.search.queue_size: 100000 thread_pool.bulk.queue_size: 100000 Adapt http.host and transport.host to your environment.","title":"Configuration"},{"location":"thehive/legacy/thehive3/migration-guide/#docker","text":"The default ElasticSearch image has been deprecated. It is recommended to use the docker image from Elastic.co . The new image doesn't use the same user ID so you need to change the owner of the data files. You can simply run chown -R 1000.1000 $DATA_DIR (DATA_DIR is the folder which contains ElasticSearch data). Then you can use the following script: docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --publish 127.0.0.1:9300:9300 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"transport.host=0.0.0.0\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:5.5.2 Note : TheHive doesn't support X-Pack. Don't enable it .","title":"Docker"},{"location":"thehive/legacy/thehive3/migration-guide/#warnings-you-can-safely-ignore-with-es-55","text":"ElasticSearch 5.5 will output the following warnings: - unexpected docvalues type NONE for field '_parent' (expected one of [SORTED, SORTED_SET]). Re-index with correct docvalues type. You can safely ignore this message. For more information see issues #25849 and #26341 - License [will expire] on [***]. If you have a new license, please update it. Ignore this warning as TheHive doesn't use Elasticsearch's commercial features. Note : ElasticSearch 5.6 fixes those warnings.","title":"Warnings You Can Safely Ignore with ES 5.5"},{"location":"thehive/legacy/thehive3/migration-guide/#from-211x-to-212x","text":"","title":"From 2.11.x to 2.12.x"},{"location":"thehive/legacy/thehive3/migration-guide/#database-migration","text":"At the first connection to TheHive 2.12, a migration of the database will be asked. This will create a new ElasticSearch index (the_hive_10). See Updating .","title":"Database migration"},{"location":"thehive/legacy/thehive3/migration-guide/#from-210x-to-211x","text":"","title":"From 2.10.x to 2.11.x"},{"location":"thehive/legacy/thehive3/migration-guide/#database-migration_1","text":"At the first connection to TheHive 2.11, a migration of the database will be asked. This will create a new ElastciSearch index (the_hive_9). See Updating .","title":"Database migration"},{"location":"thehive/legacy/thehive3/migration-guide/#misp-to-alert","text":"MISP synchronization is now done using alerting framework. MISP events are seen like other alert. You can use TheHive4py to create your own alert.","title":"MISP to alert"},{"location":"thehive/legacy/thehive3/migration-guide/#configuration-changes","text":"","title":"Configuration changes"},{"location":"thehive/legacy/thehive3/migration-guide/#misp-certificate-authority-deprecated","text":"Specifying certificate authority in MISP configuration using \"cert\" key is now deprecated. You must replace it by - before: misp { [...] cert = \"/path/to/truststore.jks\" } - after: misp { [...] ws.ssl.trustManager.stores = [ { type: \"JKS\" path: \"/path/to/truststore.jks\" } ] } ws key can be placed in MISP server section or in global MISP section. In the latter, ws configuration will be applied on all MISP instances.","title":"MISP certificate authority deprecated"},{"location":"thehive/legacy/thehive3/migration-guide/#cortex-and-misp-http-client-options","text":"HTTP client used by Cortex and MISP is more configurable. Proxy can be configured, with or without authentication. Refer to configuration for all possible options.","title":"Cortex and MISP HTTP client options"},{"location":"thehive/legacy/thehive3/migration-guide/#packages","text":"","title":"Packages"},{"location":"thehive/legacy/thehive3/migration-guide/#new-rpm-and-deb-packages","text":"RPM and DEB packages are now available. This makes the installation easier than using a binary package (ZIP). See the Installation Guide for reference.","title":"New RPM and DEB packages"},{"location":"thehive/legacy/thehive3/migration-guide/#docker_1","text":"All-in-One docker (containing TheHive and Cortex) is not provided any longer. New TheHive docker image doesn't contain ElasticSearch. We recommend to use docker-compose to link TheHive, ElasticSearch and Cortex dockers. For more information, see the Installation Guide for reference. TheHive configuration is located in /etc/thehive/application.conf for all packages. If you use docker package you must update its location (previously was /opt/docker/conf/application.conf ).","title":"Docker"},{"location":"thehive/legacy/thehive3/admin/admin-guide/","text":"Administrator's guide # 1. User management # Users can be managed through the Administration > Users page. Only administrators may access it. Each user is identified by their login, full name and role. Please note that you still need to create user accounts if you use LDAP or Active Directory authentication. This is necessary for TheHive to retrieve their role and authenticate them against the local database, LDAP and/or AD directories. There are 4 roles currently: - read : all non-sensitive data can be read. With this role, a user can't make any change. They can't add a case, task, log or observable. They also can't run analyzers; - write : create, remove and change data of any type. This role is for standard users. write role inherits read rights; - admin : this role is reserved for TheHive administrators. Users with this role can manage user accounts, metrics, create case templates and observable data types. admin inherits write rights; - alert : users with this role can only create alerts. Warning : Please note that user accounts cannot be removed once they have been created, otherwise audit logs will refer to an unknown user. However, unwanted or unused accounts can be locked. 2. Case template management # Some cases may share the same structure (tags, tasks, description, metrics). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template. To create a template, as admin go into the administration menu, and open the \"Case templates\" item. In this screen, you can add, remove or change template. A template contains: * default severity * default tags * title prefix (can be changed by user at case creation) * default TLP * default default * task list (title and description) * metrics * custom fields Except for title prefix, task list and metrics, the user can change values defined in template. 3. Report template management # When TheHive is connected to a Cortex server, observables can be analyzed to get additional information on them. Cortex outputs reports in JSON format. In order to make reports more readable, you can configure report templates. Report templates convert JSON in to HTML using the AngularJS template engine. For each analyzer available in Cortex you can define two kinds of templates: short and long. A short report exposes synthetic information, shows in top of observable page. With short reports you can see a summary of all run analyzers. Long reports show detailed information only when the user selects the report. Raw data in JSON format is always available. Report templates can be configured in the Admin > Report templates menu. We offer report templates for default Cortex analyzers. A package with all report templates can be downloaded at https://download.thehive-project.org/report-templates.zip and can be injected using the Import templates button. 4. Metrics management # Metrics have been integrated to have relevant indicators about cases. Metrics are numerical values associated to cases (for example, the number of impacted users). Each metric has a name , a title and a description , defined by an administrator. When a metric is added to a case, it can't be removed and must be filled. Metrics are used to monitor business indicators, thanks to graphs. Metrics are defined globally. To create metrics, as admin go into the administration menu, and open the \"Case metrics\" item. Metrics are used to create statistics (\"Statistics\" item in the user profile menu). They can be filtered on time interval, and case with specific tags. For example you can show metrics of case with \"malspam\" tag on January 2016 : For graphs based on time, the user can choose metrics to show. They are aggregated on interval of time (by day, week, month of year) using a function (sum, min or max). Some metrics are predefined (in addition to those defined by administrator) like case handling duration (how much time the case had been open) and number of cases open or closed.","title":"Administrator's guide"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#administrators-guide","text":"","title":"Administrator's guide"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#1-user-management","text":"Users can be managed through the Administration > Users page. Only administrators may access it. Each user is identified by their login, full name and role. Please note that you still need to create user accounts if you use LDAP or Active Directory authentication. This is necessary for TheHive to retrieve their role and authenticate them against the local database, LDAP and/or AD directories. There are 4 roles currently: - read : all non-sensitive data can be read. With this role, a user can't make any change. They can't add a case, task, log or observable. They also can't run analyzers; - write : create, remove and change data of any type. This role is for standard users. write role inherits read rights; - admin : this role is reserved for TheHive administrators. Users with this role can manage user accounts, metrics, create case templates and observable data types. admin inherits write rights; - alert : users with this role can only create alerts. Warning : Please note that user accounts cannot be removed once they have been created, otherwise audit logs will refer to an unknown user. However, unwanted or unused accounts can be locked.","title":"1. User management"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#2-case-template-management","text":"Some cases may share the same structure (tags, tasks, description, metrics). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template. To create a template, as admin go into the administration menu, and open the \"Case templates\" item. In this screen, you can add, remove or change template. A template contains: * default severity * default tags * title prefix (can be changed by user at case creation) * default TLP * default default * task list (title and description) * metrics * custom fields Except for title prefix, task list and metrics, the user can change values defined in template.","title":"2. Case template management"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#3-report-template-management","text":"When TheHive is connected to a Cortex server, observables can be analyzed to get additional information on them. Cortex outputs reports in JSON format. In order to make reports more readable, you can configure report templates. Report templates convert JSON in to HTML using the AngularJS template engine. For each analyzer available in Cortex you can define two kinds of templates: short and long. A short report exposes synthetic information, shows in top of observable page. With short reports you can see a summary of all run analyzers. Long reports show detailed information only when the user selects the report. Raw data in JSON format is always available. Report templates can be configured in the Admin > Report templates menu. We offer report templates for default Cortex analyzers. A package with all report templates can be downloaded at https://download.thehive-project.org/report-templates.zip and can be injected using the Import templates button.","title":"3. Report template management"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#4-metrics-management","text":"Metrics have been integrated to have relevant indicators about cases. Metrics are numerical values associated to cases (for example, the number of impacted users). Each metric has a name , a title and a description , defined by an administrator. When a metric is added to a case, it can't be removed and must be filled. Metrics are used to monitor business indicators, thanks to graphs. Metrics are defined globally. To create metrics, as admin go into the administration menu, and open the \"Case metrics\" item. Metrics are used to create statistics (\"Statistics\" item in the user profile menu). They can be filtered on time interval, and case with specific tags. For example you can show metrics of case with \"malspam\" tag on January 2016 : For graphs based on time, the user can choose metrics to show. They are aggregated on interval of time (by day, week, month of year) using a function (sum, min or max). Some metrics are predefined (in addition to those defined by administrator) like case handling duration (how much time the case had been open) and number of cases open or closed.","title":"4. Metrics management"},{"location":"thehive/legacy/thehive3/admin/backup-restore/","text":"Backup and restore data # All persistent data is stored in an Elasticsearch database. The backup and restore procedures are the ones that are detailed in Elasticsearch documentation . Note : you may have to adapt your indices in the examples below. To find the right index, use the following command : curl 'localhost:9200/_cat/indices?v' You can also refer to the schema version page. To save all your data you only need to backup the last indice. For example, if the previous command gives you the following results, all your data belongs to the_hive_12 . health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open the_hive_11 HVVYDC68SrGAfSbcjVPZWg 5 1 43018 17 24.9mb 24.9mb yellow open the_hive_12 Cq4Gc4qkRPaTCqrorFgDRw 5 1 43226 0 25.3mb 25.3mb In the rest of this document, ensure to change to your own last index in order to backup or restore all your data. 1. Create a backup repository # First you must define a location in local filesystem (where Elasticsearch instance runs) where the backup will be written. This repository must be declared in the Elasticsearch configuration. Edit elasticsearch.yml file by adding: path.repo: [\"/absolute/path/to/backup/directory\"] Then, restart the Elasticsearch service. Note : Be careful if you run Elasticsearch in Docker, the directory must be mapped in host filesystem using --volume parameter (cf. Docker documentation ). 2. Register a snapshot repository # Create an Elasticsearch snapshot point named the_hive_backup with the following command (set the same path in the location setting as the one set in the configuration file): $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -H 'Content-Type: application/json' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' The result of the command should look like this : {\"acknowledged\":true} Since, everything is fine to backup and restore data. 3. Backup your data # Create a backup named snapshot_1 of all your data by executing the following command : $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -H 'Content-Type: application/json' -d '{ \"indices\": \"<INDEX>\" }' This command terminates only when the backup is complete and the result of the command should look like this: { \"snapshots\": [{ \"snapshot\": \"snapshot_1\", \"uuid\": \"ZQ3kv5-FQoeN3NFIhfKgMg\", \"version_id\": 5060099, \"version\": \"5.6.0\", \"indices\": [\"the_hive_12\"], \"state\": \"SUCCESS\", \"start_time\": \"2018-01-29T14:41:51.580Z\", \"start_time_in_millis\": 1517236911580, \"end_time\": \"2018-01-29T14:42:05.216Z\", \"end_time_in_millis\": 1517236925216, \"duration_in_millis\": 13636, \"failures\": [], \"shards\": { \"total\": 41, \"failed\": 0, \"successful\": 41 } }] } Note : You can backup the last index of TheHive (you can list indices in your Elasticsearch cluster with curl -s http://localhost:9200/_cat/indices | cut -d ' ' -f3 ) or all indices with _all value. 4. Restore data # Restore will do the reverse actions : it reads the backup in your snapshot directory and loads indices into the Elasticsearch cluster. This operation is done with the following command : $ curl -XPOST 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1/_restore' -d ' { \"indices\": \"<INDEX>\" }' The result of the command should look like this : {\"accepted\":true} Note : be sure to restore data from the same version of Elasticsearch. 5. Moving data from one server to another # If you want to move your data from one server from another: - Create your backup on the origin server (steps 1 , 2 , 3 ) - copy your backup directory from the origin server to the destination server - On the destination server : - Register your backup repository in the Elasticsearch configuration (step 1 ) - Register your snapshot repository with the same snapshot name (step 2 ) - Restore your data (step 4 )","title":"Backup and restore data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#backup-and-restore-data","text":"All persistent data is stored in an Elasticsearch database. The backup and restore procedures are the ones that are detailed in Elasticsearch documentation . Note : you may have to adapt your indices in the examples below. To find the right index, use the following command : curl 'localhost:9200/_cat/indices?v' You can also refer to the schema version page. To save all your data you only need to backup the last indice. For example, if the previous command gives you the following results, all your data belongs to the_hive_12 . health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open the_hive_11 HVVYDC68SrGAfSbcjVPZWg 5 1 43018 17 24.9mb 24.9mb yellow open the_hive_12 Cq4Gc4qkRPaTCqrorFgDRw 5 1 43226 0 25.3mb 25.3mb In the rest of this document, ensure to change to your own last index in order to backup or restore all your data.","title":"Backup and restore data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#1-create-a-backup-repository","text":"First you must define a location in local filesystem (where Elasticsearch instance runs) where the backup will be written. This repository must be declared in the Elasticsearch configuration. Edit elasticsearch.yml file by adding: path.repo: [\"/absolute/path/to/backup/directory\"] Then, restart the Elasticsearch service. Note : Be careful if you run Elasticsearch in Docker, the directory must be mapped in host filesystem using --volume parameter (cf. Docker documentation ).","title":"1. Create a backup repository"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#2-register-a-snapshot-repository","text":"Create an Elasticsearch snapshot point named the_hive_backup with the following command (set the same path in the location setting as the one set in the configuration file): $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -H 'Content-Type: application/json' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' The result of the command should look like this : {\"acknowledged\":true} Since, everything is fine to backup and restore data.","title":"2. Register a snapshot repository"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#3-backup-your-data","text":"Create a backup named snapshot_1 of all your data by executing the following command : $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -H 'Content-Type: application/json' -d '{ \"indices\": \"<INDEX>\" }' This command terminates only when the backup is complete and the result of the command should look like this: { \"snapshots\": [{ \"snapshot\": \"snapshot_1\", \"uuid\": \"ZQ3kv5-FQoeN3NFIhfKgMg\", \"version_id\": 5060099, \"version\": \"5.6.0\", \"indices\": [\"the_hive_12\"], \"state\": \"SUCCESS\", \"start_time\": \"2018-01-29T14:41:51.580Z\", \"start_time_in_millis\": 1517236911580, \"end_time\": \"2018-01-29T14:42:05.216Z\", \"end_time_in_millis\": 1517236925216, \"duration_in_millis\": 13636, \"failures\": [], \"shards\": { \"total\": 41, \"failed\": 0, \"successful\": 41 } }] } Note : You can backup the last index of TheHive (you can list indices in your Elasticsearch cluster with curl -s http://localhost:9200/_cat/indices | cut -d ' ' -f3 ) or all indices with _all value.","title":"3. Backup your data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#4-restore-data","text":"Restore will do the reverse actions : it reads the backup in your snapshot directory and loads indices into the Elasticsearch cluster. This operation is done with the following command : $ curl -XPOST 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1/_restore' -d ' { \"indices\": \"<INDEX>\" }' The result of the command should look like this : {\"accepted\":true} Note : be sure to restore data from the same version of Elasticsearch.","title":"4. Restore data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#5-moving-data-from-one-server-to-another","text":"If you want to move your data from one server from another: - Create your backup on the origin server (steps 1 , 2 , 3 ) - copy your backup directory from the origin server to the destination server - On the destination server : - Register your backup repository in the Elasticsearch configuration (step 1 ) - Register your snapshot repository with the same snapshot name (step 2 ) - Restore your data (step 4 )","title":"5. Moving data from one server to another"},{"location":"thehive/legacy/thehive3/admin/certauth/","text":"Single Sign-On on TheHive with X.509 Certificates # Abstract # SSL managed by TheHive is known to have some stability problem. It is advise to not enable it in production and configure SSL on a reverse proxy, in front of TheHive. This make X509 certificate authentication non applicable. In order to do x509 authentication it is recommended to do it in the reverse proxy and then forward user identity to TheHive in a HTTP header. This feature has been added in version 3.2. WARNING This setup is valid only if nobody except the reverse proxy can connect to TheHive. Users must have to use the reverse proxy. Otherwise, an user would be able to choose his identity on TheHive. Setup a reverse proxy # If you use nginx, the site configuration file should look like: server { listen 443 ssl; server_name thehive.example.com; ssl on; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; # Force client to have a certificate ssl_verify_client on; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; # Map certificate DN to user login stored in TheHive map $ssl_client_s_dn $thehive_user { default \"\"; /C=FR/O=TheHive-Project/CN=Thomas toom; /C=FR/O=TheHive-Project/CN=Georges bofh; }; # Redirect all request to local TheHive location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; # Send the mapped user login to TheHive, in THEHIVE_USER HTTP header proxy_set_header THEHIVE_USER $thehive_user; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; } } Enable authentication delegation in TheHive # Setup TheHive to identify user by the configured HTTP header (THEHIVE_USER): auth { method.header = true header.name = THEHIVE_USER } # Listen only on localhost to prevent direct access to TheHive http.address=127.0.0.1","title":"Single Sign-On on TheHive with X.509 Certificates"},{"location":"thehive/legacy/thehive3/admin/certauth/#single-sign-on-on-thehive-with-x509-certificates","text":"","title":"Single Sign-On on TheHive with X.509 Certificates"},{"location":"thehive/legacy/thehive3/admin/certauth/#abstract","text":"SSL managed by TheHive is known to have some stability problem. It is advise to not enable it in production and configure SSL on a reverse proxy, in front of TheHive. This make X509 certificate authentication non applicable. In order to do x509 authentication it is recommended to do it in the reverse proxy and then forward user identity to TheHive in a HTTP header. This feature has been added in version 3.2. WARNING This setup is valid only if nobody except the reverse proxy can connect to TheHive. Users must have to use the reverse proxy. Otherwise, an user would be able to choose his identity on TheHive.","title":"Abstract"},{"location":"thehive/legacy/thehive3/admin/certauth/#setup-a-reverse-proxy","text":"If you use nginx, the site configuration file should look like: server { listen 443 ssl; server_name thehive.example.com; ssl on; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; # Force client to have a certificate ssl_verify_client on; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; # Map certificate DN to user login stored in TheHive map $ssl_client_s_dn $thehive_user { default \"\"; /C=FR/O=TheHive-Project/CN=Thomas toom; /C=FR/O=TheHive-Project/CN=Georges bofh; }; # Redirect all request to local TheHive location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; # Send the mapped user login to TheHive, in THEHIVE_USER HTTP header proxy_set_header THEHIVE_USER $thehive_user; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; } }","title":"Setup a reverse proxy"},{"location":"thehive/legacy/thehive3/admin/certauth/#enable-authentication-delegation-in-thehive","text":"Setup TheHive to identify user by the configured HTTP header (THEHIVE_USER): auth { method.header = true header.name = THEHIVE_USER } # Listen only on localhost to prevent direct access to TheHive http.address=127.0.0.1","title":"Enable authentication delegation in TheHive"},{"location":"thehive/legacy/thehive3/admin/cluster/","text":"Cluster Configuration # Starting from version 3.1.0, TheHive can scale horizontally very easily. You can dynamically add nodes to your cluster to increase the performance of the platform. TheHive API is stateless to the exclusion of the stream (or real-time flow). For this reason, the cluster nodes need to communicate with each other. The first node of the cluster has a specific role: it must initiate the cluster creation. Any additional node only needs to contact at least one node of the cluster to join it. This is done by configuring so-called seed nodes . The first node must have itself in the seed node list. The other nodes must have at least one entry corresponding to a node that has already joined the seed node list. Note : all cluster nodes must share the same secret ( play.http.secret.key in application.conf ). Configuration # Define node1 (for example with IP address 10.0.0.1 ) as the first node of the cluster. The configuration section in application.conf should look like the following: akka { remote { netty.tcp { hostname = \"10.0.0.1\" port = 2552 } } # seed node is itself as it is the first node of the cluster cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } Then add another node. Let's call it node2 and assume its IP address is 10.0.0.2 to our one-node cluster. You can see that it is referring to the first node in cluster.seed-nodes : akka { remote { netty.tcp { hostname = \"10.0.0.2\" port = 2552 } } # seed node list contains at least one active node cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } We recommend defining several seed nodes in the respective configuration files, except for the first one. For example: node configured seed nodes node1 node1 node2 node1, node3 node3 node2, node4 node4 node1, node2, node3 Load Balancing # In front of TheHive cluster, you can add a load balancer which distributes HTTP requests to cluster nodes. One client does not need to always use the same node as affinity is not required. Below is an non-optimized example of a haproxy configuration: # Global standard configuration, nothing specific for TheHive global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon ca-base /etc/ssl/certs crt-base /etc/ssl/private ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 500 timeout client 50000 # server timeout must be at least the stream.refresh parameter in application.conf timeout server 2m errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # Listen on all interfaces, on port 9000/tcp frontend http-in bind *:9000 default_backend servers # Configure all cluster node backend servers balance roundrobin server node1 10.0.0.1:9000 check server node2 10.0.0.2:9000 check server node3 10.0.0.3:9000 check server node4 10.0.0.4:9000 check Troubleshooting # Should you encounter troubles with your setup, you can enable debug messages with the following configuration: akka { actor { debug { receive = on autoreceive = on lifecycle = on unhandled = on } } } Additional Information # TheHive Leverages Akka Cluster. You can refer to the Akka documentation for additional information.","title":"Cluster Configuration"},{"location":"thehive/legacy/thehive3/admin/cluster/#cluster-configuration","text":"Starting from version 3.1.0, TheHive can scale horizontally very easily. You can dynamically add nodes to your cluster to increase the performance of the platform. TheHive API is stateless to the exclusion of the stream (or real-time flow). For this reason, the cluster nodes need to communicate with each other. The first node of the cluster has a specific role: it must initiate the cluster creation. Any additional node only needs to contact at least one node of the cluster to join it. This is done by configuring so-called seed nodes . The first node must have itself in the seed node list. The other nodes must have at least one entry corresponding to a node that has already joined the seed node list. Note : all cluster nodes must share the same secret ( play.http.secret.key in application.conf ).","title":"Cluster Configuration"},{"location":"thehive/legacy/thehive3/admin/cluster/#configuration","text":"Define node1 (for example with IP address 10.0.0.1 ) as the first node of the cluster. The configuration section in application.conf should look like the following: akka { remote { netty.tcp { hostname = \"10.0.0.1\" port = 2552 } } # seed node is itself as it is the first node of the cluster cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } Then add another node. Let's call it node2 and assume its IP address is 10.0.0.2 to our one-node cluster. You can see that it is referring to the first node in cluster.seed-nodes : akka { remote { netty.tcp { hostname = \"10.0.0.2\" port = 2552 } } # seed node list contains at least one active node cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } We recommend defining several seed nodes in the respective configuration files, except for the first one. For example: node configured seed nodes node1 node1 node2 node1, node3 node3 node2, node4 node4 node1, node2, node3","title":"Configuration"},{"location":"thehive/legacy/thehive3/admin/cluster/#load-balancing","text":"In front of TheHive cluster, you can add a load balancer which distributes HTTP requests to cluster nodes. One client does not need to always use the same node as affinity is not required. Below is an non-optimized example of a haproxy configuration: # Global standard configuration, nothing specific for TheHive global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon ca-base /etc/ssl/certs crt-base /etc/ssl/private ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 500 timeout client 50000 # server timeout must be at least the stream.refresh parameter in application.conf timeout server 2m errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # Listen on all interfaces, on port 9000/tcp frontend http-in bind *:9000 default_backend servers # Configure all cluster node backend servers balance roundrobin server node1 10.0.0.1:9000 check server node2 10.0.0.2:9000 check server node3 10.0.0.3:9000 check server node4 10.0.0.4:9000 check","title":"Load Balancing"},{"location":"thehive/legacy/thehive3/admin/cluster/#troubleshooting","text":"Should you encounter troubles with your setup, you can enable debug messages with the following configuration: akka { actor { debug { receive = on autoreceive = on lifecycle = on unhandled = on } } }","title":"Troubleshooting"},{"location":"thehive/legacy/thehive3/admin/cluster/#additional-information","text":"TheHive Leverages Akka Cluster. You can refer to the Akka documentation for additional information.","title":"Additional Information"},{"location":"thehive/legacy/thehive3/admin/configuration/","text":"Configuration Guide # The configuration file of TheHive is /etc/thehive/application.conf by default. This file uses the HOCON format . All configuration parameters should go in this file. You can have a look at the default settings . Table of Contents # 1. Database 2. Datastore 3. Authentication 3.1 LDAP/AD 3.2 OAuth2/OpenID Connect 4. Streaming (a.k.a The Flow) 5. Entity size limit 6. Cortex 7. MISP 7.1 Configuration 7.2 Associate a Case Template to Alerts corresponding to MISP events 7.3 Event Filters 7.4 MISP Purpose 8. HTTP Client Configuration 9. Monitoring and Performance Metrics (deprecated) 10. HTTPS 10.1 HTTPS using a reverse proxy 10.2 HTTPS without reverse proxy 10.3 Strengthen security 1. Database # TheHive uses the Elasticsearch search engine to store all persistent data. Elasticsearch is not part of TheHive package. It must be installed and configured as a standalone instance which can be located on the same machine. For more information on how to set up Elasticsearch, please refer to Elasticsearch installation guide . Three settings are required to connect to Elasticsearch: * the base name of the index * the name of the cluster * the address(es) and port(s) of the Elasticsearch instance The Defaults settings are: # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200/\" # Scroll keepalive keepalive = 1m # Size of the page for scroll pagesize = 50 # Number of shards nbshards = 5 # Number of replicas nbreplicas = 1 # Arbitrary settings settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } ## Authentication configuration #search.username = \"\" #search.password = \"\" ## SSL configuration #search.keyStore { # path = \"/path/to/keystore\" # type = \"JKS\" # or PKCS12 # password = \"keystore-password\" #} #search.trustStore { # path = \"/path/to/trustStore\" # type = \"JKS\" # or PKCS12 # password = \"trustStore-password\" #} } If you use a different configuration, modify the parameters accordingly in the application.conf file. If multiple Elasticsearch nodes are used as a cluster, you should add addresses of the master nodes in the url like this: search { uri = http://node1:9200,node2:9200/ ... TheHive uses the http port of Elasticsearch (9200/tcp by default). TheHive versions index schema (mapping) in Elasticsearch. Version numbers are appended to the index base name (the 8th version of the schema uses the index the_hive_8 if search.index = the_hive ). When too many documents are requested to TheHive, it uses the scroll feature: the results are retrieved through pagination. You can specify the size of the page ( search.pagesize ) and how long pages are kept in Elasticsearch (( search.keepalive ) before purging. 2. Datastore # TheHive stores attachments as Elasticsearch documents. They are split in chunks and each chunk sent to Elasticsearch is identified by the hash of the entire attachment and the associated chunk number. The chunk size ( datastore.chunksize ) can be changed but any change will only affect new attachments. Existing ones won't be changed. An attachment is identified by its hash. The algorithm used is configurable ( datastore.hash.main ) but must not be changed after the first attachment insertion. Otherwise, previous files cannot be retrieved. Extra hash algorithms can be configured using datastore.hash.extra . These hashes are not used to identify the attachment but are shown in the user interface (the hash associated to the main algorithm is also shown). If you change extra algorithms, you should inform TheHive and ask it to recompute all hashes. Please note that the associated API call is currently disabled in Buckfast (v 2.10). It will be reinstated in the next release. Observables can contain malicious data. When you try to download an attachment from an observable (typically a file), it is automatically zipped and the resulting ZIP file is password-protected. The default password is malware but it can be changed with the datastore.attachment.password setting. Default values are: # Datastore datastore { name = data # Size of stored data chunks chunksize = 50k hash { # Main hash algorithm /!\\ Don't change this value main = \"SHA-256\" # Additional hash algorithms (used in attachments) extra = [\"SHA-1\", \"MD5\"] } attachment.password = \"malware\" } 3. Authentication # TheHive supports local, LDAP, Active Directory (AD) or OAuth2/OpenID Connect for authentication. By default, it relies on local credentials stored in Elasticsearch. Authentication methods are stored in the auth.provider parameter, which is multi-valued. When a user logs in, each authentication method is tried in order until one succeeds. If no authentication method works, an error is returned and the user cannot log in. The default values within the configuration file are: auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # services.LocalAuthSrv : passwords are stored in user entity (in Elasticsearch). No configuration is required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key # oauth2 : use OAuth/OIDC to authenticate users. Configuration is under \"auth.oauth2\" and \"auth.sso\" keys provider = [local] # By default, basic authentication is disabled. You can enable it by setting \"method.basic\" to true. #method.basic = true ad { # The Windows domain name in DNS format. This parameter is required if you do not use # 'serverNames' below. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers instead of using 'domainFQDN # above. If this parameter is not set, TheHive uses 'domainFQDN'. #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Windows domain name using short format. This parameter is required. #domainName = \"MYDOMAIN\" # If 'true', use SSL to connect to the domain controller. #useSSL = true } ldap { # The LDAP server name or address. The port can be specified using the 'host:port' # syntax. This parameter is required if you don't use 'serverNames' below. #serverName = \"ldap.mydomain.local:389\" # If you have multiple LDAP servers, use the multi-valued setting 'serverNames' instead. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Account to use to bind to the LDAP server. This parameter is required. #bindDN = \"cn=thehive,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user in the directory server. Please note that {0} is replaced # by the actual user name. This parameter is required. #filter = \"(cn={0})\" # If 'true', use SSL to connect to the LDAP directory server. #useSSL = true } oauth2 { # URL of the authorization server #clientId = \"client-id\" #clientSecret = \"client-secret\" #redirectUri = \"https://my-thehive-instance.example/api/ssoLogin\" #responseType = \"code\" #grantType = \"authorization_code\" # URL from where to get the access token #authorizationUrl = \"https://auth-site.com/OAuth/Authorize\" #tokenUrl = \"https://auth-site.com/OAuth/Token\" # The endpoint from which to obtain user details using the OAuth token, after successful login #userUrl = \"https://auth-site.com/api/User\" #scope = [\"openid profile\"] } # Single-Sign On sso { # Autocreate user in database? #autocreate = false # Autoupdate its profile and roles? #autoupdate = false # Autologin user using SSO? #autologin = false # Attributes mappings #attributes { # login = \"sub\" # name = \"name\" # groups = \"groups\" # #roles = \"roles\" #} # Name of mapping class from user resource to backend user ('simple' or 'group') #mapper = group # Default roles for users with no groups mapped (\"read\", \"write\", \"admin\") #defaultRoles = [] #groups { # # URL to retrieve groups (leave empty if you are using OIDC) # #url = \"https://auth-site.com/api/Groups\" # # Group mappings, you can have multiple roles for each group: they are merged # mappings { # admin-profile-name = [\"admin\"] # editor-profile-name = [\"write\"] # reader-profile-name = [\"read\"] # } #} } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h } 3.1. LDAP/AD # To enable authentication using AD or LDAP, edit the application.conf file and supply the values for your environment. Then you need to create an account on TheHive for each AD or LDAP user in Administration > Users page (which can only be accessed by an administrator). This is required as TheHive needs to look up the role associated with the user and that role is stored locally by TheHive. Obviously, you don't need to supply a password as TheHive will check the credentials against the remote directory. In order to use SSL on LDAP or AD, TheHive must be able to validate remote certificates. To that end, the Java truststore must contain certificate authorities used to generate the AD and/or LDAP certificates. The Default JVM truststore contains the main official authorities but LDAP and AD certificates are probably not issued by them. Use keytool to create the truststore: keytool -import -file /path/to/your/ca.cert -alias InternalCA -keystore /path/to/your/truststore.jks Then add -Djavax.net.ssl.trustStore=/path/to/your/truststore.jks parameter when you start TheHive or put it in the JAVA_OPTS environment variable before starting TheHive. 3.2. OAuth2/OpenID Connect # To enable authentication using OAuth2/OpenID Connect, edit the application.conf file and supply the values of auth.oauth2 according to your environment. In addition, you need to supply: auth.sso.attributes.login : name of the attribute containing the OAuth2 user's login in retreived user info (mandatory) auth.sso.attributes.name : name of the attribute containing the OAuth2 user's name in retreived user info (mandatory) auth.sso.attributes.groups : name of the attribute containing the OAuth2 user's groups (mandatory using groups mappings) auth.sso.attributes.roles : name of the attribute containing the OAuth2 user's roles in retreived user info (mandatory using simple mapping) Important notes # Authenticate the user using an external OAuth2 authenticator server. The configuration is: clientId (string) client ID in the OAuth2 server. clientSecret (string) client secret in the OAuth2 server. redirectUri (string) the url of TheHive AOuth2 page (.../api/ssoLogin). responseType (string) type of the response. Currently only \"code\" is accepted. grantType (string) type of the grant. Currently only \"authorization_code\" is accepted. authorizationUrl (string) the url of the OAuth2 server. authorizationHeader (string) prefix of the authorization header to get user info: Bearer, token, ... tokenUrl (string) the token url of the OAuth2 server. userUrl (string) the url to get user information in OAuth2 server. scope (list of string) list of scope. Example # auth { provider = [local, oauth2] [..] sso { autocreate: false autoupdate: false mapper: \"simple\" attributes { login: \"login\" name: \"name\" roles: \"role\" } defaultRoles: [\"read\", \"write\"] defaultOrganization: \"demo\" } oauth2 { name: oauth2 clientId: \"Client_ID\" clientSecret: \"Client_ID\" redirectUri: \"http://localhost:9000/api/ssoLogin\" responseType: code grantType: \"authorization_code\" authorizationUrl: \"https://github.com/login/oauth/authorize\" authorizationHeader: \"token\" tokenUrl: \"https://github.com/login/oauth/access_token\" userUrl: \"https://api.github.com/user\" scope: [\"user\"] } [..] } 3.2.1. Roles mappings # You can choose a roles mapping with the auth.sso.mapper parameter. The available options are simple and group : Using simple mapping, we assume that the user info retrieved from auth.oauth2.userUrl contains the roles associated to the OAuth2 user. They can be: read , write or admin . Using groups mappings, we assume the retrieved user info contains groups that have to be associated to internal roles. In that case, you have to define mapppings in auth.sso.groups.mappings . If a user has multiple groups, mapped roles are merged. If you need to retreive groups from another endpoint that the one used for user info, you can provide it in auth.sso.groups.url . The retrieved groups can be a valid JSON array or a string listing them: { \"sub\": \"userid1\", \"name\": \"User name 1\", \"groups\": [\"admin-profile-name\", \"reader-profile-name\"] } OR { \"sub\": \"userid2\", \"name\": \"User name 2\", \"groups\": \"[admin-profile-name, reader-profile-name, \\\"another profile\\\", 'a last group']\" } OR { \"sub\": \"userid3\", \"name\": \"User name 3\", \"groups\": \"the-only-group-of-the-user\" } Finally, you can setup default roles associated with user with no roles/groups retrieved, using the auth.sso.defaultRoles parameter. 3.2.2. User autocreation, autoupdate and autologin # The main advantage of OAuth2/OpenID Connect authentication is you won't need to create an account on TheHive for each OAuth2 user if you set the config parameter auth.sso.autocreate to true . However, by default, OAuth2 users won't be updated on SSO login unless you set auth.sso.autoupdate to true . If you set this last parameter, roles and name will be fetched from retrieved user info and will be updated in local database on each login of the user. With auth.sso.autologin set to true , each user connecting to TheHive will automatically be redirected to auth.oauth2.authorizationUrl . The only way to authenticate in TheHive using a local user will be either: Connecting to TheHive using https://my-hive-instance.com/index.html#!/login?code=BAD_CODE . You will get an Authentication Failure but will then be able to authenticate. Connecting to TheHive using a real OAuth2 account, then disconnect. On disconnection, you won't be redirected to authorization URL. 3.2.3. Debugging # To debug the OAuth2 feature, you can uncomment the following lines in /etc/thehive/logback.xml : <!-- Uncomment the next lines to log debug information for OAuth/OIDC login --> <logger name=\"org.elastic4play.services.auth\" level=\"DEBUG\" /> <logger name=\"services.OAuth2Srv\" level=\"DEBUG\" /> <logger name=\"services.mappers\" level=\"DEBUG\" /> 4. Streaming (a.k.a The Flow) # The user interface is automatically updated when data is changed in the back-end. To do this, the back-end sends events to all the connected front-ends. The mechanism used to notify the front-end is called long polling and its settings are: refresh : when there is no notification, close the connection after this duration (the default is 1 minute). cache : before polling a session must be created, in order to make sure no event is lost between two polls. If there is no poll during the cache setting, the session is destroyed (the default is 15 minutes). nextItemMaxWait , globalMaxWait : when an event occurs, it is not immediately sent to the front-ends. The back-end waits nextItemMaxWait and up to globalMaxWait in case another event can be included in the notification. This mechanism saves many HTTP requests. Default values are: # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s } 5. Entity size limit # The Play framework used by TheHive sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. This could be too small in most cases so you may want to change it with the following settings in the application.conf file: # Max textual content length play.http.parser.maxMemoryBuffer=1M # Max file size play.http.parser.maxDiskBuffer=1G Note : if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file. 6. Cortex # TheHive can use one or several Cortex analysis engines to get additional information on observables. When configured, analyzers available in Cortex become usable on TheHive. First you must enable CortexConnector , choose an identifier then specify the URL for each Cortex server: ## Enable the Cortex module play.modules.enabled += connectors.cortex.CortexConnector cortex { \"CORTEX-SERVER-ID\" { # URL of the Cortex server url = \"http://CORTEX_SERVER:CORTEX_PORT\" # Key of the Cortex user, mandatory for Cortex 2 key = \"API key\" } # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # Check job update time interval refreshDelay = 1 minute # Maximum number of successive errors before give up maxRetryOnError = 3 # Check remote Cortex status time interval statusCheckInterval = 1 minute } If you connect TheHive with Cortex 2, you must create a user in Cortex with the read, analyze roles, set an API key and add this key in the Cortex server definition in TheHive application.conf . For Cortex 1, authentication is not required, the key is not used. To create a user with the read, analyze role in Cortex 2, you must have at least one organization configured then you can connect to the Cortex 2 Web UI using a orgAdmin account for that organization to create the user and generate their API key. Please refer to the Cortex Quick Start Guide for more information. Cortex analyzes observables and outputs reports in JSON format. TheHive shows the report as-is by default. In order to make reports more readable, we provide report templates which are in a separate package and must be installed manually: - download the report template package from https://dl.bintray.com/thehive-project/binary/report-templates.zip - log in TheHive using an administrator account - go to Admin > Report templates menu - click on Import templates button and select the downloaded package HTTP client used by Cortex connector use global configuration (in play.ws ) but can be overridden in Cortex section and in each Cortex server configuration. Refer to section 8 for more detail on how to configure HTTP client. 7. MISP # TheHive has the ability to connect to one or several MISP instances in order to import and export events. Hence TheHive is able to: receive events as they are added or updated from multiple MISP instances. These events will appear within the Alerts pane. export cases as MISP events to one or several MISP instances. The exported cases will not be published automatically though as they need to be reviewed prior to publishing. We strongly advise you to review the categories and types of attributes at least, before publishing the corresponding MISP events. Note : Please note that only and all the observables marked as IOCs will be used to create the MISP event. Any other observable will not be shared. This is not configurable. Within the configuration file, you can register your MISP server(s) under the misp configuration keyword. Each server shall be identified using an arbitrary name, its url , the corresponding authentication key and optional tags to add each observable created from a MISP event. Any registered server will be used to import events as alerts. It can also be used to export cases to as MISP events, if the account used by TheHive on the MISP instance has sufficient rights. This means that TheHive can import events from configured MISP servers and export cases to the same configured MISP servers. Having different configuration for sources and destination servers is expected in a future version. Important Notes # TheHive requires MISP 2.4.73 or better . Make sure that your are using a compatible version of MISP before reporting problems. MISP 2.4.72 and below do not work correctly with TheHive. 7.1 Configuration # To sync with a MISP server and retrieve events or export cases, edit the application.conf file and adjust the example shown below to your setup: ## Enable the MISP module (import and export) play.modules.enabled += connectors.misp.MispConnector misp { \"MISP-SERVER-ID\" { # URL of the MISP instance. url = \"<The_URL_of_the_MISP_Server_goes_here>\" # Authentication key. key = \"<the_auth_key_goes_here>\" # Name of the case template created in TheHive that shall be used to import # MISP events as cases by default. caseTemplate = \"<Template_Name_goes_here>\" # Tags to add to each observable imported from an event available on # this instance. tags = [\"misp-server-id\"] # Truststore to use to validate the X.509 certificate of the MISP # instance if the default truststore is not sufficient. #ws.ssl.trustManager.stores = [ #{ # type: \"JKS\" # path: \"/path/to/truststore.jks\" #} #] # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } whitelist.tags = [\"whitelist-tag1\", \"whitelist-tag2\"] # MISP purpose defines if this instance can be used to import events (ImportOnly), export cases (ExportOnly) or both (ImportAndExport) # Default is ImportAndExport purpose = ImportAndExport } # Check remote TheHive status time interval statusCheckInterval = 1 minute # Interval between consecutive MISP event imports in hours (h) or # minutes (m). interval = 1h } The HTTP client used by the MISP connector uses a global configuration (in play.ws ) but it can be overridden within the MISP section of the configuation file and/or in the configuration section of each MISP server (in misp.MISP-SERVER-ID.ws ). Refer to section 8 for more details on how to configure the HTTP client. 7.2 Associate a Case Template to Alerts corresponding to MISP events # As stated in the subsection above, TheHive is able to automatically import MISP events (they will appear as alerts within the Alerts pane) and create cases out of them. This operation leverages the template engine. Thus you'll need to create a case template prior to importing MISP events. First, create a case template. Let's call it MISP-EVENT . Then update TheHive's configuration to add a 'caseTemplate' parameter as shown in the example below: misp { \"MISP-SERVER-ID\" { # URL of the MISP server url = \"<The_URL_of_the_MISP_Server_goes_here>\" # authentication key key = \"<the_auth_key_goes_here>\" # tags that must be automatically added to the case corresponding to the imported event tags = [\"misp\"] # case template caseTemplate = \"MISP-EVENT\" } Once the configuration file has been edited, restart TheHive. Every new import of a MISP event will generate a case using to the MISP-EVENT template by default. The template can be overridden though during the event import. MISP events will only be imported by TheHive if they have at least one attribute and were published. 7.3 Event Filters # When you first connect TheHive to a MISP instance, you can be overwhelmed by the number of alerts that will be generated, particularly if the MISP instance contains a lot of events. Indeed, every event, even those that date back to the beginning of the Internet, will generate an alert. To avoid alert fatigue, and starting from TheHive 3.0.4 (Cerana 0.4), you can exclude MISP events using different filters: the maximum number of attributes (max-attributes) the maximum size of the event's JSON message (max-size) the maximum age of the last publication (max-age) the organisation is black-listed (exclusion.organisation) one of the tags is black-listed (exclusion.tags) doesn't contain one of the whitelist tag (whitelist.tags) Please note that MISP event filters can be adapted to the configuration associated to each MISP server TheHive is connected with. As regards the max-age filter, it applies to the publication date of MISP events and not to the creation date. In the example below, the following MISP events won't generate alerts in TheHive: events that have more than 1000 attributes events which JSON message size is greater than 1MB events that have been published more than one week from the current date events that have been created by bad organisation or other orga events that contain tag1 or tag2 # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } Of course, you can omit some of the filters or all of them. 7.4 MISP Purpose # TheHive can interact with MISP in two ways: import a MISP event to create a case in TheHive and export a TheHive case to create a MISP event. By default, any MISP instance that is added to TheHive's configuration will be used for importing events and exporting cases ( ImportAndExport ). If you want to use MISP in only one way, you can set its purpose in the configuration as ImportOnly or ExportOnly . Starting from TheHive 3.3, when exporting a case to a MISP instance, you can export all its tags to the freshly created MISP event. This behaviour is not enabled by default. If you want to enable it you must set the exportCaseTags variable to true as shown below: misp { \"local\" { url = \"http://127.0.0.1\" key = \"<the_auth_key_goes_here>\" exportCaseTags = true [...] # additional parameters go here } } 8. HTTP Client Configuration # HTTP client can be configured by adding ws key in sections that needs to connect to remote HTTP service. The key can contain configuration items defined in play WS configuration : ws.followRedirects : Configures the client to follow 301 and 302 redirects (default is true). ws.useragent : To configure the User-Agent header field. ws.compressionEnabled : Set it to true to use gzip/deflater encoding (default is false). Timeouts # There are 3 different timeouts in WS. Reaching a timeout causes the WS request to interrupt. - ws.timeout.connection : The maximum time to wait when connecting to the remote host (default is 120 seconds). - ws.timeout.idle : The maximum time the request can stay idle (connection is established but waiting for more data) (default is 120 seconds). - ws.timeout.request : The total time you accept a request to take (it will be interrupted even if the remote host is still sending data) (default is 120 seconds). Proxy # Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. - ws.useProxyProperties : To use the JVM system\u2019s HTTP proxy settings (http.proxyHost, http.proxyPort) (default is true). This setting is ignored if ws.proxy settings is present. - ws.proxy.host : The hostname of the proxy server. - ws.proxy.post : The port of the proxy server. - ws.proxy.protocol : The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. - ws.proxy.user : The username of the credentials for the proxy server. - ws.proxy.password : The password for the credentials for the proxy server. - ws.proxy.ntlmDomain : The password for the credentials for the proxy server. - ws.proxy.encoding : The realm's charset. - ws.proxy.nonProxyHosts : The list of hosts on which proxy must not be used. SSL # SSL of HTTP client can be completely configured in application.conf file. Certificate manager # Certificate manager is used to store client certificates and certificate authorities. keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) ws.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] } Certificate authorities are configured using trustManager key. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. ws.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } Debugging # To debug the key manager / trust manager, set the following flags: ws.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true } Protocols # If you want to define a different default protocol, you can set it specifically in the client: ws.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] Ciphers # Cipher suites can be configured using ws.ssl.enabledCipherSuites : ws.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ] 9. Monitoring and Performance Metrics (deprecated) # Performance metrics (response time, call rate to Elasticsearch and HTTP request, throughput, memory used...) can be collected if enabled in configuration. Enable it by editing the application.conf file, and add: # Register module for dependency injection play.modules.enabled += connectors.metrics.MetricsModule metrics.enabled = true These metrics can optionally be sent to an external database (graphite, ganglia or influxdb) in order to monitor the health of the platform. This feature is disabled by default. metrics { name = default enabled = true rateUnit = SECONDS durationUnit = SECONDS showSamples = false jvm = true logback = true graphite { enabled = false host = \"127.0.0.1\" port = 2003 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS period = 10s } ganglia { enabled = false host = \"127.0.0.1\" port = 8649 mode = UNICAST ttl = 1 version = 3.1 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS tmax = 60 dmax = 0 period = 10s } influx { enabled = false url = \"http://127.0.0.1:8086\" user = root password = root database = thehive retention = default consistency = ALL #tags = { # tag1 = value1 # tag2 = value2 #} period = 10s } } 10. HTTPS # You can enable HTTPS on TheHive application or add a reverse proxy in front of TheHive. The latter solution is recommended. 10.1 HTTPS using a reverse proxy # You can choose any reverse proxy to add SSL on TheHive. Below an example of NGINX configuration: server { listen 443 ssl; server_name thehive.example.com; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; proxy_set_header Connection \"\"; # cf. https://github.com/akka/akka/issues/19542 } } 10.2 HTTPS without reverse proxy # This is not supported. 10.3 Strengthen security # When SSL is enable (with reverse proxy or not), you can configure cookie to be \"secure\" (usable only with HTTPS protocol). This is done by adding play.http.session.secure=true in the application.conf file. You can also enable HSTS . This header must be configured on the SSL termination component. For NGINX, use add_header directive, as show above.","title":"Configuration Guide"},{"location":"thehive/legacy/thehive3/admin/configuration/#configuration-guide","text":"The configuration file of TheHive is /etc/thehive/application.conf by default. This file uses the HOCON format . All configuration parameters should go in this file. You can have a look at the default settings .","title":"Configuration Guide"},{"location":"thehive/legacy/thehive3/admin/configuration/#table-of-contents","text":"1. Database 2. Datastore 3. Authentication 3.1 LDAP/AD 3.2 OAuth2/OpenID Connect 4. Streaming (a.k.a The Flow) 5. Entity size limit 6. Cortex 7. MISP 7.1 Configuration 7.2 Associate a Case Template to Alerts corresponding to MISP events 7.3 Event Filters 7.4 MISP Purpose 8. HTTP Client Configuration 9. Monitoring and Performance Metrics (deprecated) 10. HTTPS 10.1 HTTPS using a reverse proxy 10.2 HTTPS without reverse proxy 10.3 Strengthen security","title":"Table of Contents"},{"location":"thehive/legacy/thehive3/admin/configuration/#1-database","text":"TheHive uses the Elasticsearch search engine to store all persistent data. Elasticsearch is not part of TheHive package. It must be installed and configured as a standalone instance which can be located on the same machine. For more information on how to set up Elasticsearch, please refer to Elasticsearch installation guide . Three settings are required to connect to Elasticsearch: * the base name of the index * the name of the cluster * the address(es) and port(s) of the Elasticsearch instance The Defaults settings are: # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200/\" # Scroll keepalive keepalive = 1m # Size of the page for scroll pagesize = 50 # Number of shards nbshards = 5 # Number of replicas nbreplicas = 1 # Arbitrary settings settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } ## Authentication configuration #search.username = \"\" #search.password = \"\" ## SSL configuration #search.keyStore { # path = \"/path/to/keystore\" # type = \"JKS\" # or PKCS12 # password = \"keystore-password\" #} #search.trustStore { # path = \"/path/to/trustStore\" # type = \"JKS\" # or PKCS12 # password = \"trustStore-password\" #} } If you use a different configuration, modify the parameters accordingly in the application.conf file. If multiple Elasticsearch nodes are used as a cluster, you should add addresses of the master nodes in the url like this: search { uri = http://node1:9200,node2:9200/ ... TheHive uses the http port of Elasticsearch (9200/tcp by default). TheHive versions index schema (mapping) in Elasticsearch. Version numbers are appended to the index base name (the 8th version of the schema uses the index the_hive_8 if search.index = the_hive ). When too many documents are requested to TheHive, it uses the scroll feature: the results are retrieved through pagination. You can specify the size of the page ( search.pagesize ) and how long pages are kept in Elasticsearch (( search.keepalive ) before purging.","title":"1. Database"},{"location":"thehive/legacy/thehive3/admin/configuration/#2-datastore","text":"TheHive stores attachments as Elasticsearch documents. They are split in chunks and each chunk sent to Elasticsearch is identified by the hash of the entire attachment and the associated chunk number. The chunk size ( datastore.chunksize ) can be changed but any change will only affect new attachments. Existing ones won't be changed. An attachment is identified by its hash. The algorithm used is configurable ( datastore.hash.main ) but must not be changed after the first attachment insertion. Otherwise, previous files cannot be retrieved. Extra hash algorithms can be configured using datastore.hash.extra . These hashes are not used to identify the attachment but are shown in the user interface (the hash associated to the main algorithm is also shown). If you change extra algorithms, you should inform TheHive and ask it to recompute all hashes. Please note that the associated API call is currently disabled in Buckfast (v 2.10). It will be reinstated in the next release. Observables can contain malicious data. When you try to download an attachment from an observable (typically a file), it is automatically zipped and the resulting ZIP file is password-protected. The default password is malware but it can be changed with the datastore.attachment.password setting. Default values are: # Datastore datastore { name = data # Size of stored data chunks chunksize = 50k hash { # Main hash algorithm /!\\ Don't change this value main = \"SHA-256\" # Additional hash algorithms (used in attachments) extra = [\"SHA-1\", \"MD5\"] } attachment.password = \"malware\" }","title":"2. Datastore"},{"location":"thehive/legacy/thehive3/admin/configuration/#3-authentication","text":"TheHive supports local, LDAP, Active Directory (AD) or OAuth2/OpenID Connect for authentication. By default, it relies on local credentials stored in Elasticsearch. Authentication methods are stored in the auth.provider parameter, which is multi-valued. When a user logs in, each authentication method is tried in order until one succeeds. If no authentication method works, an error is returned and the user cannot log in. The default values within the configuration file are: auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # services.LocalAuthSrv : passwords are stored in user entity (in Elasticsearch). No configuration is required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key # oauth2 : use OAuth/OIDC to authenticate users. Configuration is under \"auth.oauth2\" and \"auth.sso\" keys provider = [local] # By default, basic authentication is disabled. You can enable it by setting \"method.basic\" to true. #method.basic = true ad { # The Windows domain name in DNS format. This parameter is required if you do not use # 'serverNames' below. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers instead of using 'domainFQDN # above. If this parameter is not set, TheHive uses 'domainFQDN'. #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Windows domain name using short format. This parameter is required. #domainName = \"MYDOMAIN\" # If 'true', use SSL to connect to the domain controller. #useSSL = true } ldap { # The LDAP server name or address. The port can be specified using the 'host:port' # syntax. This parameter is required if you don't use 'serverNames' below. #serverName = \"ldap.mydomain.local:389\" # If you have multiple LDAP servers, use the multi-valued setting 'serverNames' instead. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Account to use to bind to the LDAP server. This parameter is required. #bindDN = \"cn=thehive,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user in the directory server. Please note that {0} is replaced # by the actual user name. This parameter is required. #filter = \"(cn={0})\" # If 'true', use SSL to connect to the LDAP directory server. #useSSL = true } oauth2 { # URL of the authorization server #clientId = \"client-id\" #clientSecret = \"client-secret\" #redirectUri = \"https://my-thehive-instance.example/api/ssoLogin\" #responseType = \"code\" #grantType = \"authorization_code\" # URL from where to get the access token #authorizationUrl = \"https://auth-site.com/OAuth/Authorize\" #tokenUrl = \"https://auth-site.com/OAuth/Token\" # The endpoint from which to obtain user details using the OAuth token, after successful login #userUrl = \"https://auth-site.com/api/User\" #scope = [\"openid profile\"] } # Single-Sign On sso { # Autocreate user in database? #autocreate = false # Autoupdate its profile and roles? #autoupdate = false # Autologin user using SSO? #autologin = false # Attributes mappings #attributes { # login = \"sub\" # name = \"name\" # groups = \"groups\" # #roles = \"roles\" #} # Name of mapping class from user resource to backend user ('simple' or 'group') #mapper = group # Default roles for users with no groups mapped (\"read\", \"write\", \"admin\") #defaultRoles = [] #groups { # # URL to retrieve groups (leave empty if you are using OIDC) # #url = \"https://auth-site.com/api/Groups\" # # Group mappings, you can have multiple roles for each group: they are merged # mappings { # admin-profile-name = [\"admin\"] # editor-profile-name = [\"write\"] # reader-profile-name = [\"read\"] # } #} } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h }","title":"3. Authentication"},{"location":"thehive/legacy/thehive3/admin/configuration/#31-ldapad","text":"To enable authentication using AD or LDAP, edit the application.conf file and supply the values for your environment. Then you need to create an account on TheHive for each AD or LDAP user in Administration > Users page (which can only be accessed by an administrator). This is required as TheHive needs to look up the role associated with the user and that role is stored locally by TheHive. Obviously, you don't need to supply a password as TheHive will check the credentials against the remote directory. In order to use SSL on LDAP or AD, TheHive must be able to validate remote certificates. To that end, the Java truststore must contain certificate authorities used to generate the AD and/or LDAP certificates. The Default JVM truststore contains the main official authorities but LDAP and AD certificates are probably not issued by them. Use keytool to create the truststore: keytool -import -file /path/to/your/ca.cert -alias InternalCA -keystore /path/to/your/truststore.jks Then add -Djavax.net.ssl.trustStore=/path/to/your/truststore.jks parameter when you start TheHive or put it in the JAVA_OPTS environment variable before starting TheHive.","title":"3.1. LDAP/AD"},{"location":"thehive/legacy/thehive3/admin/configuration/#32-oauth2openid-connect","text":"To enable authentication using OAuth2/OpenID Connect, edit the application.conf file and supply the values of auth.oauth2 according to your environment. In addition, you need to supply: auth.sso.attributes.login : name of the attribute containing the OAuth2 user's login in retreived user info (mandatory) auth.sso.attributes.name : name of the attribute containing the OAuth2 user's name in retreived user info (mandatory) auth.sso.attributes.groups : name of the attribute containing the OAuth2 user's groups (mandatory using groups mappings) auth.sso.attributes.roles : name of the attribute containing the OAuth2 user's roles in retreived user info (mandatory using simple mapping)","title":"3.2. OAuth2/OpenID Connect"},{"location":"thehive/legacy/thehive3/admin/configuration/#important-notes","text":"Authenticate the user using an external OAuth2 authenticator server. The configuration is: clientId (string) client ID in the OAuth2 server. clientSecret (string) client secret in the OAuth2 server. redirectUri (string) the url of TheHive AOuth2 page (.../api/ssoLogin). responseType (string) type of the response. Currently only \"code\" is accepted. grantType (string) type of the grant. Currently only \"authorization_code\" is accepted. authorizationUrl (string) the url of the OAuth2 server. authorizationHeader (string) prefix of the authorization header to get user info: Bearer, token, ... tokenUrl (string) the token url of the OAuth2 server. userUrl (string) the url to get user information in OAuth2 server. scope (list of string) list of scope.","title":"Important notes"},{"location":"thehive/legacy/thehive3/admin/configuration/#example","text":"auth { provider = [local, oauth2] [..] sso { autocreate: false autoupdate: false mapper: \"simple\" attributes { login: \"login\" name: \"name\" roles: \"role\" } defaultRoles: [\"read\", \"write\"] defaultOrganization: \"demo\" } oauth2 { name: oauth2 clientId: \"Client_ID\" clientSecret: \"Client_ID\" redirectUri: \"http://localhost:9000/api/ssoLogin\" responseType: code grantType: \"authorization_code\" authorizationUrl: \"https://github.com/login/oauth/authorize\" authorizationHeader: \"token\" tokenUrl: \"https://github.com/login/oauth/access_token\" userUrl: \"https://api.github.com/user\" scope: [\"user\"] } [..] }","title":"Example"},{"location":"thehive/legacy/thehive3/admin/configuration/#321-roles-mappings","text":"You can choose a roles mapping with the auth.sso.mapper parameter. The available options are simple and group : Using simple mapping, we assume that the user info retrieved from auth.oauth2.userUrl contains the roles associated to the OAuth2 user. They can be: read , write or admin . Using groups mappings, we assume the retrieved user info contains groups that have to be associated to internal roles. In that case, you have to define mapppings in auth.sso.groups.mappings . If a user has multiple groups, mapped roles are merged. If you need to retreive groups from another endpoint that the one used for user info, you can provide it in auth.sso.groups.url . The retrieved groups can be a valid JSON array or a string listing them: { \"sub\": \"userid1\", \"name\": \"User name 1\", \"groups\": [\"admin-profile-name\", \"reader-profile-name\"] } OR { \"sub\": \"userid2\", \"name\": \"User name 2\", \"groups\": \"[admin-profile-name, reader-profile-name, \\\"another profile\\\", 'a last group']\" } OR { \"sub\": \"userid3\", \"name\": \"User name 3\", \"groups\": \"the-only-group-of-the-user\" } Finally, you can setup default roles associated with user with no roles/groups retrieved, using the auth.sso.defaultRoles parameter.","title":"3.2.1. Roles mappings"},{"location":"thehive/legacy/thehive3/admin/configuration/#322-user-autocreation-autoupdate-and-autologin","text":"The main advantage of OAuth2/OpenID Connect authentication is you won't need to create an account on TheHive for each OAuth2 user if you set the config parameter auth.sso.autocreate to true . However, by default, OAuth2 users won't be updated on SSO login unless you set auth.sso.autoupdate to true . If you set this last parameter, roles and name will be fetched from retrieved user info and will be updated in local database on each login of the user. With auth.sso.autologin set to true , each user connecting to TheHive will automatically be redirected to auth.oauth2.authorizationUrl . The only way to authenticate in TheHive using a local user will be either: Connecting to TheHive using https://my-hive-instance.com/index.html#!/login?code=BAD_CODE . You will get an Authentication Failure but will then be able to authenticate. Connecting to TheHive using a real OAuth2 account, then disconnect. On disconnection, you won't be redirected to authorization URL.","title":"3.2.2. User autocreation, autoupdate and autologin"},{"location":"thehive/legacy/thehive3/admin/configuration/#323-debugging","text":"To debug the OAuth2 feature, you can uncomment the following lines in /etc/thehive/logback.xml : <!-- Uncomment the next lines to log debug information for OAuth/OIDC login --> <logger name=\"org.elastic4play.services.auth\" level=\"DEBUG\" /> <logger name=\"services.OAuth2Srv\" level=\"DEBUG\" /> <logger name=\"services.mappers\" level=\"DEBUG\" />","title":"3.2.3. Debugging"},{"location":"thehive/legacy/thehive3/admin/configuration/#4-streaming-aka-the-flow","text":"The user interface is automatically updated when data is changed in the back-end. To do this, the back-end sends events to all the connected front-ends. The mechanism used to notify the front-end is called long polling and its settings are: refresh : when there is no notification, close the connection after this duration (the default is 1 minute). cache : before polling a session must be created, in order to make sure no event is lost between two polls. If there is no poll during the cache setting, the session is destroyed (the default is 15 minutes). nextItemMaxWait , globalMaxWait : when an event occurs, it is not immediately sent to the front-ends. The back-end waits nextItemMaxWait and up to globalMaxWait in case another event can be included in the notification. This mechanism saves many HTTP requests. Default values are: # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s }","title":"4. Streaming (a.k.a The Flow)"},{"location":"thehive/legacy/thehive3/admin/configuration/#5-entity-size-limit","text":"The Play framework used by TheHive sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. This could be too small in most cases so you may want to change it with the following settings in the application.conf file: # Max textual content length play.http.parser.maxMemoryBuffer=1M # Max file size play.http.parser.maxDiskBuffer=1G Note : if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"5. Entity size limit"},{"location":"thehive/legacy/thehive3/admin/configuration/#6-cortex","text":"TheHive can use one or several Cortex analysis engines to get additional information on observables. When configured, analyzers available in Cortex become usable on TheHive. First you must enable CortexConnector , choose an identifier then specify the URL for each Cortex server: ## Enable the Cortex module play.modules.enabled += connectors.cortex.CortexConnector cortex { \"CORTEX-SERVER-ID\" { # URL of the Cortex server url = \"http://CORTEX_SERVER:CORTEX_PORT\" # Key of the Cortex user, mandatory for Cortex 2 key = \"API key\" } # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # Check job update time interval refreshDelay = 1 minute # Maximum number of successive errors before give up maxRetryOnError = 3 # Check remote Cortex status time interval statusCheckInterval = 1 minute } If you connect TheHive with Cortex 2, you must create a user in Cortex with the read, analyze roles, set an API key and add this key in the Cortex server definition in TheHive application.conf . For Cortex 1, authentication is not required, the key is not used. To create a user with the read, analyze role in Cortex 2, you must have at least one organization configured then you can connect to the Cortex 2 Web UI using a orgAdmin account for that organization to create the user and generate their API key. Please refer to the Cortex Quick Start Guide for more information. Cortex analyzes observables and outputs reports in JSON format. TheHive shows the report as-is by default. In order to make reports more readable, we provide report templates which are in a separate package and must be installed manually: - download the report template package from https://dl.bintray.com/thehive-project/binary/report-templates.zip - log in TheHive using an administrator account - go to Admin > Report templates menu - click on Import templates button and select the downloaded package HTTP client used by Cortex connector use global configuration (in play.ws ) but can be overridden in Cortex section and in each Cortex server configuration. Refer to section 8 for more detail on how to configure HTTP client.","title":"6. Cortex"},{"location":"thehive/legacy/thehive3/admin/configuration/#7-misp","text":"TheHive has the ability to connect to one or several MISP instances in order to import and export events. Hence TheHive is able to: receive events as they are added or updated from multiple MISP instances. These events will appear within the Alerts pane. export cases as MISP events to one or several MISP instances. The exported cases will not be published automatically though as they need to be reviewed prior to publishing. We strongly advise you to review the categories and types of attributes at least, before publishing the corresponding MISP events. Note : Please note that only and all the observables marked as IOCs will be used to create the MISP event. Any other observable will not be shared. This is not configurable. Within the configuration file, you can register your MISP server(s) under the misp configuration keyword. Each server shall be identified using an arbitrary name, its url , the corresponding authentication key and optional tags to add each observable created from a MISP event. Any registered server will be used to import events as alerts. It can also be used to export cases to as MISP events, if the account used by TheHive on the MISP instance has sufficient rights. This means that TheHive can import events from configured MISP servers and export cases to the same configured MISP servers. Having different configuration for sources and destination servers is expected in a future version.","title":"7. MISP"},{"location":"thehive/legacy/thehive3/admin/configuration/#important-notes_1","text":"TheHive requires MISP 2.4.73 or better . Make sure that your are using a compatible version of MISP before reporting problems. MISP 2.4.72 and below do not work correctly with TheHive.","title":"Important Notes"},{"location":"thehive/legacy/thehive3/admin/configuration/#71-configuration","text":"To sync with a MISP server and retrieve events or export cases, edit the application.conf file and adjust the example shown below to your setup: ## Enable the MISP module (import and export) play.modules.enabled += connectors.misp.MispConnector misp { \"MISP-SERVER-ID\" { # URL of the MISP instance. url = \"<The_URL_of_the_MISP_Server_goes_here>\" # Authentication key. key = \"<the_auth_key_goes_here>\" # Name of the case template created in TheHive that shall be used to import # MISP events as cases by default. caseTemplate = \"<Template_Name_goes_here>\" # Tags to add to each observable imported from an event available on # this instance. tags = [\"misp-server-id\"] # Truststore to use to validate the X.509 certificate of the MISP # instance if the default truststore is not sufficient. #ws.ssl.trustManager.stores = [ #{ # type: \"JKS\" # path: \"/path/to/truststore.jks\" #} #] # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } whitelist.tags = [\"whitelist-tag1\", \"whitelist-tag2\"] # MISP purpose defines if this instance can be used to import events (ImportOnly), export cases (ExportOnly) or both (ImportAndExport) # Default is ImportAndExport purpose = ImportAndExport } # Check remote TheHive status time interval statusCheckInterval = 1 minute # Interval between consecutive MISP event imports in hours (h) or # minutes (m). interval = 1h } The HTTP client used by the MISP connector uses a global configuration (in play.ws ) but it can be overridden within the MISP section of the configuation file and/or in the configuration section of each MISP server (in misp.MISP-SERVER-ID.ws ). Refer to section 8 for more details on how to configure the HTTP client.","title":"7.1 Configuration"},{"location":"thehive/legacy/thehive3/admin/configuration/#72-associate-a-case-template-to-alerts-corresponding-to-misp-events","text":"As stated in the subsection above, TheHive is able to automatically import MISP events (they will appear as alerts within the Alerts pane) and create cases out of them. This operation leverages the template engine. Thus you'll need to create a case template prior to importing MISP events. First, create a case template. Let's call it MISP-EVENT . Then update TheHive's configuration to add a 'caseTemplate' parameter as shown in the example below: misp { \"MISP-SERVER-ID\" { # URL of the MISP server url = \"<The_URL_of_the_MISP_Server_goes_here>\" # authentication key key = \"<the_auth_key_goes_here>\" # tags that must be automatically added to the case corresponding to the imported event tags = [\"misp\"] # case template caseTemplate = \"MISP-EVENT\" } Once the configuration file has been edited, restart TheHive. Every new import of a MISP event will generate a case using to the MISP-EVENT template by default. The template can be overridden though during the event import. MISP events will only be imported by TheHive if they have at least one attribute and were published.","title":"7.2 Associate a Case Template to Alerts corresponding to MISP events"},{"location":"thehive/legacy/thehive3/admin/configuration/#73-event-filters","text":"When you first connect TheHive to a MISP instance, you can be overwhelmed by the number of alerts that will be generated, particularly if the MISP instance contains a lot of events. Indeed, every event, even those that date back to the beginning of the Internet, will generate an alert. To avoid alert fatigue, and starting from TheHive 3.0.4 (Cerana 0.4), you can exclude MISP events using different filters: the maximum number of attributes (max-attributes) the maximum size of the event's JSON message (max-size) the maximum age of the last publication (max-age) the organisation is black-listed (exclusion.organisation) one of the tags is black-listed (exclusion.tags) doesn't contain one of the whitelist tag (whitelist.tags) Please note that MISP event filters can be adapted to the configuration associated to each MISP server TheHive is connected with. As regards the max-age filter, it applies to the publication date of MISP events and not to the creation date. In the example below, the following MISP events won't generate alerts in TheHive: events that have more than 1000 attributes events which JSON message size is greater than 1MB events that have been published more than one week from the current date events that have been created by bad organisation or other orga events that contain tag1 or tag2 # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } Of course, you can omit some of the filters or all of them.","title":"7.3 Event Filters"},{"location":"thehive/legacy/thehive3/admin/configuration/#74-misp-purpose","text":"TheHive can interact with MISP in two ways: import a MISP event to create a case in TheHive and export a TheHive case to create a MISP event. By default, any MISP instance that is added to TheHive's configuration will be used for importing events and exporting cases ( ImportAndExport ). If you want to use MISP in only one way, you can set its purpose in the configuration as ImportOnly or ExportOnly . Starting from TheHive 3.3, when exporting a case to a MISP instance, you can export all its tags to the freshly created MISP event. This behaviour is not enabled by default. If you want to enable it you must set the exportCaseTags variable to true as shown below: misp { \"local\" { url = \"http://127.0.0.1\" key = \"<the_auth_key_goes_here>\" exportCaseTags = true [...] # additional parameters go here } }","title":"7.4 MISP Purpose"},{"location":"thehive/legacy/thehive3/admin/configuration/#8-http-client-configuration","text":"HTTP client can be configured by adding ws key in sections that needs to connect to remote HTTP service. The key can contain configuration items defined in play WS configuration : ws.followRedirects : Configures the client to follow 301 and 302 redirects (default is true). ws.useragent : To configure the User-Agent header field. ws.compressionEnabled : Set it to true to use gzip/deflater encoding (default is false).","title":"8. HTTP Client Configuration"},{"location":"thehive/legacy/thehive3/admin/configuration/#timeouts","text":"There are 3 different timeouts in WS. Reaching a timeout causes the WS request to interrupt. - ws.timeout.connection : The maximum time to wait when connecting to the remote host (default is 120 seconds). - ws.timeout.idle : The maximum time the request can stay idle (connection is established but waiting for more data) (default is 120 seconds). - ws.timeout.request : The total time you accept a request to take (it will be interrupted even if the remote host is still sending data) (default is 120 seconds).","title":"Timeouts"},{"location":"thehive/legacy/thehive3/admin/configuration/#proxy","text":"Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. - ws.useProxyProperties : To use the JVM system\u2019s HTTP proxy settings (http.proxyHost, http.proxyPort) (default is true). This setting is ignored if ws.proxy settings is present. - ws.proxy.host : The hostname of the proxy server. - ws.proxy.post : The port of the proxy server. - ws.proxy.protocol : The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. - ws.proxy.user : The username of the credentials for the proxy server. - ws.proxy.password : The password for the credentials for the proxy server. - ws.proxy.ntlmDomain : The password for the credentials for the proxy server. - ws.proxy.encoding : The realm's charset. - ws.proxy.nonProxyHosts : The list of hosts on which proxy must not be used.","title":"Proxy"},{"location":"thehive/legacy/thehive3/admin/configuration/#ssl","text":"SSL of HTTP client can be completely configured in application.conf file.","title":"SSL"},{"location":"thehive/legacy/thehive3/admin/configuration/#certificate-manager","text":"Certificate manager is used to store client certificates and certificate authorities. keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) ws.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] } Certificate authorities are configured using trustManager key. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. ws.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] }","title":"Certificate manager"},{"location":"thehive/legacy/thehive3/admin/configuration/#debugging","text":"To debug the key manager / trust manager, set the following flags: ws.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true }","title":"Debugging"},{"location":"thehive/legacy/thehive3/admin/configuration/#protocols","text":"If you want to define a different default protocol, you can set it specifically in the client: ws.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"]","title":"Protocols"},{"location":"thehive/legacy/thehive3/admin/configuration/#ciphers","text":"Cipher suites can be configured using ws.ssl.enabledCipherSuites : ws.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ]","title":"Ciphers"},{"location":"thehive/legacy/thehive3/admin/configuration/#9-monitoring-and-performance-metrics-deprecated","text":"Performance metrics (response time, call rate to Elasticsearch and HTTP request, throughput, memory used...) can be collected if enabled in configuration. Enable it by editing the application.conf file, and add: # Register module for dependency injection play.modules.enabled += connectors.metrics.MetricsModule metrics.enabled = true These metrics can optionally be sent to an external database (graphite, ganglia or influxdb) in order to monitor the health of the platform. This feature is disabled by default. metrics { name = default enabled = true rateUnit = SECONDS durationUnit = SECONDS showSamples = false jvm = true logback = true graphite { enabled = false host = \"127.0.0.1\" port = 2003 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS period = 10s } ganglia { enabled = false host = \"127.0.0.1\" port = 8649 mode = UNICAST ttl = 1 version = 3.1 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS tmax = 60 dmax = 0 period = 10s } influx { enabled = false url = \"http://127.0.0.1:8086\" user = root password = root database = thehive retention = default consistency = ALL #tags = { # tag1 = value1 # tag2 = value2 #} period = 10s } }","title":"9. Monitoring and Performance Metrics (deprecated)"},{"location":"thehive/legacy/thehive3/admin/configuration/#10-https","text":"You can enable HTTPS on TheHive application or add a reverse proxy in front of TheHive. The latter solution is recommended.","title":"10. HTTPS"},{"location":"thehive/legacy/thehive3/admin/configuration/#101-https-using-a-reverse-proxy","text":"You can choose any reverse proxy to add SSL on TheHive. Below an example of NGINX configuration: server { listen 443 ssl; server_name thehive.example.com; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; proxy_set_header Connection \"\"; # cf. https://github.com/akka/akka/issues/19542 } }","title":"10.1 HTTPS using a reverse proxy"},{"location":"thehive/legacy/thehive3/admin/configuration/#102-https-without-reverse-proxy","text":"This is not supported.","title":"10.2 HTTPS without reverse proxy"},{"location":"thehive/legacy/thehive3/admin/configuration/#103-strengthen-security","text":"When SSL is enable (with reverse proxy or not), you can configure cookie to be \"secure\" (usable only with HTTPS protocol). This is done by adding play.http.session.secure=true in the application.conf file. You can also enable HSTS . This header must be configured on the SSL termination component. For NGINX, use add_header directive, as show above.","title":"10.3 Strengthen security"},{"location":"thehive/legacy/thehive3/admin/default-configuration/","text":"You can find the default configuration settings of TheHive below: # maximum number of similar cases maxSimilarCases = 100 # ElasticSearch search { # Name of the index index = the_hive # Name of the ElasticSearch cluster cluster = hive # Address of the ElasticSearch instance host = [\"127.0.0.1:9300\"] # Scroll keepalive keepalive = 1m # Size of the page for scroll pagesize = 50 # Arbitrary settings settings { # Maximum number of nested fields mapping.nested_fields.limit = 50 } } # Datastore datastore { # Size of stored data chunks chunksize = 50k hash { # Main hash algorithm /!\\ Don't change this value main = \"SHA-256\" # Additional hash algorithms (used in attachments) extra = [\"SHA-1\", \"MD5\"] } attachment.password = \"malware\" } auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # local : passwords are stored in user entity (in ElasticSearch). No configuration are required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key provider = [local] ad { # The name of the Microsoft Windows domaine using the DNS format. This parameter is required. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers. If not set, TheHive uses \"domainFQDN\". #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Microsoft Windows domain name using the short format. This parameter is required. #domainName = \"MYDOMAIN\" # Use SSL to connect to the domain controller(s). #useSSL = true } ldap { # LDAP server name or address. Port can be specified (host:port). This parameter is required. #serverName = \"ldap.mydomain.local:389\" # If you have multiple ldap servers, use the multi-valued settings. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Use SSL to connect to directory server #useSSL = true # Account to use to bind on LDAP server. This parameter is required. #bindDN = \"cn=thehive,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user {0} is replaced by user name. This parameter is required. #filter = \"(cn={0})\" } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h } # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s } # Cortex configuration ######## cortex { #\"CORTEX-SERVER-ID\" { # # URL of MISP server # url = \"\" # #HTTP client configuration, more details in section 8 # ws { # ws.useProxyProperties = true # proxy { # # The hostname of the proxy server. # #host = \"\" # # The port of the proxy server. # #post = 0 # # The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. # #protocol = \"http\" # # The username of the credentials for the proxy server. # #user = \"\" # # The password for the credentials for the proxy server. # #password = \"\" # # The password for the credentials for the proxy server. # #ntlmDomain = \"\" # # The realm's charset. # #encoding = \"\" # # The list of host on which proxy must not be used. # #nonProxyHosts = \"\" # } # ssl { # keyManager { # used for client certificate authentication # stores = [{ # type: \"pkcs12\" // JKS or PEM # path: \"mycert.p12\" # password: \"password1\" # }] # } # # Add certificate authorities to trust remote certificate # trustManager { # stores = [{ # type: \"JKS\" // JKS or PEM # path: \"keystore.jks\" # password: \"password1\" # }] # } # debug = { # ssl = false # trustmanager = false # keymanager = false # sslctx = false # handshake = false # verbose = false # data = false # certpath = false # } # # # default SSL protocol # #protocol = \"TLSv1.2\" # # # list of enabled SSL protocols # #ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] # # # SSL Cipher suite # #enabledCipherSuites = [ # # \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", # # \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", # #] # } # } #} } # MISP configuration ######## misp { #\"MISP-SERVER-ID\" { # # URL of MISP server # url = \"\" # # authentication key # key = \"\" # #tags to be added to imported artifact # tags = [\"misp\"] # # # filters: # # the maximum number of attributes (max-attributes) # #max-attributes = 1000 # # the maximum size of the event json message # #max-size = 1 MiB # # the age of the last publication # #max-age = 7 days # exclusion { # # the organisation is black-listed # #organisation = [\"bad organisation\", \"other orga\"] # # one of the tags is black-listed # #tags = [\"tag1\", \"tag2\"] # } # # ws { # ws.useProxyProperties = true # proxy { # # The hostname of the proxy server. # #host = \"\" # # The port of the proxy server. # #post = 0 # # The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. # #protocol = \"http\" # # The username of the credentials for the proxy server. # #user = \"\" # # The password for the credentials for the proxy server. # #password = \"\" # # The password for the credentials for the proxy server. # #ntlmDomain = \"\" # # The realm's charset. # #encoding = \"\" # # The list of host on which proxy must not be used. # #nonProxyHosts = \"\" # } # # ssl { # keyManager { # used for client certificate authentication # stores = [{ # type: \"pkcs12\" // JKS or PEM # path: \"mycert.p12\" # password: \"password1\" # }] # } # # Add certificate authorities to trust remote certificate # trustManager { # stores = [{ # type: \"JKS\" // JKS or PEM # path: \"keystore.jks\" # password: \"password1\" # }] # } # debug = { # ssl = false # trustmanager = false # keymanager = false # sslctx = false # handshake = false # verbose = false # data = false # certpath = false # } # # # default SSL protocol # #protocol = \"TLSv1.2\" # # # list of enabled SSL protocols # #ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] # # # SSL Cipher suite # #enabledCipherSuites = [ # # \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", # # \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", # #] # } # } #} # Interval between two MISP event import interval = 1h } # Metrics configuration ######## metrics { name = default enabled = false rateUnit = SECONDS durationUnit = SECONDS jvm = true logback = true graphite { enabled = false host = \"127.0.0.1\" port = 2003 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS period = 10s } ganglia { enabled = false host = \"127.0.0.1\" port = 8649 mode = UNICAST ttl = 1 version = 3.1 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS tmax = 60 dmax = 0 period = 10s } influx { enabled = false url = \"http://127.0.0.1:8086\" user = root password = root database = thehive retention = default consistency = ALL #tags = { # tag1 = value1 # tag2 = value2 #} period = 10s } }","title":"Default configuration"},{"location":"thehive/legacy/thehive3/admin/schema_version/","text":"Schema version # The data of TheHive is stored in an ElasticSearch index. The name of the index is suffixed by the revision of the schema. When the schema of TheHive database changes, a new one is created and the version is incremented. By default, index base name is \"the_hive\" but can be configured ( index.index in application.conf). The following table show for each version of TheHive the default name of the index: TheHive version Index name 2.9.1 the_hive_7 2.9.2 the_hive_7 2.10.0 the_hive_8 2.10.1 the_hive_8 2.10.2 the_hive_8 2.11.0 the_hive_9 2.11.1 the_hive_9 2.11.2 the_hive_9 2.11.3 the_hive_9 2.12.0 the_hive_10 2.12.1 the_hive_10 2.13.0 the_hive_10 2.13.1 the_hive_10 2.13.2 the_hive_11 3.0.0 the_hive_12 3.0.1 the_hive_12 3.0.2 the_hive_12 3.0.3 the_hive_12 3.0.4 the_hive_13","title":"Schema version"},{"location":"thehive/legacy/thehive3/admin/schema_version/#schema-version","text":"The data of TheHive is stored in an ElasticSearch index. The name of the index is suffixed by the revision of the schema. When the schema of TheHive database changes, a new one is created and the version is incremented. By default, index base name is \"the_hive\" but can be configured ( index.index in application.conf). The following table show for each version of TheHive the default name of the index: TheHive version Index name 2.9.1 the_hive_7 2.9.2 the_hive_7 2.10.0 the_hive_8 2.10.1 the_hive_8 2.10.2 the_hive_8 2.11.0 the_hive_9 2.11.1 the_hive_9 2.11.2 the_hive_9 2.11.3 the_hive_9 2.12.0 the_hive_10 2.12.1 the_hive_10 2.13.0 the_hive_10 2.13.1 the_hive_10 2.13.2 the_hive_11 3.0.0 the_hive_12 3.0.1 the_hive_12 3.0.2 the_hive_12 3.0.3 the_hive_12 3.0.4 the_hive_13","title":"Schema version"},{"location":"thehive/legacy/thehive3/admin/updating/","text":"Update TheHive # TheHive is simple to update. You only need to replace your current package files by new ones. If the schema of the data changes between the two versions, the first request to the application asks the user to start a data migration. In this case, authentication is not required. This process creates a new index in ElasticSearch (suffixed by the version of the schema) and copies all the data on it (before adapting its format). It is always possible to rollback to the previous version but all modifications done on the new version will be lost.","title":"Update TheHive"},{"location":"thehive/legacy/thehive3/admin/updating/#update-thehive","text":"TheHive is simple to update. You only need to replace your current package files by new ones. If the schema of the data changes between the two versions, the first request to the application asks the user to start a data migration. In this case, authentication is not required. This process creates a new index in ElasticSearch (suffixed by the version of the schema) and copies all the data on it (before adapting its format). It is always possible to rollback to the previous version but all modifications done on the new version will be lost.","title":"Update TheHive"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/","text":"This document is related to upgrading TheHive and Elasticsearch on Ubuntu 16.04 . Upgrade TheHive to version 3.4 # Perform a backup of data # curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/opt/backup\", \"compress\": true } }' Next: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"<INDEX>\" }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/the_hive_152019060701_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"the_hive_15\" }' Output example: { \"snapshot\" : { \"snapshot\" : \"the_hive_152019060701_1\" , \"uuid\" : \"ZKhBL2BHTAS2g71Xby2OgQ\" , \"version_id\" : 5061699 , \"version\" : \"5.6.16\" , \"indices\" : [ \"the_hive_15\" ], \"state\" : \"SUCCESS\" , \"start_time\" : \"2019-06-07T13:07:38.844Z\" , \"start_time_in_millis\" : 1559912858844 , \"end_time\" : \"2019-06-07T13:07:40.640Z\" , \"end_time_in_millis\" : 1559912860640 , \"duration_in_millis\" : 1796 , \"failures\" : [ ], \"shards\" : { \"total\" : 5 , \"failed\" : 0 , \"successful\" : 5 } } } You can find more information about backup and restore in the dedicated documentation . Stop TheHive service # service thehive stop Update TheHive configuration # Current /etc/thehive/application.conf # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch cluster name. cluster = hive # ElasticSearch instance address. #host = [\"127.0.0.1:9300\"] [..] } New /etc/thehive/application.conf : Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200\" [] } cluster { name = hive } Upgrade TheHive # apt upgrade thehive Restart TheHive service # Ensure everything is working. Upgrade Elasticsearch from version 5.x to 6.x # This is greatly inspired by the official documentation : https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html with additional info we had to set up to make everything work. Upgrading from earlier 5.x versions requires a full cluster restart . Disable shard allocation # curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": \"none\" } } ' Output: {\"acknowledged\":true,\"persistent\":{\"cluster\":{\"routing\":{\"allocation\":{\"enable\":\"none\"}}}},\"transient\":{}} curl -X POST \"localhost:9200/_flush/synced\" Output : {\"_shards\":{\"total\":60,\"successful\":30,\"failed\":0},\"cortex_4\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_2\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_3\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_13\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_15\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_14\":{\"total\":10,\"successful\":5,\"failed\":0}} shut down a single node # sudo -i service elasticsearch stop Upgrade the node you shut down # wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - sudo apt-get install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/6.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list sudo apt-get update && sudo apt-get install elasticsearch Upgrade plugins # /usr/share/elasticsearch/bin/elasticsearch-plugin list ## for all plugin: /usr/share/elasticsearch/bin/elasticsearch-plugin install $plugin Update Elasticsearch configuration # Add path.logs and path.data in `/etc/elasticsearch/elasticsearch.yml: http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.index.queue_size : 100000 thread_pool.search.queue_size : 100000 thread_pool.bulk.queue_size : 100000 path.repo : [ \"/opt/backup\" ] path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" Set $JAVA_HOME in /etc/default/elasticsearch for example: [..] JAVA_HOME=/usr/lib/jvm/java-8-oracle/ [..] On Ubuntu 16.04 we had to set read persmissions manually to this file: chmod o+r /etc/default/elasticsearch Restart the node # sudo update-rc.d elasticsearch defaults 95 10 sudo -i service elasticsearch start Check that elasticsearch is running # curl -X GET \"localhost:9200/\" curl -X GET \"localhost:9200/_cat/nodes\" Reenable shard allocation # Once the node has joined the cluster: curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": null } } ' Output: {\"acknowledged\":true,\"persistent\":{},\"transient\":{}} Wait for the node to recover # Before upgrading the next node, wait for the cluster to finish shard allocation. You can check progress by submitting a _cat/health request: curl -X GET \"localhost:9200/_cat/health?v\" Resources # https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/restart-upgrade.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/deb.html","title":"Upgrade to thehive 3 4 and es 6 x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-thehive-to-version-34","text":"","title":"Upgrade TheHive to version 3.4"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#perform-a-backup-of-data","text":"curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/opt/backup\", \"compress\": true } }' Next: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"<INDEX>\" }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/the_hive_152019060701_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"the_hive_15\" }' Output example: { \"snapshot\" : { \"snapshot\" : \"the_hive_152019060701_1\" , \"uuid\" : \"ZKhBL2BHTAS2g71Xby2OgQ\" , \"version_id\" : 5061699 , \"version\" : \"5.6.16\" , \"indices\" : [ \"the_hive_15\" ], \"state\" : \"SUCCESS\" , \"start_time\" : \"2019-06-07T13:07:38.844Z\" , \"start_time_in_millis\" : 1559912858844 , \"end_time\" : \"2019-06-07T13:07:40.640Z\" , \"end_time_in_millis\" : 1559912860640 , \"duration_in_millis\" : 1796 , \"failures\" : [ ], \"shards\" : { \"total\" : 5 , \"failed\" : 0 , \"successful\" : 5 } } } You can find more information about backup and restore in the dedicated documentation .","title":"Perform a backup of data"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#stop-thehive-service","text":"service thehive stop","title":"Stop TheHive service"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#update-thehive-configuration","text":"Current /etc/thehive/application.conf # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch cluster name. cluster = hive # ElasticSearch instance address. #host = [\"127.0.0.1:9300\"] [..] } New /etc/thehive/application.conf : Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200\" [] } cluster { name = hive }","title":"Update TheHive configuration"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-thehive","text":"apt upgrade thehive","title":"Upgrade TheHive"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#restart-thehive-service","text":"Ensure everything is working.","title":"Restart TheHive service"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-elasticsearch-from-version-5x-to-6x","text":"This is greatly inspired by the official documentation : https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html with additional info we had to set up to make everything work. Upgrading from earlier 5.x versions requires a full cluster restart .","title":"Upgrade Elasticsearch from version 5.x to 6.x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#disable-shard-allocation","text":"curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": \"none\" } } ' Output: {\"acknowledged\":true,\"persistent\":{\"cluster\":{\"routing\":{\"allocation\":{\"enable\":\"none\"}}}},\"transient\":{}} curl -X POST \"localhost:9200/_flush/synced\" Output : {\"_shards\":{\"total\":60,\"successful\":30,\"failed\":0},\"cortex_4\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_2\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_3\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_13\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_15\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_14\":{\"total\":10,\"successful\":5,\"failed\":0}}","title":"Disable shard allocation"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#shut-down-a-single-node","text":"sudo -i service elasticsearch stop","title":"shut down a single node"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-the-node-you-shut-down","text":"wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - sudo apt-get install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/6.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list sudo apt-get update && sudo apt-get install elasticsearch","title":"Upgrade the node you shut down"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-plugins","text":"/usr/share/elasticsearch/bin/elasticsearch-plugin list ## for all plugin: /usr/share/elasticsearch/bin/elasticsearch-plugin install $plugin","title":"Upgrade plugins"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#update-elasticsearch-configuration","text":"Add path.logs and path.data in `/etc/elasticsearch/elasticsearch.yml: http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.index.queue_size : 100000 thread_pool.search.queue_size : 100000 thread_pool.bulk.queue_size : 100000 path.repo : [ \"/opt/backup\" ] path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" Set $JAVA_HOME in /etc/default/elasticsearch for example: [..] JAVA_HOME=/usr/lib/jvm/java-8-oracle/ [..] On Ubuntu 16.04 we had to set read persmissions manually to this file: chmod o+r /etc/default/elasticsearch","title":"Update Elasticsearch configuration"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#restart-the-node","text":"sudo update-rc.d elasticsearch defaults 95 10 sudo -i service elasticsearch start","title":"Restart the node"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#check-that-elasticsearch-is-running","text":"curl -X GET \"localhost:9200/\" curl -X GET \"localhost:9200/_cat/nodes\"","title":"Check that elasticsearch is running"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#reenable-shard-allocation","text":"Once the node has joined the cluster: curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": null } } ' Output: {\"acknowledged\":true,\"persistent\":{},\"transient\":{}}","title":"Reenable shard allocation"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#wait-for-the-node-to-recover","text":"Before upgrading the next node, wait for the cluster to finish shard allocation. You can check progress by submitting a _cat/health request: curl -X GET \"localhost:9200/_cat/health?v\"","title":"Wait for the node to recover"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#resources","text":"https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/restart-upgrade.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/deb.html","title":"Resources"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/","text":"Migration from Elasticsearch 6.8.2 to ES 7.x # \u26a0\ufe0f IMPORTANT NOTE This migration process is intended for single node of Elasticsearch database The current version of this document is provided for testing purpose ONLY! This guide has been written and tested to migrate data from ES 6.8.2 to ES 7.8.1, and TheHive 3.4.2 to TheHive 3.5.0-RC1 only! This guide starts with Elasticsearch version 6.8.2 up and running, indexes and data. To test this guide, we recommend using a backup of you production server. (see Backup and Restore page for more information) This guide is illustrated with TheHive index. The process is identical for Cortex, you just have to adjust index names. Prerequisite # The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Identify if your index should be reindexed # You can easily identify if indexes should be reindexed or not. On the index named the_hive_15 run the following command: curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created' if the output is similar to \"5xxxxxx\" then reindexing is required, you should follow this guide. If it is \"6xxxxxx\" then the index can be read by Elasticsearch 7.8.x. Upgrade Elasticsearch, and TheHive-3.5.0. Migration guide # Current status # Current context is: - Elasticsearch 6.8.2 - TheHive 3.4.2 All up and running. Start by identifying indices on you Elasticsearch instance. curl http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb The index name is the_hive_15 . Record this somewhere. Stop services # Before starting updating the database, lets stop applications: sudo service thehive stop Create a new index # The First operation lies in creating a new index named new_the_hive_15 with settings from current index the_hive_15 (ensure to keep index version, needed for future upgrade). curl -XPUT 'http://localhost:9200/new_the_hive_15' \\ -H 'Content-Type: application/json' \\ -d \" $( curl http://localhost:9200/the_hive_15 | \\ jq '.the_hive_15 | del(.settings.index.provided_name, .settings.index.creation_date, .settings.index.uuid, .settings.index.version, .settings.index.mapping.single_type, .mappings.doc._all)' ) \" Check the new index is well created: curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open new_the_hive_15 A2KLoZPpSXygutlfy_RNCQ 5 1 0 0 1.1kb 1.1kb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb Proceed to Reindex # Next operation lies in running the reindex command in the newly created index: curl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_reindex -d '{ \"conflicts\": \"proceed\", \"source\": { \"index\": \"the_hive_15\" }, \"dest\": { \"index\": \"new_the_hive_15\" } }' After a moment, you should get a similar output: { \"took\" : 5119 , \"timed_out\" : false , \"total\" : 5889 , \"updated\" : 0 , \"created\" : 5889 , \"deleted\" : 0 , \"batches\" : 6 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [] } Ensure new index has been created # Run the following command, and ensure the new index is like the current one (size can vary): curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb Delete old indices # This is the thrilling part. Now the new index new_the_hive_15 is created and similar the_hive_15, older indexes should be completely deleted from the database. To delete index named the_hive_15 , run the following command: curl -XDELETE http://localhost:9200/the_hive_15 Run the same command for older indexes if exist (the_hive_14, the_hive_13....). Elasticsearch 7.x cannot run with index created with Elasticsearch 5.x. Create an alias # Before stopping Elasticsearch service, let\u2019s create an alias to keep index names in the future. curl -XPOST -H 'Content-Type: application/json' 'http://localhost:9200/_aliases' -d '{ \"actions\": [ { \"add\": { \"index\": \"new_the_hive_15\", \"alias\": \"the_hive_15\" } } ] }' Doing so will allow TheHive 3.5.0 to find the index without updating the configuration file. Check the alias has been well created by running the following command curl -XGET http://localhost:9200/_alias?pretty The output should look like: { \"new_the_hive_15\" : { \"aliases\" : { \"the_hive_15\" : { } } } } Stop Elasticsearch version 6.8.2 # sudo service elasticsearch stop Update Elasticsearch # Update the configuration of Elastisearch. Configuration file should look like this: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Now, upgrade Elasticsearch to version 7.x following the documentation for your Operating System, and ensure the service start successfully. Install or update to TheHive 3.5.0 # DEB package # If using Debian based Linux operating system, configure it to follow our beta repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then install it by running: sudo apt install thehive or sudo apt install thehive = 3 .5.0-1 RPM # Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=http://rpm.thehive-project.org/release/noarch gpgcheck=1 Then install it by running: sudo yum install thehive or sudo yum install thehive-3.5.0-1 Install binaries # cd /opt wget https://download.thehive-project.org/thehive-3.5.0-1.zip unzip thehive-3.5.0-1.zip ln -s thehive-3.5.0-1 thehive Docker images # Docker images are also provided on Dockerhub. docker pull thehiveproject/thehive:3.5.0-1 Update Database # Connect to TheHive, the maintenance page should ask to update. Once updated, ensure a new index named the_hive_16 has been created. curl -XGET http://localhost:9200/_cat/indices \\? v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb yellow open the_hive_16 Nz0vCKqhRK2xkx1t_WF-0g 5 1 30977 0 26.1mb 26.1mb","title":"Migration from Elasticsearch 6.8.2 to ES 7.x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#migration-from-elasticsearch-682-to-es-7x","text":"\u26a0\ufe0f IMPORTANT NOTE This migration process is intended for single node of Elasticsearch database The current version of this document is provided for testing purpose ONLY! This guide has been written and tested to migrate data from ES 6.8.2 to ES 7.8.1, and TheHive 3.4.2 to TheHive 3.5.0-RC1 only! This guide starts with Elasticsearch version 6.8.2 up and running, indexes and data. To test this guide, we recommend using a backup of you production server. (see Backup and Restore page for more information) This guide is illustrated with TheHive index. The process is identical for Cortex, you just have to adjust index names.","title":"Migration from Elasticsearch 6.8.2 to ES 7.x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#prerequisite","text":"The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ .","title":"Prerequisite"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#identify-if-your-index-should-be-reindexed","text":"You can easily identify if indexes should be reindexed or not. On the index named the_hive_15 run the following command: curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created' if the output is similar to \"5xxxxxx\" then reindexing is required, you should follow this guide. If it is \"6xxxxxx\" then the index can be read by Elasticsearch 7.8.x. Upgrade Elasticsearch, and TheHive-3.5.0.","title":"Identify if your index should be reindexed"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#migration-guide","text":"","title":"Migration guide"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#current-status","text":"Current context is: - Elasticsearch 6.8.2 - TheHive 3.4.2 All up and running. Start by identifying indices on you Elasticsearch instance. curl http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb The index name is the_hive_15 . Record this somewhere.","title":"Current status"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#stop-services","text":"Before starting updating the database, lets stop applications: sudo service thehive stop","title":"Stop services"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#create-a-new-index","text":"The First operation lies in creating a new index named new_the_hive_15 with settings from current index the_hive_15 (ensure to keep index version, needed for future upgrade). curl -XPUT 'http://localhost:9200/new_the_hive_15' \\ -H 'Content-Type: application/json' \\ -d \" $( curl http://localhost:9200/the_hive_15 | \\ jq '.the_hive_15 | del(.settings.index.provided_name, .settings.index.creation_date, .settings.index.uuid, .settings.index.version, .settings.index.mapping.single_type, .mappings.doc._all)' ) \" Check the new index is well created: curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open new_the_hive_15 A2KLoZPpSXygutlfy_RNCQ 5 1 0 0 1.1kb 1.1kb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb","title":"Create a new index"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#proceed-to-reindex","text":"Next operation lies in running the reindex command in the newly created index: curl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_reindex -d '{ \"conflicts\": \"proceed\", \"source\": { \"index\": \"the_hive_15\" }, \"dest\": { \"index\": \"new_the_hive_15\" } }' After a moment, you should get a similar output: { \"took\" : 5119 , \"timed_out\" : false , \"total\" : 5889 , \"updated\" : 0 , \"created\" : 5889 , \"deleted\" : 0 , \"batches\" : 6 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [] }","title":"Proceed to Reindex"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#ensure-new-index-has-been-created","text":"Run the following command, and ensure the new index is like the current one (size can vary): curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb","title":"Ensure new index has been created"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#delete-old-indices","text":"This is the thrilling part. Now the new index new_the_hive_15 is created and similar the_hive_15, older indexes should be completely deleted from the database. To delete index named the_hive_15 , run the following command: curl -XDELETE http://localhost:9200/the_hive_15 Run the same command for older indexes if exist (the_hive_14, the_hive_13....). Elasticsearch 7.x cannot run with index created with Elasticsearch 5.x.","title":"Delete old indices"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#create-an-alias","text":"Before stopping Elasticsearch service, let\u2019s create an alias to keep index names in the future. curl -XPOST -H 'Content-Type: application/json' 'http://localhost:9200/_aliases' -d '{ \"actions\": [ { \"add\": { \"index\": \"new_the_hive_15\", \"alias\": \"the_hive_15\" } } ] }' Doing so will allow TheHive 3.5.0 to find the index without updating the configuration file. Check the alias has been well created by running the following command curl -XGET http://localhost:9200/_alias?pretty The output should look like: { \"new_the_hive_15\" : { \"aliases\" : { \"the_hive_15\" : { } } } }","title":"Create an alias"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#stop-elasticsearch-version-682","text":"sudo service elasticsearch stop","title":"Stop Elasticsearch version 6.8.2"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#update-elasticsearch","text":"Update the configuration of Elastisearch. Configuration file should look like this: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Now, upgrade Elasticsearch to version 7.x following the documentation for your Operating System, and ensure the service start successfully.","title":"Update Elasticsearch"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#install-or-update-to-thehive-350","text":"","title":"Install or update to TheHive 3.5.0"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#deb-package","text":"If using Debian based Linux operating system, configure it to follow our beta repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then install it by running: sudo apt install thehive or sudo apt install thehive = 3 .5.0-1","title":"DEB package"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#rpm","text":"Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=http://rpm.thehive-project.org/release/noarch gpgcheck=1 Then install it by running: sudo yum install thehive or sudo yum install thehive-3.5.0-1","title":"RPM"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#install-binaries","text":"cd /opt wget https://download.thehive-project.org/thehive-3.5.0-1.zip unzip thehive-3.5.0-1.zip ln -s thehive-3.5.0-1 thehive","title":"Install binaries"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#docker-images","text":"Docker images are also provided on Dockerhub. docker pull thehiveproject/thehive:3.5.0-1","title":"Docker images"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#update-database","text":"Connect to TheHive, the maintenance page should ask to update. Once updated, ensure a new index named the_hive_16 has been created. curl -XGET http://localhost:9200/_cat/indices \\? v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb yellow open the_hive_16 Nz0vCKqhRK2xkx1t_WF-0g 5 1 30977 0 26.1mb 26.1mb","title":"Update Database"},{"location":"thehive/legacy/thehive3/admin/webhooks/","text":"WebHooks # Starting from version 2.13, TheHive supports webhooks . When enabled, TheHive will send each action that has been performed on it (add case, update case, add task, ...), in real time, to an HTTP endpoint. You can then create a program or application on the HTTP endpoint to react on specific events. Configuration Data Sent to the HTTP Endpoint Sample Webhook Server Application Dependencies Python Script Run Configuration # Webhooks are configured using the webhook key in the configuration file ( /etc/thehive/application.conf by default). A minimal configuration contains an arbitrary name and an URL. The URL corresponds to the HTTP endpoint: webhooks { myLocalWebHook { url = \"http://my_HTTP_endpoint/webhook\" } } Proxy and SSL configuration can be added in the same manner as for MISP or Cortex: webhooks { securedWebHook { url = \"https://my_HTTP_endpoint/webhook\" ws { ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } proxy { host: \"10.1.0.1\" port: 3128 } } } } Data Sent to the HTTP Endpoint # For each action performed on it, TheHive sends an audit trail entry in JSON format to the HTTP endpoint. Here is an example corresponding to the creation of a case: { \"operation\": \"Creation\", # Creation, Update or Delete \"objectType\": \"case\", # Type of object \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", # Object ID \"startDate\": 1505476659427, # When the operation has been done \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:426\", # HTTP request ID which has done the operation \"details\": { # Attributes used for creation of update \"customFields\": {}, \"metrics\": {}, \"description\": \"Example of case creation\", \"flag\": false, \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [] }, \"base\": true, # Internal information used to determine the main operation when there are several operations for the same request \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", # ID of the root parent of the object (internal use) \"object\": { # The object after the operation \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } } For an update, the data will look like: { \"operation\": \"Update\", \"details\": { \"severity\": 3 }, \"objectType\": \"case\", \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", \"base\": true, \"startDate\": 1505477372601, \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:446\", \"object\": { \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 3, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"updatedBy\": \"me\", \"updatedAt\": 1505477372246, \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } } Sample Webhook Server Application # The following application is a sample intended to help you get started with webhooks. It is very basic as it listens to a local port and displays the contents of the received POST JSON data. Dependencies # Install dependencies: sudo pip install flask Python Script # Create a simple Python script (e.g. webhooktest.py ): from flask import Flask, request import json app = Flask(__name__) @app.route('/',methods=['POST']) def foo(): data = json.loads(request.data) print(json.dumps(data, indent=4)) return \"OK\" if __name__ == '__main__': app.run() Run # Run the server: python webhooktest.py","title":"WebHooks"},{"location":"thehive/legacy/thehive3/admin/webhooks/#webhooks","text":"Starting from version 2.13, TheHive supports webhooks . When enabled, TheHive will send each action that has been performed on it (add case, update case, add task, ...), in real time, to an HTTP endpoint. You can then create a program or application on the HTTP endpoint to react on specific events. Configuration Data Sent to the HTTP Endpoint Sample Webhook Server Application Dependencies Python Script Run","title":"WebHooks"},{"location":"thehive/legacy/thehive3/admin/webhooks/#configuration","text":"Webhooks are configured using the webhook key in the configuration file ( /etc/thehive/application.conf by default). A minimal configuration contains an arbitrary name and an URL. The URL corresponds to the HTTP endpoint: webhooks { myLocalWebHook { url = \"http://my_HTTP_endpoint/webhook\" } } Proxy and SSL configuration can be added in the same manner as for MISP or Cortex: webhooks { securedWebHook { url = \"https://my_HTTP_endpoint/webhook\" ws { ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } proxy { host: \"10.1.0.1\" port: 3128 } } } }","title":"Configuration"},{"location":"thehive/legacy/thehive3/admin/webhooks/#data-sent-to-the-http-endpoint","text":"For each action performed on it, TheHive sends an audit trail entry in JSON format to the HTTP endpoint. Here is an example corresponding to the creation of a case: { \"operation\": \"Creation\", # Creation, Update or Delete \"objectType\": \"case\", # Type of object \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", # Object ID \"startDate\": 1505476659427, # When the operation has been done \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:426\", # HTTP request ID which has done the operation \"details\": { # Attributes used for creation of update \"customFields\": {}, \"metrics\": {}, \"description\": \"Example of case creation\", \"flag\": false, \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [] }, \"base\": true, # Internal information used to determine the main operation when there are several operations for the same request \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", # ID of the root parent of the object (internal use) \"object\": { # The object after the operation \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } } For an update, the data will look like: { \"operation\": \"Update\", \"details\": { \"severity\": 3 }, \"objectType\": \"case\", \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", \"base\": true, \"startDate\": 1505477372601, \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:446\", \"object\": { \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 3, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"updatedBy\": \"me\", \"updatedAt\": 1505477372246, \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } }","title":"Data Sent to the HTTP Endpoint"},{"location":"thehive/legacy/thehive3/admin/webhooks/#sample-webhook-server-application","text":"The following application is a sample intended to help you get started with webhooks. It is very basic as it listens to a local port and displays the contents of the received POST JSON data.","title":"Sample Webhook Server Application"},{"location":"thehive/legacy/thehive3/admin/webhooks/#dependencies","text":"Install dependencies: sudo pip install flask","title":"Dependencies"},{"location":"thehive/legacy/thehive3/admin/webhooks/#python-script","text":"Create a simple Python script (e.g. webhooktest.py ): from flask import Flask, request import json app = Flask(__name__) @app.route('/',methods=['POST']) def foo(): data = json.loads(request.data) print(json.dumps(data, indent=4)) return \"OK\" if __name__ == '__main__': app.run()","title":"Python Script"},{"location":"thehive/legacy/thehive3/admin/webhooks/#run","text":"Run the server: python webhooktest.py","title":"Run"},{"location":"thehive/legacy/thehive3/api/","text":"TheHive API # TheHive exposes REST APIs through JSON over HTTP. HTTP request format Authentication Model Alert Case Observable Task Log User Connectors","title":"TheHive API"},{"location":"thehive/legacy/thehive3/api/#thehive-api","text":"TheHive exposes REST APIs through JSON over HTTP. HTTP request format Authentication Model Alert Case Observable Task Log User Connectors","title":"TheHive API"},{"location":"thehive/legacy/thehive3/api/alert/","text":"Alert # Model definition # Required attributes: - title (text) : title of the alert - description (text) : description of the alert - severity (number) : severity of the alert (1: low; 2: medium; 3: high) default=2 - date (date) : date and time when the alert was raised default=now - tags (multi-string) : case tags default=empty - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - status (AlertStatus) : status of the alert ( New , Updated , Ignored , Imported ) default=New - type (string) : type of the alert (read only) - source (string) : source of the alert (read only) - sourceRef (string) : source reference of the alert (read only) - artifacts (multi-artifact) : artifact of the alert. It is a array of JSON object containing artifact attributes default=empty - follow (boolean) : if true, the alert becomes active when updated default=true Optional attributes: - caseTemplate (string) : case template to use when a case is created from this alert. If the alert specifies a non-existent case template or doesn't supply one, TheHive will import the alert into a case using a case template that has the exact same name as the alert's type if it exists. For example, if you raise an alert with a type value of splunk and you do not provide the caseTemplate attribute or supply a non-existent one (for example splink ), TheHive will import the alert using the case template called splunk if it exists. Otherwise, the alert will be imported using an empty case (i.e. from scratch). Attributes generated by the backend: - lastSyncDate (date) : date of the last synchronization - case (string) : id of the case, if created Alert ID is computed from type , source and sourceRef . Alert Manipulation # Alert methods # HTTP Method URI Action GET /api/alert List alerts POST /api/alert/_search Find alerts PATCH /api/alert/_bulk Update alerts in bulk POST /api/alert/_stats Compute stats on alerts POST /api/alert Create an alert GET /api/alert/:alertId Get an alert PATCH /api/alert/:alertId Update an alert DELETE /api/alert/:alertId Delete an alert POST /api/alert/:alertId/markAsRead Mark an alert as read POST /api/alert/:alertId/markAsUnread Mark an alert as unread POST /api/alert/:alertId/createCase Create a case from an alert POST /api/alert/:alertId/follow Follow an alert POST /api/alert/:alertId/unfollow Unfollow an alert POST /api/alert/:alertId/merge/:caseId Merge an alert in a case POST /api/alert/merge/_bulk Merge several alerts in one case Get an alert # An alert's details can be retrieve using the url: GET /api/alert/:alertId The alert ID is obtained by List alerts or Find alerts API. If the parameter similarity is set to \"1\" or \"true\", this API returns information on cases which have similar observables. With this feature, output will contain the similarCases attribute which list case details with: - artifactCount: number of observables in the original case - iocCount: number of observables marked as IOC in original case - similarArtifactCount: number of observables which are in alert and in case - similarIocCount: number of IOCs which are in alert and in case warning IOCs are observables Examples # Get alert without similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Get alert with similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810?similarity=1 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\", \"similarCases\": [ { \"_id\": \"AVwwrym-Rw5vhyJUfdJW\", \"artifactCount\": 5, \"endDate\": null, \"id\": \"AVwwrym-Rw5vhyJUfdJW\", \"iocCount\": 1, \"resolutionStatus\": null, \"severity\": 1, \"similarArtifactCount\": 2, \"similarIocCount\": 1, \"startDate\": 1495465039000, \"status\": \"Open\", \"tags\": [ \"src:MISP\" ], \"caseId\": 1405, \"title\": \"TEST TheHive\", \"tlp\": 2 } ] } Create an alert # An alert can be created using the following url: POST /api/alert Required case attributes (cf. models) must be provided. If an alert with the same tuple type , source and sourceRef already exists, TheHive will refuse to create it. This call returns attributes of the created alert. Examples # Creation of a simple alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"New Alert\", \"description\": \"N/A\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\" }' It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Creation of another alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"Other alert\", \"description\": \"alert description\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"severity\": 3, \"tlp\": 3, \"artifacts\": [ { \"dataType\": \"ip\", \"data\": \"127.0.0.1\", \"message\": \"localhost\" }, { \"dataType\": \"domain\", \"data\": \"thehive-project.org\", \"tags\": [\"home\", \"TheHive\"] }, { \"dataType\": \"file\", \"data\": \"logo.svg;image/svg+xml;PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjwhLS0gR2VuZXJhdG9yOiBBZG9iZSBJbGx1c3RyYXRvciAxOC4wLjAsIFNWRyBFeHBvcnQgUGx1Zy1JbiAuIFNWRyBWZXJzaW9uOiA2LjAwIEJ1aWxkIDApICAtLT4NCjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+DQo8c3ZnIHZlcnNpb249IjEuMSIgaWQ9IkxheWVyXzEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6eGxpbms9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkveGxpbmsiIHg9IjBweCIgeT0iMHB4Ig0KCSB2aWV3Qm94PSIwIDAgNjI0IDIwMCIgZW5hYmxlLWJhY2tncm91bmQ9Im5ldyAwIDAgNjI0IDIwMCIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+DQo8Zz4NCgk8Zz4NCgkJPHBhdGggZmlsbD0iIzE1MTYzMiIgZD0iTTE3Mi4yLDczdjY2LjRoLTIwLjdWNzNoLTI3LjRWNTQuOGg3NS41VjczSDE3Mi4yeiIvPg0KCQk8cGF0aCBmaWxsPSIjMTUxNjMyIiBkPSJNMjcyLjgsMTAwLjV2MzguOWgtMjAuMXYtMzQuNmMwLTcuNC00LjQtMTIuNS0xMS0xMi41Yy03LjgsMC0xMyw1LjQtMTMsMTcuN3YyOS40aC0yMC4yVjQ4LjVoMjAuMlY4Mg0KCQkJYzQuOS01LDExLjUtNy45LDE5LjYtNy45QzI2Myw3NC4xLDI3Mi44LDg0LjYsMjcyLjgsMTAwLjV6Ii8+DQoJCTxwYXRoIGZpbGw9IiMxNTE2MzIiIGQ9Ik0zNTYuMywxMTIuOGgtNDYuNGMxLjYsNy42LDYuOCwxMi4yLDEzLjYsMTIuMmM0LjcsMCwxMC4xLTEuMSwxMy41LTcuM2wxNy45LDMuNw0KCQkJYy01LjQsMTMuNC0xNi45LDE5LjgtMzEuNCwxOS44Yy0xOC4zLDAtMzMuNC0xMy41LTMzLjQtMzMuNmMwLTE5LjksMTUuMS0zMy42LDMzLjYtMzMuNmMxNy45LDAsMzIuMywxMi45LDMyLjcsMzMuNlYxMTIuOHoNCgkJCSBNMzEwLjMsMTAwLjVoMjYuMWMtMS45LTYuOC02LjktMTAtMTIuNy0xMEMzMTgsOTAuNSwzMTIuMiw5NCwzMTAuMywxMDAuNXoiLz4NCgkJPHBhdGggZmlsbD0iI0YzRDAyRiIgZD0iTTQ0NS41LDEzOS4zaC0yMC43di0zMy40aC0zNS42djMzLjRoLTIwLjhWNTQuOGgyMC44djMyLjloMzUuNlY1NC44aDIwLjdWMTM5LjN6Ii8+DQoJCTxwYXRoIGZpbGw9IiNGM0QwMkYiIGQ9Ik00NzguNiw1Ny4zYzAsNi40LTQuOSwxMS4yLTExLjcsMTEuMmMtNi44LDAtMTEuNi00LjgtMTEuNi0xMS4yYzAtNi4yLDQuOC0xMS41LDExLjYtMTEuNQ0KCQkJQzQ3My43LDQ1LjgsNDc4LjYsNTEuMSw0NzguNiw1Ny4zeiBNNDU2LjgsMTM5LjNWNzZoMjAuMnY2My4zSDQ1Ni44eiIvPg0KCQk8cGF0aCBmaWxsPSIjRjNEMDJGIiBkPSJNNTI4LjUsMTM5LjNoLTIwLjZsLTI2LjItNjMuNUg1MDNsMTUuMywzOS4xbDE1LjEtMzkuMWgyMS4zTDUyOC41LDEzOS4zeiIvPg0KCQk8cGF0aCBmaWxsPSIjRjNEMDJGIiBkPSJNNjE4LjMsMTEyLjhoLTQ2LjRjMS42LDcuNiw2LjgsMTIuMiwxMy42LDEyLjJjNC43LDAsMTAuMS0xLjEsMTMuNS03LjNsMTcuOSwzLjcNCgkJCWMtNS40LDEzLjQtMTYuOSwxOS44LTMxLjQsMTkuOGMtMTguMywwLTMzLjQtMTMuNS0zMy40LTMzLjZjMC0xOS45LDE1LjEtMzMuNiwzMy42LTMzLjZjMTcuOSwwLDMyLjMsMTIuOSwzMi43LDMzLjZWMTEyLjh6DQoJCQkgTTU3Mi4yLDEwMC41aDI2LjFjLTEuOS02LjgtNi45LTEwLTEyLjctMTBDNTc5LjksOTAuNSw1NzQuMSw5NCw1NzIuMiwxMDAuNXoiLz4NCgk8L2c+DQoJPGc+DQoJCTxnPg0KCQkJPHBhdGggZmlsbD0iI0YzRDAyRiIgZD0iTTU3LDcwLjNjNi42LDAsMTIuMiw2LjQsMTIuMiwxMS41YzAsNi4xLTEwLDYuNi0xMiw2LjZsMCwwYy0yLjIsMC0xMi0wLjMtMTItNi42DQoJCQkJQzQ0LjgsNzYuNyw1MC40LDcwLjMsNTcsNzAuM0w1Nyw3MC4zeiBNNDQuMSwxMzMuNmwyNS4yLDAuMWwyLjIsNS42bC0yOS42LTAuMUw0NC4xLDEzMy42eiBNNDcuNiwxMjUuNmwyLjItNS42bDE0LjIsMGwyLjIsNS42DQoJCQkJTDQ3LjYsMTI1LjZ6IE01MywxMTIuMWwzLjktOS41bDMuOSw5LjVMNTMsMTEyLjF6IE0yMy4zLDE0My42Yy0xLjcsMC0zLjItMC4zLTQuNi0xYy02LjEtMi43LTkuMy05LjgtNi41LTE1LjkNCgkJCQljNi45LTE2LjYsMjcuNy0yOC41LDM5LTMwLjJsLTcuNCwxOC4xbDAsMEwzOC4zLDEyOGwwLDBsLTMuNSw4LjFDMzIuNiwxNDAuNywyOC4yLDE0My42LDIzLjMsMTQzLjZMMjMuMywxNDMuNnogTTU2LjcsMTYxLjgNCgkJCQljLTguMSwwLTE0LjctNS45LTE3LjMtMTVsMzQuNywwLjFDNzEuNCwxNTYuMiw2NC44LDE2MS44LDU2LjcsMTYxLjhMNTYuNywxNjEuOHogTTk1LDE0Mi45Yy0xLjUsMC43LTMuMiwxLTQuNiwxDQoJCQkJYy00LjksMC05LjMtMy0xMS4yLTcuNmwtMy40LTguMWwwLDBsLTUuMS0xMi43YzAtMC41LTAuMi0xLTAuNS0xLjVsLTctMTcuNmMxMS4yLDIsMzIsMTQsMzguOCwzMC41DQoJCQkJQzEwNC4zLDEzMy4zLDEwMS4zLDE0MC40LDk1LDE0Mi45TDk1LDE0Mi45eiIvPg0KCQkJDQoJCQkJPGxpbmUgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjRjNEMDJGIiBzdHJva2Utd2lkdGg9IjUuMjE0NiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbWl0ZXJsaW1pdD0iMTAiIHgxPSI0Ny44IiB5MT0iNjcuNSIgeDI9IjQzLjciIHkyPSI1OC45Ii8+DQoJCQkNCgkJCQk8bGluZSBmaWxsPSJub25lIiBzdHJva2U9IiNGM0QwMkYiIHN0cm9rZS13aWR0aD0iNS4yMTQ2IiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1taXRlcmxpbWl0PSIxMCIgeDE9IjY2LjEiIHkxPSI2Ny41IiB4Mj0iNzAuMSIgeTI9IjU4LjkiLz4NCgkJPC9nPg0KCQkNCgkJCTxwb2x5bGluZSBmaWxsPSJub25lIiBzdHJva2U9IiNGM0QwMkYiIHN0cm9rZS13aWR0aD0iNS4yMTQ2IiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIHN0cm9rZS1taXRlcmxpbWl0PSIxMCIgcG9pbnRzPSINCgkJCTk0LjgsMTAzLjUgMTA1LjUsODQuMiA4MS4xLDQyLjEgMzIuNyw0Mi4xIDguMyw4NC4yIDIwLDEwMy41IAkJIi8+DQoJPC9nPg0KPC9nPg0KPC9zdmc+DQo=\", \"message\": \"logo\" } ], \"caseTemplate\": \"external-alert\" }' Merge an alert # An alert can be merge in a case using the URL: POST /api/alert/:alertId/merge/:caseId Each observable of the alert will be added to the case if it doesn't exist in the case. The description of the alert will be appended to the case's description. The HTTP response contains the updated case. Example # Merge the alert ce2c00f17132359cb3c50dfbb1901810 in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810/merge/AVXeF-pZmeHK_2HEYj2z The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script ### Merged with alert #10 my alert title This is my alert description\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" } Bulk merge alert # This API merge several alerts with one case: POST /api/alert/merge/_bulk The observable of each alert listed in alertIds field will be imported into the case (identified by caseId field). The description of the case is not modified. The HTTP response contains the case. Example # Merge the alerts ce2c00f17132359cb3c50dfbb1901810 and a97148693200f731cfa5237ff2edf67b in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert/merge/_bulk -d '{ \"caseId\": \"AVXeF-pZmeHK_2HEYj2z\", \"alertIds\": [\"ce2c00f17132359cb3c50dfbb1901810\", \"a97148693200f731cfa5237ff2edf67b\"] }' The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" }","title":"Alert"},{"location":"thehive/legacy/thehive3/api/alert/#alert","text":"","title":"Alert"},{"location":"thehive/legacy/thehive3/api/alert/#model-definition","text":"Required attributes: - title (text) : title of the alert - description (text) : description of the alert - severity (number) : severity of the alert (1: low; 2: medium; 3: high) default=2 - date (date) : date and time when the alert was raised default=now - tags (multi-string) : case tags default=empty - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - status (AlertStatus) : status of the alert ( New , Updated , Ignored , Imported ) default=New - type (string) : type of the alert (read only) - source (string) : source of the alert (read only) - sourceRef (string) : source reference of the alert (read only) - artifacts (multi-artifact) : artifact of the alert. It is a array of JSON object containing artifact attributes default=empty - follow (boolean) : if true, the alert becomes active when updated default=true Optional attributes: - caseTemplate (string) : case template to use when a case is created from this alert. If the alert specifies a non-existent case template or doesn't supply one, TheHive will import the alert into a case using a case template that has the exact same name as the alert's type if it exists. For example, if you raise an alert with a type value of splunk and you do not provide the caseTemplate attribute or supply a non-existent one (for example splink ), TheHive will import the alert using the case template called splunk if it exists. Otherwise, the alert will be imported using an empty case (i.e. from scratch). Attributes generated by the backend: - lastSyncDate (date) : date of the last synchronization - case (string) : id of the case, if created Alert ID is computed from type , source and sourceRef .","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/alert/#alert-manipulation","text":"","title":"Alert Manipulation"},{"location":"thehive/legacy/thehive3/api/alert/#alert-methods","text":"HTTP Method URI Action GET /api/alert List alerts POST /api/alert/_search Find alerts PATCH /api/alert/_bulk Update alerts in bulk POST /api/alert/_stats Compute stats on alerts POST /api/alert Create an alert GET /api/alert/:alertId Get an alert PATCH /api/alert/:alertId Update an alert DELETE /api/alert/:alertId Delete an alert POST /api/alert/:alertId/markAsRead Mark an alert as read POST /api/alert/:alertId/markAsUnread Mark an alert as unread POST /api/alert/:alertId/createCase Create a case from an alert POST /api/alert/:alertId/follow Follow an alert POST /api/alert/:alertId/unfollow Unfollow an alert POST /api/alert/:alertId/merge/:caseId Merge an alert in a case POST /api/alert/merge/_bulk Merge several alerts in one case","title":"Alert methods"},{"location":"thehive/legacy/thehive3/api/alert/#get-an-alert","text":"An alert's details can be retrieve using the url: GET /api/alert/:alertId The alert ID is obtained by List alerts or Find alerts API. If the parameter similarity is set to \"1\" or \"true\", this API returns information on cases which have similar observables. With this feature, output will contain the similarCases attribute which list case details with: - artifactCount: number of observables in the original case - iocCount: number of observables marked as IOC in original case - similarArtifactCount: number of observables which are in alert and in case - similarIocCount: number of IOCs which are in alert and in case warning IOCs are observables","title":"Get an alert"},{"location":"thehive/legacy/thehive3/api/alert/#examples","text":"Get alert without similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Get alert with similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810?similarity=1 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\", \"similarCases\": [ { \"_id\": \"AVwwrym-Rw5vhyJUfdJW\", \"artifactCount\": 5, \"endDate\": null, \"id\": \"AVwwrym-Rw5vhyJUfdJW\", \"iocCount\": 1, \"resolutionStatus\": null, \"severity\": 1, \"similarArtifactCount\": 2, \"similarIocCount\": 1, \"startDate\": 1495465039000, \"status\": \"Open\", \"tags\": [ \"src:MISP\" ], \"caseId\": 1405, \"title\": \"TEST TheHive\", \"tlp\": 2 } ] }","title":"Examples"},{"location":"thehive/legacy/thehive3/api/alert/#create-an-alert","text":"An alert can be created using the following url: POST /api/alert Required case attributes (cf. models) must be provided. If an alert with the same tuple type , source and sourceRef already exists, TheHive will refuse to create it. This call returns attributes of the created alert.","title":"Create an alert"},{"location":"thehive/legacy/thehive3/api/alert/#examples_1","text":"Creation of a simple alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"New Alert\", \"description\": \"N/A\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\" }' It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Creation of another alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"Other alert\", \"description\": \"alert description\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"severity\": 3, \"tlp\": 3, \"artifacts\": [ { \"dataType\": \"ip\", \"data\": \"127.0.0.1\", \"message\": \"localhost\" }, { \"dataType\": \"domain\", \"data\": \"thehive-project.org\", \"tags\": [\"home\", \"TheHive\"] }, { \"dataType\": \"file\", \"data\": \"logo.svg;image/svg+xml;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\", \"message\": \"logo\" } ], \"caseTemplate\": \"external-alert\" }'","title":"Examples"},{"location":"thehive/legacy/thehive3/api/alert/#merge-an-alert","text":"An alert can be merge in a case using the URL: POST /api/alert/:alertId/merge/:caseId Each observable of the alert will be added to the case if it doesn't exist in the case. The description of the alert will be appended to the case's description. The HTTP response contains the updated case.","title":"Merge an alert"},{"location":"thehive/legacy/thehive3/api/alert/#example","text":"Merge the alert ce2c00f17132359cb3c50dfbb1901810 in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810/merge/AVXeF-pZmeHK_2HEYj2z The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script ### Merged with alert #10 my alert title This is my alert description\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" }","title":"Example"},{"location":"thehive/legacy/thehive3/api/alert/#bulk-merge-alert","text":"This API merge several alerts with one case: POST /api/alert/merge/_bulk The observable of each alert listed in alertIds field will be imported into the case (identified by caseId field). The description of the case is not modified. The HTTP response contains the case.","title":"Bulk merge alert"},{"location":"thehive/legacy/thehive3/api/alert/#example_1","text":"Merge the alerts ce2c00f17132359cb3c50dfbb1901810 and a97148693200f731cfa5237ff2edf67b in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert/merge/_bulk -d '{ \"caseId\": \"AVXeF-pZmeHK_2HEYj2z\", \"alertIds\": [\"ce2c00f17132359cb3c50dfbb1901810\", \"a97148693200f731cfa5237ff2edf67b\"] }' The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" }","title":"Example"},{"location":"thehive/legacy/thehive3/api/artifact/","text":"Observable # Model definition # Required attributes: data (string) : content of the observable (read only). An observable can't contain data and attachment attributes attachment (attachment) : observable file content (read-only). An observable can't contain data and attachment attributes dataType (enumeration) : type of the observable (read only) message (text) : description of the observable in the context of the case startDate (date) : date of the observable creation default=now tlp (number) : TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 ioc (boolean) : indicates if the observable is an IOC default=false status (artifactStatus) : status of the observable ( Ok or Deleted ) default=Ok Optional attributes: - tags (multi-string) : observable tags Observable manipulation # Observable methods # HTTP Method URI Action POST /api/case/artifact/_search Find observables POST /api/case/artifact/_stats Compute stats on observables POST /api/case/:caseId/artifact Create an observable GET /api/case/artifact/:artifactId Get an observable DELETE /api/case/artifact/:artifactId Remove an observable PATCH /api/case/artifact/:artifactId Update an observable GET /api/case/artifact/:artifactId/similar Get list of similar observables PATCH /api/case/artifact/_bulk Update observables in bulk List Observables of a Case # Complete observable list of a case can be retrieved by performing a search: POST /api/case/artifact/_search Parameters: - query : { \"_parent\": { \"_type\": \"case\", \"_query\": { \"_id\": \"<<caseId>>\" } } } - range : all \\<\\<caseId>> must be replaced by case id (not the case number !)","title":"Observable"},{"location":"thehive/legacy/thehive3/api/artifact/#observable","text":"","title":"Observable"},{"location":"thehive/legacy/thehive3/api/artifact/#model-definition","text":"Required attributes: data (string) : content of the observable (read only). An observable can't contain data and attachment attributes attachment (attachment) : observable file content (read-only). An observable can't contain data and attachment attributes dataType (enumeration) : type of the observable (read only) message (text) : description of the observable in the context of the case startDate (date) : date of the observable creation default=now tlp (number) : TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 ioc (boolean) : indicates if the observable is an IOC default=false status (artifactStatus) : status of the observable ( Ok or Deleted ) default=Ok Optional attributes: - tags (multi-string) : observable tags","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/artifact/#observable-manipulation","text":"","title":"Observable manipulation"},{"location":"thehive/legacy/thehive3/api/artifact/#observable-methods","text":"HTTP Method URI Action POST /api/case/artifact/_search Find observables POST /api/case/artifact/_stats Compute stats on observables POST /api/case/:caseId/artifact Create an observable GET /api/case/artifact/:artifactId Get an observable DELETE /api/case/artifact/:artifactId Remove an observable PATCH /api/case/artifact/:artifactId Update an observable GET /api/case/artifact/:artifactId/similar Get list of similar observables PATCH /api/case/artifact/_bulk Update observables in bulk","title":"Observable methods"},{"location":"thehive/legacy/thehive3/api/artifact/#list-observables-of-a-case","text":"Complete observable list of a case can be retrieved by performing a search: POST /api/case/artifact/_search Parameters: - query : { \"_parent\": { \"_type\": \"case\", \"_query\": { \"_id\": \"<<caseId>>\" } } } - range : all \\<\\<caseId>> must be replaced by case id (not the case number !)","title":"List Observables of a Case"},{"location":"thehive/legacy/thehive3/api/authentication/","text":"Authentication # Most API calls require authentication. Credentials can be provided using a session cookie, an API key or directly using HTTP basic authentication (when enabled). Session cookie is suitable for browser authentication, not for a dedicated tool. The easiest solution if you want to write a tool that leverages TheHive's API is to use API key authentication. API keys can be generated using the Web interface of the product, under the user admin area. For example, to list cases, use the following curl command: # Using API key curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case TheHive also supports basic authentication (disabled by default). You can enable it by adding auth.method.basic=true in the configuration file. # Using basic authentication curl -u mylogin:mypassword http://127.0.0.1:9000/api/case","title":"Authentication"},{"location":"thehive/legacy/thehive3/api/authentication/#authentication","text":"Most API calls require authentication. Credentials can be provided using a session cookie, an API key or directly using HTTP basic authentication (when enabled). Session cookie is suitable for browser authentication, not for a dedicated tool. The easiest solution if you want to write a tool that leverages TheHive's API is to use API key authentication. API keys can be generated using the Web interface of the product, under the user admin area. For example, to list cases, use the following curl command: # Using API key curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case TheHive also supports basic authentication (disabled by default). You can enable it by adding auth.method.basic=true in the configuration file. # Using basic authentication curl -u mylogin:mypassword http://127.0.0.1:9000/api/case","title":"Authentication"},{"location":"thehive/legacy/thehive3/api/case/","text":"Case # Model definition # Required attributes: - title (text) : title of the case - description (text) : description of the case - severity (number) : severity of the case (1: low; 2: medium; 3: high) default=2 - startDate (date) : date and time of the begin of the case default=now - owner (string) : user to whom the case has been assigned default=use who create the case - flag (boolean) : flag of the case default=false - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - tags (multi-string) : case tags default=empty Optional attributes: - resolutionStatus (caseResolutionStatus) : resolution status of the case ( Indeterminate , FalsePositive , TruePositive , Other or Duplicated ) - impactStatus (caseImpactStatus) : impact status of the case ( NoImpact , WithImpact or NotApplicable ) - summary (text) : summary of the case, to be provided when closing a case - endDate (date) : resolution date - metrics (metrics) : list of metrics Attributes generated by the backend: - status (caseStatus) : status of the case ( Open , Resolved or Deleted ) default=Open - caseId (number) : Id of the case (auto-generated) - mergeInto (string) : ID of the case created by the merge - mergeFrom (multi-string) : IDs of the cases that were merged Case Manipulation # Case methods # HTTP Method URI Action GET /api/case List cases POST /api/case/_search Find cases PATCH /api/case/_bulk Update cases in bulk POST /api/case/_stats Compute stats on cases POST /api/case Create a case GET /api/case/:caseId Get a case PATCH /api/case/:caseId Update a case DELETE /api/case/:caseId Remove a case GET /api/case/:caseId/links Get list of cases linked to this case POST /api/case/:caseId1/_merge/:caseId2 Merge two cases Create a Case # A case can be created using the following url : POST /api/case Required case attributes (cf. models) must be provided. This call returns attributes of the created case. Examples # Creation of a simple case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" }' It returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_type\":\"case\" } Creation of another case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My second case\", \"description\": \"This case has been created by my custom script, its severity is high, tlp is red and it contains tags\", \"severity\": 3, \"tlp\": 3, \"tags\": [\"automatic\", \"creation\"] }' Creating a case with Tasks & Customfields: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" \"tasks\": [{ \"title\": \"mytask\", \"description\": \"description of my task\" }], \"customFields\": { \"cvss\": { \"number\": 9, }, \"businessImpact\": { \"string\": \"HIGH\" } } }' For the customFields object, the attribute names should correspond to the ExternalReference (cvss and businessImpact in the example above) not to the name of custom fields.","title":"Case"},{"location":"thehive/legacy/thehive3/api/case/#case","text":"","title":"Case"},{"location":"thehive/legacy/thehive3/api/case/#model-definition","text":"Required attributes: - title (text) : title of the case - description (text) : description of the case - severity (number) : severity of the case (1: low; 2: medium; 3: high) default=2 - startDate (date) : date and time of the begin of the case default=now - owner (string) : user to whom the case has been assigned default=use who create the case - flag (boolean) : flag of the case default=false - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - tags (multi-string) : case tags default=empty Optional attributes: - resolutionStatus (caseResolutionStatus) : resolution status of the case ( Indeterminate , FalsePositive , TruePositive , Other or Duplicated ) - impactStatus (caseImpactStatus) : impact status of the case ( NoImpact , WithImpact or NotApplicable ) - summary (text) : summary of the case, to be provided when closing a case - endDate (date) : resolution date - metrics (metrics) : list of metrics Attributes generated by the backend: - status (caseStatus) : status of the case ( Open , Resolved or Deleted ) default=Open - caseId (number) : Id of the case (auto-generated) - mergeInto (string) : ID of the case created by the merge - mergeFrom (multi-string) : IDs of the cases that were merged","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/case/#case-manipulation","text":"","title":"Case Manipulation"},{"location":"thehive/legacy/thehive3/api/case/#case-methods","text":"HTTP Method URI Action GET /api/case List cases POST /api/case/_search Find cases PATCH /api/case/_bulk Update cases in bulk POST /api/case/_stats Compute stats on cases POST /api/case Create a case GET /api/case/:caseId Get a case PATCH /api/case/:caseId Update a case DELETE /api/case/:caseId Remove a case GET /api/case/:caseId/links Get list of cases linked to this case POST /api/case/:caseId1/_merge/:caseId2 Merge two cases","title":"Case methods"},{"location":"thehive/legacy/thehive3/api/case/#create-a-case","text":"A case can be created using the following url : POST /api/case Required case attributes (cf. models) must be provided. This call returns attributes of the created case.","title":"Create a Case"},{"location":"thehive/legacy/thehive3/api/case/#examples","text":"Creation of a simple case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" }' It returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_type\":\"case\" } Creation of another case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My second case\", \"description\": \"This case has been created by my custom script, its severity is high, tlp is red and it contains tags\", \"severity\": 3, \"tlp\": 3, \"tags\": [\"automatic\", \"creation\"] }' Creating a case with Tasks & Customfields: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" \"tasks\": [{ \"title\": \"mytask\", \"description\": \"description of my task\" }], \"customFields\": { \"cvss\": { \"number\": 9, }, \"businessImpact\": { \"string\": \"HIGH\" } } }' For the customFields object, the attribute names should correspond to the ExternalReference (cvss and businessImpact in the example above) not to the name of custom fields.","title":"Examples"},{"location":"thehive/legacy/thehive3/api/log/","text":"Log # Model definition # Required attributes: - message (text) : content of the Log - startDate (date) : date of the log submission default=now - status (logStatus) : status of the log ( Ok or Deleted ) default=Ok Optional attributes: - attachment (attachment) : file attached to the log Log manipulation # Log methods # HTTP Method URI Action GET /api/case/task/:taskId/log Get logs of the task POST /api/case/task/:taskId/log/_search Find logs in specified task POST /api/case/task/log/_search Find logs POST /api/case/task/:taskId/log Create a log PATCH /api/case/task/log/:logId Update a log DELETE /api/case/task/log/:logId Remove a log GET /api/case/task/log/:logId Get a log Create a log # The URL used to create a task is: POST /api/case/task/<<taskId>>/log \\<\\<taskId>> must be replaced by task id Required log attributes (cf. models) must be provided. This call returns attributes of the created log. Examples # Creation of a simple log in task AVqqeXc9yQ6w1DNC8aDj : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -d '{ \"message\": \"Some message\" }' It returns: { \"startDate\": 1488919949497, \"createdBy\": \"admin\", \"createdAt\": 1488919949495, \"user\": \"myuser\", \"message\":\"Some message\", \"status\": \"Ok\", \"id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_type\":\"case_task_log\" } If log contains an attachment, the request must be in multipart format: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -F '_json={\"message\": \"Screenshot of fake site\"};type=application/json' -F 'attachment=@screenshot1.png;type=image/png' It returns: { \"createdBy\": \"myuser\", \"message\": \"Screenshot of fake site\", \"createdAt\": 1488920587391, \"startDate\": 1488920587394, \"user\": \"myuser\", \"status\": \"Ok\", \"attachment\": { \"name\": \"screenshot1.png\", \"hashes\": [ \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\", \"8b81e038ae0809488f20b5ec7dc91e488ef601e2\", \"c5883708f42a00c3ab1fba5bbb65786c\" ], \"size\": 15296, \"contentType\": \"image/png\", \"id\": \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\" }, \"id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_type\": \"case_task_log\" }","title":"Log"},{"location":"thehive/legacy/thehive3/api/log/#log","text":"","title":"Log"},{"location":"thehive/legacy/thehive3/api/log/#model-definition","text":"Required attributes: - message (text) : content of the Log - startDate (date) : date of the log submission default=now - status (logStatus) : status of the log ( Ok or Deleted ) default=Ok Optional attributes: - attachment (attachment) : file attached to the log","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/log/#log-manipulation","text":"","title":"Log manipulation"},{"location":"thehive/legacy/thehive3/api/log/#log-methods","text":"HTTP Method URI Action GET /api/case/task/:taskId/log Get logs of the task POST /api/case/task/:taskId/log/_search Find logs in specified task POST /api/case/task/log/_search Find logs POST /api/case/task/:taskId/log Create a log PATCH /api/case/task/log/:logId Update a log DELETE /api/case/task/log/:logId Remove a log GET /api/case/task/log/:logId Get a log","title":"Log methods"},{"location":"thehive/legacy/thehive3/api/log/#create-a-log","text":"The URL used to create a task is: POST /api/case/task/<<taskId>>/log \\<\\<taskId>> must be replaced by task id Required log attributes (cf. models) must be provided. This call returns attributes of the created log.","title":"Create a log"},{"location":"thehive/legacy/thehive3/api/log/#examples","text":"Creation of a simple log in task AVqqeXc9yQ6w1DNC8aDj : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -d '{ \"message\": \"Some message\" }' It returns: { \"startDate\": 1488919949497, \"createdBy\": \"admin\", \"createdAt\": 1488919949495, \"user\": \"myuser\", \"message\":\"Some message\", \"status\": \"Ok\", \"id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_type\":\"case_task_log\" } If log contains an attachment, the request must be in multipart format: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -F '_json={\"message\": \"Screenshot of fake site\"};type=application/json' -F 'attachment=@screenshot1.png;type=image/png' It returns: { \"createdBy\": \"myuser\", \"message\": \"Screenshot of fake site\", \"createdAt\": 1488920587391, \"startDate\": 1488920587394, \"user\": \"myuser\", \"status\": \"Ok\", \"attachment\": { \"name\": \"screenshot1.png\", \"hashes\": [ \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\", \"8b81e038ae0809488f20b5ec7dc91e488ef601e2\", \"c5883708f42a00c3ab1fba5bbb65786c\" ], \"size\": 15296, \"contentType\": \"image/png\", \"id\": \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\" }, \"id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_type\": \"case_task_log\" }","title":"Examples"},{"location":"thehive/legacy/thehive3/api/model/","text":"TheHive Model Definition # Field Types # string : textual data (example \"malware\"). text : textual data. The difference between string and text is in the way content can be searched. string is searchable as-is whereas text , words (token) are searchable, not the whole content (example \"Ten users have received this ransomware\"). date : date and time using timestamps with milliseconds format. boolean : true or false number : numeric value metrics : JSON object that contains only numbers Field can be prefixed with multi- in order to indicate that multiple values can be provided. Common Attributes # All entities share the following attributes: - createdBy (text) : login of the user who created the entity - createdAt (date) : date and time of the creation - updatedBy (text) : login of the user who last updated the entity - upadtedAt (date) : date and time of the last update - user (text) : same value as createdBy (this field is deprecated) These attributes are handled by the back-end and can't be directly updated.","title":"TheHive Model Definition"},{"location":"thehive/legacy/thehive3/api/model/#thehive-model-definition","text":"","title":"TheHive Model Definition"},{"location":"thehive/legacy/thehive3/api/model/#field-types","text":"string : textual data (example \"malware\"). text : textual data. The difference between string and text is in the way content can be searched. string is searchable as-is whereas text , words (token) are searchable, not the whole content (example \"Ten users have received this ransomware\"). date : date and time using timestamps with milliseconds format. boolean : true or false number : numeric value metrics : JSON object that contains only numbers Field can be prefixed with multi- in order to indicate that multiple values can be provided.","title":"Field Types"},{"location":"thehive/legacy/thehive3/api/model/#common-attributes","text":"All entities share the following attributes: - createdBy (text) : login of the user who created the entity - createdAt (date) : date and time of the creation - updatedBy (text) : login of the user who last updated the entity - upadtedAt (date) : date and time of the last update - user (text) : same value as createdBy (this field is deprecated) These attributes are handled by the back-end and can't be directly updated.","title":"Common Attributes"},{"location":"thehive/legacy/thehive3/api/request/","text":"Request formats # TheHive accepts several parameter formats within a HTTP request. They can be used indifferently. Input data can be: - a query string - URL-encoded form - multi-part - JSON Hence, the requests below are equivalent. Query String # curl -XPOST 'http://127.0.0.1:9000/api/login?user=me&password=secret' URL-encoded Form # curl -XPOST 'http://127.0.0.1:9000/api/login' -d user=me -d password=secret JSON # curl -XPOST http://127.0.0.1:9000/api/login -H 'Content-Type: application/json' -d '{ \"user\": \"me\", \"password\": \"secret\" }' Multi-part # curl -XPOST http://127.0.0.1:9000/api/login -F '_json=<-;type=application/json' << _EOF_ { \"user\": \"me\", \"password\": \"secret\" } _EOF_ ResponseFormat # TheHive outputs JSON data.","title":"Request"},{"location":"thehive/legacy/thehive3/api/request/#request-formats","text":"TheHive accepts several parameter formats within a HTTP request. They can be used indifferently. Input data can be: - a query string - URL-encoded form - multi-part - JSON Hence, the requests below are equivalent.","title":"Request formats"},{"location":"thehive/legacy/thehive3/api/request/#query-string","text":"curl -XPOST 'http://127.0.0.1:9000/api/login?user=me&password=secret'","title":"Query String"},{"location":"thehive/legacy/thehive3/api/request/#url-encoded-form","text":"curl -XPOST 'http://127.0.0.1:9000/api/login' -d user=me -d password=secret","title":"URL-encoded Form"},{"location":"thehive/legacy/thehive3/api/request/#json","text":"curl -XPOST http://127.0.0.1:9000/api/login -H 'Content-Type: application/json' -d '{ \"user\": \"me\", \"password\": \"secret\" }'","title":"JSON"},{"location":"thehive/legacy/thehive3/api/request/#multi-part","text":"curl -XPOST http://127.0.0.1:9000/api/login -F '_json=<-;type=application/json' << _EOF_ { \"user\": \"me\", \"password\": \"secret\" } _EOF_","title":"Multi-part"},{"location":"thehive/legacy/thehive3/api/request/#responseformat","text":"TheHive outputs JSON data.","title":"ResponseFormat"},{"location":"thehive/legacy/thehive3/api/task/","text":"Task # Model definition # Required attributes: - title (text) : title of the task - status (taskStatus) : status of the task ( Waiting , InProgress , Completed or Cancel ) default=Waiting - flag (boolean) : flag of the task default=false Optional attributes: - owner (string) : user who owns the task. This is automatically set to current user when status is set to InProgress - description (text) : task details - startDate (date) : date of the beginning of the task. This is automatically set when status is set to Open - endDate (date) : date of the end of the task. This is automatically set when status is set to Completed Task manipulation # Task methods # HTTP Method URI Action POST /api/case/:caseId/task/_search Find tasks in a case (deprecated) POST /api/case/task/_search Find tasks POST /api/case/task/_stats Compute stats on tasks GET /api/case/task/:taskId Get a task PATCH /api/case/task/:taskId Update a task POST /api/case/:caseId/task Create a task Create a task # The URL used to create a task is: POST /api/case/<<caseId>>/task \\<\\<caseId>> must be replaced by case id (not the case number !) Required task attributes (cf. models) must be provided. This call returns attributes of the created task. Examples # Creation of a simple task in case AVqqdpY2yQ6w1DNC8aDh : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Do something\" }' It returns: { \"createdAt\": 1488918771513, \"status\": \"Waiting\", \"createdBy\": \"myuser\", \"title\": \"Do something\", \"order\": 0, \"user\": \"myuser\", \"flag\": false, \"id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_type\":\"case_task\" } Creation of another task: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Analyze the malware\", \"description\": \"The malware XXX is analyzed using sandbox ...\", \"owner\": \"Joe\", \"status\": \"InProgress\" }'","title":"Task"},{"location":"thehive/legacy/thehive3/api/task/#task","text":"","title":"Task"},{"location":"thehive/legacy/thehive3/api/task/#model-definition","text":"Required attributes: - title (text) : title of the task - status (taskStatus) : status of the task ( Waiting , InProgress , Completed or Cancel ) default=Waiting - flag (boolean) : flag of the task default=false Optional attributes: - owner (string) : user who owns the task. This is automatically set to current user when status is set to InProgress - description (text) : task details - startDate (date) : date of the beginning of the task. This is automatically set when status is set to Open - endDate (date) : date of the end of the task. This is automatically set when status is set to Completed","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/task/#task-manipulation","text":"","title":"Task manipulation"},{"location":"thehive/legacy/thehive3/api/task/#task-methods","text":"HTTP Method URI Action POST /api/case/:caseId/task/_search Find tasks in a case (deprecated) POST /api/case/task/_search Find tasks POST /api/case/task/_stats Compute stats on tasks GET /api/case/task/:taskId Get a task PATCH /api/case/task/:taskId Update a task POST /api/case/:caseId/task Create a task","title":"Task methods"},{"location":"thehive/legacy/thehive3/api/task/#create-a-task","text":"The URL used to create a task is: POST /api/case/<<caseId>>/task \\<\\<caseId>> must be replaced by case id (not the case number !) Required task attributes (cf. models) must be provided. This call returns attributes of the created task.","title":"Create a task"},{"location":"thehive/legacy/thehive3/api/task/#examples","text":"Creation of a simple task in case AVqqdpY2yQ6w1DNC8aDh : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Do something\" }' It returns: { \"createdAt\": 1488918771513, \"status\": \"Waiting\", \"createdBy\": \"myuser\", \"title\": \"Do something\", \"order\": 0, \"user\": \"myuser\", \"flag\": false, \"id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_type\":\"case_task\" } Creation of another task: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Analyze the malware\", \"description\": \"The malware XXX is analyzed using sandbox ...\", \"owner\": \"Joe\", \"status\": \"InProgress\" }'","title":"Examples"},{"location":"thehive/legacy/thehive3/api/user/","text":"User # Model definition # Required attributes: - login / id (string) : login of the user - userName (text) : Full name of the user - roles (multi-userRole) : Array containing roles of the user ( read , write or admin ) - status (userStatus) : Ok or Locked default=Ok - preference (string) : JSON object containing user preference default={} Optional attributes: - avatar (string) : avatar of user. It is an image encoded in base 64 - password (string) : user password if local authentication is used Attributes generated by the backend: - key (uuid) : API key to authenticate this user (deprecated) User Manipulation # User methods # HTTP Method URI Action GET /api/logout Logout POST /api/login User login GET /api/user/current Get current user POST /api/user/_search Find user POST /api/user Create a user GET /api/user/:userId Get a user DELETE /api/user/:userId Delete a user PATCH /api/user/:userId Update user details POST /api/user/:userId/password/set Set password POST /api/user/:userId/password/change Change password with-key (boolean) Create a User # A user can be created using the following URL: POST /api/user Required case attributes (cf. models) must be provided. This call returns attributes of the created user. This call is authenticated and requires admin role. Examples # Creation of a user: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/user -d '{ \"login\": \"georges\", \"name\": \"Georges Abitbol\", \"roles\": [\"read\", \"write\"], \"password\": \"La classe\" }' It returns: { \"createdBy\": \"myuser\", \"name\":\"Georges Abitbol\", \"roles\": [\"read\", \"write\" ], \"_id\": \"georges\", \"user\": \"myuser\", \"createdAt\": 1496561862924, \"status\": \"Ok\", \"id\": \"georges\", \"_type\": \"user\", \"has-key\":false } If external authentication is used (LDAP or AD) password field must not be provided.","title":"User"},{"location":"thehive/legacy/thehive3/api/user/#user","text":"","title":"User"},{"location":"thehive/legacy/thehive3/api/user/#model-definition","text":"Required attributes: - login / id (string) : login of the user - userName (text) : Full name of the user - roles (multi-userRole) : Array containing roles of the user ( read , write or admin ) - status (userStatus) : Ok or Locked default=Ok - preference (string) : JSON object containing user preference default={} Optional attributes: - avatar (string) : avatar of user. It is an image encoded in base 64 - password (string) : user password if local authentication is used Attributes generated by the backend: - key (uuid) : API key to authenticate this user (deprecated)","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/user/#user-manipulation","text":"","title":"User Manipulation"},{"location":"thehive/legacy/thehive3/api/user/#user-methods","text":"HTTP Method URI Action GET /api/logout Logout POST /api/login User login GET /api/user/current Get current user POST /api/user/_search Find user POST /api/user Create a user GET /api/user/:userId Get a user DELETE /api/user/:userId Delete a user PATCH /api/user/:userId Update user details POST /api/user/:userId/password/set Set password POST /api/user/:userId/password/change Change password with-key (boolean)","title":"User methods"},{"location":"thehive/legacy/thehive3/api/user/#create-a-user","text":"A user can be created using the following URL: POST /api/user Required case attributes (cf. models) must be provided. This call returns attributes of the created user. This call is authenticated and requires admin role.","title":"Create a User"},{"location":"thehive/legacy/thehive3/api/user/#examples","text":"Creation of a user: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/user -d '{ \"login\": \"georges\", \"name\": \"Georges Abitbol\", \"roles\": [\"read\", \"write\"], \"password\": \"La classe\" }' It returns: { \"createdBy\": \"myuser\", \"name\":\"Georges Abitbol\", \"roles\": [\"read\", \"write\" ], \"_id\": \"georges\", \"user\": \"myuser\", \"createdAt\": 1496561862924, \"status\": \"Ok\", \"id\": \"georges\", \"_type\": \"user\", \"has-key\":false } If external authentication is used (LDAP or AD) password field must not be provided.","title":"Examples"},{"location":"thehive/legacy/thehive3/api/connectors/","text":"Connectors API # TheHive offers an API to manipulate its various connectors Cortex MISP Metrics","title":"Connectors API"},{"location":"thehive/legacy/thehive3/api/connectors/#connectors-api","text":"TheHive offers an API to manipulate its various connectors Cortex MISP Metrics","title":"Connectors API"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/","text":"Cortex manipulation through TheHive # Cortex can be manipulated through TheHive with JSON over HTTP Job Analyzer","title":"Cortex manipulation through TheHive"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/#cortex-manipulation-through-thehive","text":"Cortex can be manipulated through TheHive with JSON over HTTP Job Analyzer","title":"Cortex manipulation through TheHive"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/","text":"Author : R\u00e9mi ALLAIN (rallain@cyberprotect.fr) - Cyberprotect, SDN International Analyzer # Model definition # Attributes: - id (string) : Analyzer id - name (string) : Analyzer name - version (string) : Analyzer version - description (text) : Analyzer description - dataTypeList (multi-string) : List of data type this analyzer can manage - cortexIds (string) : List of Cortex server id Analyzer manipulation # Analyzer methods # HTTP Method URI Action GET /api/connector/cortex/analyzer List all analyzers GET /api/connector/cortex/analyzer/:analyzerId Get details of an analyzer GET /api/connector/cortex/analyzer/type/:dataType List analyzers matching the dataType","title":"Analyzer"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#analyzer","text":"","title":"Analyzer"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#model-definition","text":"Attributes: - id (string) : Analyzer id - name (string) : Analyzer name - version (string) : Analyzer version - description (text) : Analyzer description - dataTypeList (multi-string) : List of data type this analyzer can manage - cortexIds (string) : List of Cortex server id","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#analyzer-manipulation","text":"","title":"Analyzer manipulation"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#analyzer-methods","text":"HTTP Method URI Action GET /api/connector/cortex/analyzer List all analyzers GET /api/connector/cortex/analyzer/:analyzerId Get details of an analyzer GET /api/connector/cortex/analyzer/type/:dataType List analyzers matching the dataType","title":"Analyzer methods"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/","text":"Job # Model definition # Required attributes: - analyzerId (string): identifier of the analyzer used by the job - status (enumeration): status of the job ( InProgress , Success , Failure ) default= InProgress - artifactId (string): identifier of the artifact to analyze - startDate (date): job start date Optional attributes: - endDate (date): job end date - report (string): raw content of the report sent back by the analyzer - cortexId (string): identifier of the cortex server - cortexJobId (string): identifier of the job in the cortex server Job manipulation # Job methods # HTTP Method URI Action POST /api/connector/cortex/job Create a new Cortex job GET /api/connector/cortex/job/:jobId Get a cortex job POST /api/connector/cortex/job/_search Search for cortex jobs Create a new Cortex job # Creating a new job can be done by performing the following query POST /api/connector/cortex/job Parameters: - cortexId : identifier of the Cortex server - artifactId : identifier of the artifact as found with an artifact search - analyzerId : name of the analyzer used by the job","title":"Job"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#job","text":"","title":"Job"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#model-definition","text":"Required attributes: - analyzerId (string): identifier of the analyzer used by the job - status (enumeration): status of the job ( InProgress , Success , Failure ) default= InProgress - artifactId (string): identifier of the artifact to analyze - startDate (date): job start date Optional attributes: - endDate (date): job end date - report (string): raw content of the report sent back by the analyzer - cortexId (string): identifier of the cortex server - cortexJobId (string): identifier of the job in the cortex server","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#job-manipulation","text":"","title":"Job manipulation"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#job-methods","text":"HTTP Method URI Action POST /api/connector/cortex/job Create a new Cortex job GET /api/connector/cortex/job/:jobId Get a cortex job POST /api/connector/cortex/job/_search Search for cortex jobs","title":"Job methods"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#create-a-new-cortex-job","text":"Creating a new job can be done by performing the following query POST /api/connector/cortex/job Parameters: - cortexId : identifier of the Cortex server - artifactId : identifier of the artifact as found with an artifact search - analyzerId : name of the analyzer used by the job","title":"Create a new Cortex job"},{"location":"thehive/legacy/thehive3/api/connectors/misp/","text":"MISP connector # MISP and TheHive can interact between each other in both ways: * TheHive is able to import events from a MISP instance as alerts and create cases from them * TheHive is able to export a case into MISP as an event and update it with the artifacts flagged as IOC as MISP attributes It is possible to use the API to control those behaviours. MISP imports # API methods # HTTP Method URI Action GET /api/connector/misp/_syncAlerts Synchronize from all MISP instances all MISP events published since the last synchronization GET /api/connector/misp/_syncAllAlerts Synchronize from all MISP instances all MISP published events since the beginning GET /api/connector/misp/_syncArtifacts Synchronize all artifacts from already imported alerts from all MISP instances MISP exports # API methods # HTTP Method URI Action POST /api/connector/misp/export/:caseId/:mispName Export a case to MISP Exporting a case to MISP # Exporting a case to MISP can be done by performing the following query POST /api/connector/misp/export/:caseId/:mispName With: * caseId: the elasticsearch id of the case * mispName: the name given to the MISP instance in TheHive configuration No parameters need to be sent in the query body. The response of this query will be a JSON table containing all artifacts sent as attributes in the MISP event.","title":"MISP connector"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#misp-connector","text":"MISP and TheHive can interact between each other in both ways: * TheHive is able to import events from a MISP instance as alerts and create cases from them * TheHive is able to export a case into MISP as an event and update it with the artifacts flagged as IOC as MISP attributes It is possible to use the API to control those behaviours.","title":"MISP connector"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#misp-imports","text":"","title":"MISP imports"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#api-methods","text":"HTTP Method URI Action GET /api/connector/misp/_syncAlerts Synchronize from all MISP instances all MISP events published since the last synchronization GET /api/connector/misp/_syncAllAlerts Synchronize from all MISP instances all MISP published events since the beginning GET /api/connector/misp/_syncArtifacts Synchronize all artifacts from already imported alerts from all MISP instances","title":"API methods"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#misp-exports","text":"","title":"MISP exports"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#api-methods_1","text":"HTTP Method URI Action POST /api/connector/misp/export/:caseId/:mispName Export a case to MISP","title":"API methods"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#exporting-a-case-to-misp","text":"Exporting a case to MISP can be done by performing the following query POST /api/connector/misp/export/:caseId/:mispName With: * caseId: the elasticsearch id of the case * mispName: the name given to the MISP instance in TheHive configuration No parameters need to be sent in the query body. The response of this query will be a JSON table containing all artifacts sent as attributes in the MISP event.","title":"Exporting a case to MISP"},{"location":"thehive/legacy/thehive3/installation/install-guide/","text":"Installation Guide # \u26a0\ufe0f Please read carrefully this documentation. Depending on you make a fresh installation or update an existing version, install version 3 or version 4, repository or packages names may vary. Current supported versions of TheHive are: - Version 3.5.0 and later that supports only Elasticsearch 7.x. - Version 4.0 and later. Instruction to install TheHive supporting Elasticsearch 6.x (EoL in Nov. 2020) are still detailled in this documentation. Before installing TheHive, you need to choose the installation option which suits your environment as described below. Once you have a chosen an option and installed the software, read the Configuration Guide . We also advise reading the Administration Guide . Table of Contents # Installation Options RPM DEB Docker Binary Build it Yourself Elasticsearch Installation System Package Start the Service Elasticsearch inside a Docker Installation Options # TheHive is available as: an RPM package a DEB package a Docker image a binary package In addition, TheHive can be also be built from the source code . RPM # RPM packages are published on a our RPM repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C Run the following command to import the GPG key : sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY Release versions # The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then you will able to install TheHive 3.5.0+ the package using yum : yum install thehive or install TheHive 4.0.0+ : yum install thehive4 Stable versions (or legacy versions) # The Stable repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x**, but version 6.x. Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/stable/noarch gpgcheck = 1 Then you will able to install TheHive 3.4.4 package using yum : yum install thehive Following beta versions # To follow beta versions of TheHive, use the following setup: And setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then you will able to install beta version of TheHive 3.x package using yum : yum install thehive or install beta version of TheHive 4.x : yum install thehive4 \u26a0\ufe0f We do not recommend that configuration for production servers Once the package is installed, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide . DEB # Debian packages are published on a our DEB packages repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C Release versions # The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup apt configuration with the release repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.5.0+ the package using apt command: apt install thehive or install TheHive 4.0.0+ : apt install thehive4 Stable versions (or legacy versions) # The main repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x , but version 6.x. Setup apt configuration with the main repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org stable main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.4.4 package using apt command: apt install thehive Beta versions # To follow beta versions of TheHive, use the following commands: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive \u26a0\ufe0f We do not recommend that configuration for production servers Docker # To use the Docker image, you must use Docker (courtesy of Captain Obvious). TheHive requires Elasticsearch to run. You can use docker-compose to start them together in Docker or install and configure Elasticsearch manually. Use Docker-compose # Docker-compose can start multiple dockers and link them together. The following docker-compose.yml file starts Elasticsearch and TheHive: version: \"2\" services: elasticsearch: image: elasticsearch:7.9.1 environment: - http.host=0.0.0.0 - discovery.type=single-node ulimits: nofile: soft: 65536 hard: 65536 cortex: image: thehiveproject/cortex:3.1.0-1 depends_on: - elasticsearch ports: - \"0.0.0.0:9001:9001\" thehive: image: thehiveproject/thehive:3.5.0-1 depends_on: - elasticsearch - cortex ports: - \"0.0.0.0:9000:9000\" command: --cortex-port 9001 Put this file in an empty folder and run docker-compose up . TheHive is exposed on 9000/tcp port and Cortex on 9001/tcp. These ports can be changed by modifying the docker-compose file. You can specify a custom TheHive configuration file ( application.conf ) by adding the following lines in the thehive section of your docker-compose file: volumes: - /path/to/application.conf:/etc/thehive/application.conf To take effect, be sure that: - '/path/to/application.conf' is readable for the user who runs the docker daemon (typically 644) - you specified command: --no-config in your docker-compose.yml file You should define where the data (i.e. the Elasticsearch database) will be located on your operating system by adding the following lines in the elasticsearch section of your docker-compose file: volumes: - /path/to/data:/usr/share/elasticsearch/data Running ElasticSearch in production mode requires a minimum vm.max_map_count of 262144. ElasticSearch documentation provides instructions on how to query and change this value. If you want to make Cortex be available on TheHive, you must create an account on Cortex, define an API key for it and provide that key to TheHive container using parameter --cortex-key or environment TH_CORTEX_KEY . Manual Installation of Elasticsearch # Elasticsearch can be installed on the same server as TheHive or on a different one. You can then configure TheHive according to the documentation and run TheHive docker as follow: docker run --volume /path/to/thehive/application.conf:/etc/thehive/application.conf thehiveproject/thehive:latest --no-config You can add the --publish docker option to expose TheHive HTTP service. Customize the Docker Image # By default, the TheHive Docker image has minimal configuration: - choose a random secret ( play.http.secret.key ) - search for the Elasticsearch instance (host named elasticsearch ) and add it to configuration - search for a Cortex instance (host named cortex ) and add it to configuration This behavior can be disabled by adding --no-config to the Docker command line: docker run thehiveproject/thehive:latest --no-config Or by adding the line command: --no-config in the thehive section of docker-compose file. It is possible to start database migration at startup with the parameter --auto-migration . If the initial administrator doesn't exist yet, you can request its creation with --create-admin followed by the user login and its password. You can also create a normal user with --create-user followed by the user login and its roles and its password. The image accepts more options. All options are available using environment variables. For boolean variable, 1 means true and other value means false. For multivalued variables, values are separated by coma. This is possible only with --create-admin . Option Env variable Description --no-config TH_NO_CONFIG Do not try to configure TheHive (add the secret and Elasticsearch) --no-config-secret TH_NO_CONFIG_SECRET Do not add the random secret to the configuration --secret <secret> TH_SECRET Cryptographic secret needed to secure sessions --show-secret TH_SHOW_SECRET Show the generated secret --no-config-es TH_NO_CONFIG_ES Do not add the Elasticsearch hosts to configuration --es-uri <uri> TH_CONFIG_ES Use this string to configure elasticsearch hosts (format: http(s)://host:port,host:port(/prefix)?querystring) --es-hostname <host> TH_ES_HOSTNAME Resolve this hostname to find Elasticsearch instances --no-config-cortex TH_NO_CONFIG_CORTEX Do not add Cortex configuration --cortex-proto <proto> TH_CORTEX_PROTO Define the protocol to connect to Cortex (default: http ) --cortex-port <port> TH_CORTEX_PORT Define the port to connect to Cortex (default: 9001 ) --cortex-url <url> TH_CORTEX_URL Add the Cortex connection --cortex-hostname <host> TH_CORTEX_HOSTNAME Resolve this hostname to find the Cortex instance --cortex-key <key> TH_CORTEX_KEY Define Cortex key --auto-migration TH_AUTO_MIGRATION Migrate the database, if needed --create-admin <user> <password TH_CREATE_ADMIN_LOGIN TH_CREATE_ADMIN_PASSWORD Create the first admin user, if not exist yet --create-user <user> <role> <password> TH_CREATE_USER_LOGIN TH_CREATE_USER_ROLE TH_CREATE_USER_PASSWORD Create a user, only in conjunction with admin creation Note : please remember that you must install and configure Elasticsearch . What to Do Next? # Once the Docker image is up and running, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide . Pre-release Versions # If you would like to use pre-release, beta versions of our Docker images and help us find bugs to the benefit of the whole community, please use thehiveproject/thehive:version-RCx . For example thehiveproject/thehive:3.1.0-RC1 . Binary # The following section contains the instructions to manually install TheHive using binaries on Ubuntu 20.04 LTS . 1. Minimal Ubuntu Installation # Install a minimal Ubuntu 20.04 system with the following software: Java runtime environment 1.8+ (JRE) Elasticsearch 7.x Make sure your system is up-to-date: sudo apt-get update sudo apt-get upgrade 2. Install a Java Virtual Machine # You can install either Oracle Java or OpenJDK. The latter is recommended. sudo apt-get install openjdk-11-jre-headless 3. Install Elasticsearch # To install Elasticsearch, please read the Elasticsearch Installation section below. 4. Install TheHive # Binary packages can be downloaded from Bintray . The latest version is called thehive-latest.zip . Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip ln -s thehive-x.x.x thehive Note : if you would like to use pre-release, beta versions of and help us find bugs to the benefit of the whole community, please download https://download.thehive-project.org/thehive-version-RCx.zip . For example https://download.thehive-project.org/thehive-3.5.0-RC1-1.zip . 5. First start # It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: sudo addgroup thehive sudo adduser --system thehive sudo cp /opt/thehive/package/thehive.service /usr/lib/systemd/system sudo chown -R thehive:thehive /opt/thehive sudo chgrp thehive /etc/thehive/application.conf sudo chmod 640 /etc/thehive/application.conf sudo systemctl enable thehive sudo service thehive start The only required parameter in order to start TheHive is the key of the server ( play.http.secret.key ). This key is used to authenticate cookies that contain data. If TheHive runs in cluster mode, all instances must share the same key. You can generate the minimal configuration with the following commands (they assume that you have created a dedicated user for TheHive, named thehive ): sudo mkdir /etc/thehive ( cat << _EOF_ # Secret key # ~~~~~ # The secret key is used to secure cryptographics functions. # If you deploy your application to several instances be sure to use the same key! play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ ) | sudo tee -a /etc/thehive/application.conf Now you can start TheHive. To do so, change your current directory to the TheHive installation directory ( /opt/thehive in this guide), then execute: bin/thehive -Dconfig.file = /etc/thehive/application.conf Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . Please note that the service may take some time to start. The first time you connect you will have to create the database schema. Click \"Migrate database\" to create the DB schema. Once done, you should be redirected to the page for creating the administrator's account. Once created, you should be redirected to the login page. Warning : at this stage, if you missed the creation of the admin account, you will not be able to do it unless you delete TheHive's index from Elasticsearch. In the case you made a mistake, first find out what is the current index of TheHive by running the following command on a host where the Elasticsearch DB used by TheHive is located: $ curl http://127.0.0.1:9200/_cat/indices?v The indexes that TheHive uses always start with the_hive_ following by a number. Let's assume that the output of the command is: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open cortex_1 PC_pLFGBS5G2TNQYr4ajgw 5 1 609 6 2 .1mb 2 .1mb yellow open the_hive_13 ft7GGTfhTr-4lSzZw5r1DQ 5 1 180131 3 51 .3mb 51 .3mb The index used by TheHive is the_hive_13 . To delete it, run the following command: $ curl -X DELETE http://127.0.0.1:9200/the_hive_13 Then reload the page or restart TheHive. 6. Update # To update TheHive from binaries, just stop the service, download the latest package, rebuild the link /opt/thehive and restart the service. service thehive stop cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip rm /opt/thehive && ln -s thehive-x.x.x thehive chown -R thehive:thehive /opt/thehive /opt/thehive-x.x.x service thehive start 7. Configuration # To configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide . Build it Yourself # The following section contains a step-by-step guide to build TheHive from its sources. 1. Pre-requisites # The following software are required to download and build TheHive: Java Development Kit 11 (JDK) git: use the system package or download it Node.js with its package manager (NPM) Grunt: after installing Node.js, run sudo npm install -g grunt-cli Bower: after installing Node.js, run sudo npm install -g bower Elasticsearch 5.6 2. Build # To install the requirements and build TheHive from sources, please follow the instructions below depending on your operating system. 2.1. CentOS/RHEL # Packages sudo yum -y install git bzip2 Installation of OpenJDK sudo yum -y install java-11-openjdk-devel Installation of Node.js Install the EPEL repository. You should have the extras repository enabled, then: sudo yum -y install epel-release Then, you can install Node.js, Grunt, and Bower: sudo yum -y install nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below. 2.2. Ubuntu # Packages sudo apt-get install git wget Installation of Oracle JDK sudo apt install openjdk-11-jdk-headless Installation of Node.js, Grunt and Bower sudo apt-get install curl curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below. 2.3. TheHive # Download The Source git clone https://github.com/TheHive-Project/TheHive.git Build the Project cd TheHive ./sbt clean stage This operation may take some time to complete as it will download all dependencies then build the back-end. This command cleans previous build files and creates an autonomous package in the target/universal/stage directory. This packages contains TheHive binaries with required libraries ( /lib ), configuration files ( /conf ) and startup scripts ( /bin ). Binaries are built and stored in TheHive/target/universal/stage/ . You can install them in /opt/thehive for example. sudo cp -r TheHive/target/universal/stage /opt/thehive Configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide . 2.4 Configure and Start Elasticsearch # Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive thread_pool.search.queue_size: 100000 Start the service: service elasticsearch restart 3. First start # Follow the first start section of the binary installation method above to start using TheHive. 4. Build the Front-end Only # Building the back-end builds also the front-end, so you don't need to build it separately. This section is useful only for troubleshooting or for installing the front-end on a reverse proxy. Go to the front-end directory: cd TheHive/ui Install Node.js libraries, which are required by this step, bower libraries (JavaScript libraries downloaded by the browser). Then build the front-end : npm install bower install grunt build This step generates static files (HTML, JavaScript and related resources) in the dist directory. They can be readily imported on a HTTP server. Elasticsearch Installation # If, for some reason, you need to install Elasticsearch, it can be installed using a system package or a Docker image. Version 5.X must be used. From version 6, Elasticsearch drops mapping type . System Package # Install the Elasticsearch package provided by Elastic Debian, Ubuntu # # PGP key installation sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key D88E42B4 # Alternative PGP key installation # wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - # Debian repository configuration echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list # Install https support for apt sudo apt install apt-transport-https # Elasticsearch installation sudo apt update && sudo apt install elasticsearch The Debian package does not start up the service by default, to prevent the instance from accidentally joining a cluster, without being configured appropriately. CentOS, RedHat, OpenSuSE # # PGP key installation sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch Create the file elasticsearch.repo in /etc/yum.repos.d/ for RedHat and CentOS, or in /etc/zypp/repos.d/ for OpenSuSE distributions, and add the following lines: [elasticsearch-5.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md Then, you can use the following command: # On CentOS and older Red Hat based distributions. sudo yum install elasticsearch # On Fedora and other newer Red Hat distributions. sudo dnf install elasticsearch # On OpenSUSE based distributions. sudo zypper install elasticsearch If you prefer using Elasticsearch inside a docker, see Elasticsearch inside a Docker . Configuration # It is highly recommended to avoid exposing this service to an untrusted zone. If Elasticsearch and TheHive run on the same host (and not in a docker), edit /etc/elasticsearch/elasticsearch.yml and set network.host parameter with 127.0.0.1 . TheHive use dynamic scripts to make partial updates. Hence, they must be activated using script.inline: true . The cluster name must also be set ( hive for example). Threadpool queue size must be set with a high value ( 100000 ). The default size will get the queue easily overloaded. Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 cluster.name: hive thread_pool.search.queue_size: 100000 Start the Service # Now that Elasticsearch is configured, start it as a service and check whether it's running: sudo systemctl enable elasticsearch.service sudo systemctl start elasticsearch.service sudo systemctl status elasticsearch.service The status should be active (running) . If it's not running, you can check for the reason in the logs: sudo journalctl -u elasticsearch.service Note that by default, the database is stored in /var/lib/elasticsearch and the logs in /var/log/elasticsearch Elasticsearch inside a Docker # You can also start Elasticsearch inside a docker. Use the following command and do not forget to specify the absolute path for persistent data on your host : docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:7.9.1","title":"Installation Guide"},{"location":"thehive/legacy/thehive3/installation/install-guide/#installation-guide","text":"\u26a0\ufe0f Please read carrefully this documentation. Depending on you make a fresh installation or update an existing version, install version 3 or version 4, repository or packages names may vary. Current supported versions of TheHive are: - Version 3.5.0 and later that supports only Elasticsearch 7.x. - Version 4.0 and later. Instruction to install TheHive supporting Elasticsearch 6.x (EoL in Nov. 2020) are still detailled in this documentation. Before installing TheHive, you need to choose the installation option which suits your environment as described below. Once you have a chosen an option and installed the software, read the Configuration Guide . We also advise reading the Administration Guide .","title":"Installation Guide"},{"location":"thehive/legacy/thehive3/installation/install-guide/#table-of-contents","text":"Installation Options RPM DEB Docker Binary Build it Yourself Elasticsearch Installation System Package Start the Service Elasticsearch inside a Docker","title":"Table of Contents"},{"location":"thehive/legacy/thehive3/installation/install-guide/#installation-options","text":"TheHive is available as: an RPM package a DEB package a Docker image a binary package In addition, TheHive can be also be built from the source code .","title":"Installation Options"},{"location":"thehive/legacy/thehive3/installation/install-guide/#rpm","text":"RPM packages are published on a our RPM repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C Run the following command to import the GPG key : sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY","title":"RPM"},{"location":"thehive/legacy/thehive3/installation/install-guide/#release-versions","text":"The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then you will able to install TheHive 3.5.0+ the package using yum : yum install thehive or install TheHive 4.0.0+ : yum install thehive4","title":"Release versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#stable-versions-or-legacy-versions","text":"The Stable repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x**, but version 6.x. Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/stable/noarch gpgcheck = 1 Then you will able to install TheHive 3.4.4 package using yum : yum install thehive","title":"Stable versions (or legacy versions)"},{"location":"thehive/legacy/thehive3/installation/install-guide/#following-beta-versions","text":"To follow beta versions of TheHive, use the following setup: And setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then you will able to install beta version of TheHive 3.x package using yum : yum install thehive or install beta version of TheHive 4.x : yum install thehive4 \u26a0\ufe0f We do not recommend that configuration for production servers Once the package is installed, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"Following beta versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#deb","text":"Debian packages are published on a our DEB packages repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C","title":"DEB"},{"location":"thehive/legacy/thehive3/installation/install-guide/#release-versions_1","text":"The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup apt configuration with the release repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.5.0+ the package using apt command: apt install thehive or install TheHive 4.0.0+ : apt install thehive4","title":"Release versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#stable-versions-or-legacy-versions_1","text":"The main repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x , but version 6.x. Setup apt configuration with the main repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org stable main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.4.4 package using apt command: apt install thehive","title":"Stable versions (or legacy versions)"},{"location":"thehive/legacy/thehive3/installation/install-guide/#beta-versions","text":"To follow beta versions of TheHive, use the following commands: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive \u26a0\ufe0f We do not recommend that configuration for production servers","title":"Beta versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#docker","text":"To use the Docker image, you must use Docker (courtesy of Captain Obvious). TheHive requires Elasticsearch to run. You can use docker-compose to start them together in Docker or install and configure Elasticsearch manually.","title":"Docker"},{"location":"thehive/legacy/thehive3/installation/install-guide/#use-docker-compose","text":"Docker-compose can start multiple dockers and link them together. The following docker-compose.yml file starts Elasticsearch and TheHive: version: \"2\" services: elasticsearch: image: elasticsearch:7.9.1 environment: - http.host=0.0.0.0 - discovery.type=single-node ulimits: nofile: soft: 65536 hard: 65536 cortex: image: thehiveproject/cortex:3.1.0-1 depends_on: - elasticsearch ports: - \"0.0.0.0:9001:9001\" thehive: image: thehiveproject/thehive:3.5.0-1 depends_on: - elasticsearch - cortex ports: - \"0.0.0.0:9000:9000\" command: --cortex-port 9001 Put this file in an empty folder and run docker-compose up . TheHive is exposed on 9000/tcp port and Cortex on 9001/tcp. These ports can be changed by modifying the docker-compose file. You can specify a custom TheHive configuration file ( application.conf ) by adding the following lines in the thehive section of your docker-compose file: volumes: - /path/to/application.conf:/etc/thehive/application.conf To take effect, be sure that: - '/path/to/application.conf' is readable for the user who runs the docker daemon (typically 644) - you specified command: --no-config in your docker-compose.yml file You should define where the data (i.e. the Elasticsearch database) will be located on your operating system by adding the following lines in the elasticsearch section of your docker-compose file: volumes: - /path/to/data:/usr/share/elasticsearch/data Running ElasticSearch in production mode requires a minimum vm.max_map_count of 262144. ElasticSearch documentation provides instructions on how to query and change this value. If you want to make Cortex be available on TheHive, you must create an account on Cortex, define an API key for it and provide that key to TheHive container using parameter --cortex-key or environment TH_CORTEX_KEY .","title":"Use Docker-compose"},{"location":"thehive/legacy/thehive3/installation/install-guide/#manual-installation-of-elasticsearch","text":"Elasticsearch can be installed on the same server as TheHive or on a different one. You can then configure TheHive according to the documentation and run TheHive docker as follow: docker run --volume /path/to/thehive/application.conf:/etc/thehive/application.conf thehiveproject/thehive:latest --no-config You can add the --publish docker option to expose TheHive HTTP service.","title":"Manual Installation of Elasticsearch"},{"location":"thehive/legacy/thehive3/installation/install-guide/#customize-the-docker-image","text":"By default, the TheHive Docker image has minimal configuration: - choose a random secret ( play.http.secret.key ) - search for the Elasticsearch instance (host named elasticsearch ) and add it to configuration - search for a Cortex instance (host named cortex ) and add it to configuration This behavior can be disabled by adding --no-config to the Docker command line: docker run thehiveproject/thehive:latest --no-config Or by adding the line command: --no-config in the thehive section of docker-compose file. It is possible to start database migration at startup with the parameter --auto-migration . If the initial administrator doesn't exist yet, you can request its creation with --create-admin followed by the user login and its password. You can also create a normal user with --create-user followed by the user login and its roles and its password. The image accepts more options. All options are available using environment variables. For boolean variable, 1 means true and other value means false. For multivalued variables, values are separated by coma. This is possible only with --create-admin . Option Env variable Description --no-config TH_NO_CONFIG Do not try to configure TheHive (add the secret and Elasticsearch) --no-config-secret TH_NO_CONFIG_SECRET Do not add the random secret to the configuration --secret <secret> TH_SECRET Cryptographic secret needed to secure sessions --show-secret TH_SHOW_SECRET Show the generated secret --no-config-es TH_NO_CONFIG_ES Do not add the Elasticsearch hosts to configuration --es-uri <uri> TH_CONFIG_ES Use this string to configure elasticsearch hosts (format: http(s)://host:port,host:port(/prefix)?querystring) --es-hostname <host> TH_ES_HOSTNAME Resolve this hostname to find Elasticsearch instances --no-config-cortex TH_NO_CONFIG_CORTEX Do not add Cortex configuration --cortex-proto <proto> TH_CORTEX_PROTO Define the protocol to connect to Cortex (default: http ) --cortex-port <port> TH_CORTEX_PORT Define the port to connect to Cortex (default: 9001 ) --cortex-url <url> TH_CORTEX_URL Add the Cortex connection --cortex-hostname <host> TH_CORTEX_HOSTNAME Resolve this hostname to find the Cortex instance --cortex-key <key> TH_CORTEX_KEY Define Cortex key --auto-migration TH_AUTO_MIGRATION Migrate the database, if needed --create-admin <user> <password TH_CREATE_ADMIN_LOGIN TH_CREATE_ADMIN_PASSWORD Create the first admin user, if not exist yet --create-user <user> <role> <password> TH_CREATE_USER_LOGIN TH_CREATE_USER_ROLE TH_CREATE_USER_PASSWORD Create a user, only in conjunction with admin creation Note : please remember that you must install and configure Elasticsearch .","title":"Customize the Docker Image"},{"location":"thehive/legacy/thehive3/installation/install-guide/#what-to-do-next","text":"Once the Docker image is up and running, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"What to Do Next?"},{"location":"thehive/legacy/thehive3/installation/install-guide/#pre-release-versions","text":"If you would like to use pre-release, beta versions of our Docker images and help us find bugs to the benefit of the whole community, please use thehiveproject/thehive:version-RCx . For example thehiveproject/thehive:3.1.0-RC1 .","title":"Pre-release Versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#binary","text":"The following section contains the instructions to manually install TheHive using binaries on Ubuntu 20.04 LTS .","title":"Binary"},{"location":"thehive/legacy/thehive3/installation/install-guide/#1-minimal-ubuntu-installation","text":"Install a minimal Ubuntu 20.04 system with the following software: Java runtime environment 1.8+ (JRE) Elasticsearch 7.x Make sure your system is up-to-date: sudo apt-get update sudo apt-get upgrade","title":"1. Minimal Ubuntu Installation"},{"location":"thehive/legacy/thehive3/installation/install-guide/#2-install-a-java-virtual-machine","text":"You can install either Oracle Java or OpenJDK. The latter is recommended. sudo apt-get install openjdk-11-jre-headless","title":"2. Install a Java Virtual Machine"},{"location":"thehive/legacy/thehive3/installation/install-guide/#3-install-elasticsearch","text":"To install Elasticsearch, please read the Elasticsearch Installation section below.","title":"3. Install Elasticsearch"},{"location":"thehive/legacy/thehive3/installation/install-guide/#4-install-thehive","text":"Binary packages can be downloaded from Bintray . The latest version is called thehive-latest.zip . Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip ln -s thehive-x.x.x thehive Note : if you would like to use pre-release, beta versions of and help us find bugs to the benefit of the whole community, please download https://download.thehive-project.org/thehive-version-RCx.zip . For example https://download.thehive-project.org/thehive-3.5.0-RC1-1.zip .","title":"4. Install TheHive"},{"location":"thehive/legacy/thehive3/installation/install-guide/#5-first-start","text":"It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: sudo addgroup thehive sudo adduser --system thehive sudo cp /opt/thehive/package/thehive.service /usr/lib/systemd/system sudo chown -R thehive:thehive /opt/thehive sudo chgrp thehive /etc/thehive/application.conf sudo chmod 640 /etc/thehive/application.conf sudo systemctl enable thehive sudo service thehive start The only required parameter in order to start TheHive is the key of the server ( play.http.secret.key ). This key is used to authenticate cookies that contain data. If TheHive runs in cluster mode, all instances must share the same key. You can generate the minimal configuration with the following commands (they assume that you have created a dedicated user for TheHive, named thehive ): sudo mkdir /etc/thehive ( cat << _EOF_ # Secret key # ~~~~~ # The secret key is used to secure cryptographics functions. # If you deploy your application to several instances be sure to use the same key! play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ ) | sudo tee -a /etc/thehive/application.conf Now you can start TheHive. To do so, change your current directory to the TheHive installation directory ( /opt/thehive in this guide), then execute: bin/thehive -Dconfig.file = /etc/thehive/application.conf Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . Please note that the service may take some time to start. The first time you connect you will have to create the database schema. Click \"Migrate database\" to create the DB schema. Once done, you should be redirected to the page for creating the administrator's account. Once created, you should be redirected to the login page. Warning : at this stage, if you missed the creation of the admin account, you will not be able to do it unless you delete TheHive's index from Elasticsearch. In the case you made a mistake, first find out what is the current index of TheHive by running the following command on a host where the Elasticsearch DB used by TheHive is located: $ curl http://127.0.0.1:9200/_cat/indices?v The indexes that TheHive uses always start with the_hive_ following by a number. Let's assume that the output of the command is: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open cortex_1 PC_pLFGBS5G2TNQYr4ajgw 5 1 609 6 2 .1mb 2 .1mb yellow open the_hive_13 ft7GGTfhTr-4lSzZw5r1DQ 5 1 180131 3 51 .3mb 51 .3mb The index used by TheHive is the_hive_13 . To delete it, run the following command: $ curl -X DELETE http://127.0.0.1:9200/the_hive_13 Then reload the page or restart TheHive.","title":"5. First start"},{"location":"thehive/legacy/thehive3/installation/install-guide/#6-update","text":"To update TheHive from binaries, just stop the service, download the latest package, rebuild the link /opt/thehive and restart the service. service thehive stop cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip rm /opt/thehive && ln -s thehive-x.x.x thehive chown -R thehive:thehive /opt/thehive /opt/thehive-x.x.x service thehive start","title":"6. Update"},{"location":"thehive/legacy/thehive3/installation/install-guide/#7-configuration","text":"To configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"7. Configuration"},{"location":"thehive/legacy/thehive3/installation/install-guide/#build-it-yourself","text":"The following section contains a step-by-step guide to build TheHive from its sources.","title":"Build it Yourself"},{"location":"thehive/legacy/thehive3/installation/install-guide/#1-pre-requisites","text":"The following software are required to download and build TheHive: Java Development Kit 11 (JDK) git: use the system package or download it Node.js with its package manager (NPM) Grunt: after installing Node.js, run sudo npm install -g grunt-cli Bower: after installing Node.js, run sudo npm install -g bower Elasticsearch 5.6","title":"1. Pre-requisites"},{"location":"thehive/legacy/thehive3/installation/install-guide/#2-build","text":"To install the requirements and build TheHive from sources, please follow the instructions below depending on your operating system.","title":"2. Build"},{"location":"thehive/legacy/thehive3/installation/install-guide/#21-centosrhel","text":"Packages sudo yum -y install git bzip2 Installation of OpenJDK sudo yum -y install java-11-openjdk-devel Installation of Node.js Install the EPEL repository. You should have the extras repository enabled, then: sudo yum -y install epel-release Then, you can install Node.js, Grunt, and Bower: sudo yum -y install nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below.","title":"2.1. CentOS/RHEL"},{"location":"thehive/legacy/thehive3/installation/install-guide/#22-ubuntu","text":"Packages sudo apt-get install git wget Installation of Oracle JDK sudo apt install openjdk-11-jdk-headless Installation of Node.js, Grunt and Bower sudo apt-get install curl curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below.","title":"2.2. Ubuntu"},{"location":"thehive/legacy/thehive3/installation/install-guide/#23-thehive","text":"Download The Source git clone https://github.com/TheHive-Project/TheHive.git Build the Project cd TheHive ./sbt clean stage This operation may take some time to complete as it will download all dependencies then build the back-end. This command cleans previous build files and creates an autonomous package in the target/universal/stage directory. This packages contains TheHive binaries with required libraries ( /lib ), configuration files ( /conf ) and startup scripts ( /bin ). Binaries are built and stored in TheHive/target/universal/stage/ . You can install them in /opt/thehive for example. sudo cp -r TheHive/target/universal/stage /opt/thehive Configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"2.3. TheHive"},{"location":"thehive/legacy/thehive3/installation/install-guide/#24-configure-and-start-elasticsearch","text":"Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive thread_pool.search.queue_size: 100000 Start the service: service elasticsearch restart","title":"2.4 Configure and Start Elasticsearch"},{"location":"thehive/legacy/thehive3/installation/install-guide/#3-first-start","text":"Follow the first start section of the binary installation method above to start using TheHive.","title":"3. First start"},{"location":"thehive/legacy/thehive3/installation/install-guide/#4-build-the-front-end-only","text":"Building the back-end builds also the front-end, so you don't need to build it separately. This section is useful only for troubleshooting or for installing the front-end on a reverse proxy. Go to the front-end directory: cd TheHive/ui Install Node.js libraries, which are required by this step, bower libraries (JavaScript libraries downloaded by the browser). Then build the front-end : npm install bower install grunt build This step generates static files (HTML, JavaScript and related resources) in the dist directory. They can be readily imported on a HTTP server.","title":"4. Build the Front-end Only"},{"location":"thehive/legacy/thehive3/installation/install-guide/#elasticsearch-installation","text":"If, for some reason, you need to install Elasticsearch, it can be installed using a system package or a Docker image. Version 5.X must be used. From version 6, Elasticsearch drops mapping type .","title":"Elasticsearch Installation"},{"location":"thehive/legacy/thehive3/installation/install-guide/#system-package","text":"Install the Elasticsearch package provided by Elastic","title":"System Package"},{"location":"thehive/legacy/thehive3/installation/install-guide/#debian-ubuntu","text":"# PGP key installation sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key D88E42B4 # Alternative PGP key installation # wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - # Debian repository configuration echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list # Install https support for apt sudo apt install apt-transport-https # Elasticsearch installation sudo apt update && sudo apt install elasticsearch The Debian package does not start up the service by default, to prevent the instance from accidentally joining a cluster, without being configured appropriately.","title":"Debian, Ubuntu"},{"location":"thehive/legacy/thehive3/installation/install-guide/#centos-redhat-opensuse","text":"# PGP key installation sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch Create the file elasticsearch.repo in /etc/yum.repos.d/ for RedHat and CentOS, or in /etc/zypp/repos.d/ for OpenSuSE distributions, and add the following lines: [elasticsearch-5.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md Then, you can use the following command: # On CentOS and older Red Hat based distributions. sudo yum install elasticsearch # On Fedora and other newer Red Hat distributions. sudo dnf install elasticsearch # On OpenSUSE based distributions. sudo zypper install elasticsearch If you prefer using Elasticsearch inside a docker, see Elasticsearch inside a Docker .","title":"CentOS, RedHat, OpenSuSE"},{"location":"thehive/legacy/thehive3/installation/install-guide/#configuration","text":"It is highly recommended to avoid exposing this service to an untrusted zone. If Elasticsearch and TheHive run on the same host (and not in a docker), edit /etc/elasticsearch/elasticsearch.yml and set network.host parameter with 127.0.0.1 . TheHive use dynamic scripts to make partial updates. Hence, they must be activated using script.inline: true . The cluster name must also be set ( hive for example). Threadpool queue size must be set with a high value ( 100000 ). The default size will get the queue easily overloaded. Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 cluster.name: hive thread_pool.search.queue_size: 100000","title":"Configuration"},{"location":"thehive/legacy/thehive3/installation/install-guide/#start-the-service","text":"Now that Elasticsearch is configured, start it as a service and check whether it's running: sudo systemctl enable elasticsearch.service sudo systemctl start elasticsearch.service sudo systemctl status elasticsearch.service The status should be active (running) . If it's not running, you can check for the reason in the logs: sudo journalctl -u elasticsearch.service Note that by default, the database is stored in /var/lib/elasticsearch and the logs in /var/log/elasticsearch","title":"Start the Service"},{"location":"thehive/legacy/thehive3/installation/install-guide/#elasticsearch-inside-a-docker","text":"You can also start Elasticsearch inside a docker. Use the following command and do not forget to specify the absolute path for persistent data on your host : docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:7.9.1","title":"Elasticsearch inside a Docker"},{"location":"thehive/operations/backup-restore/","text":"Backup and restore # This guide has only been tested on single node Cassandra server Note https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupRestore.html Overview # To be restored successfully, TheHive requires following data beeing saved: The database Files optionnally, the index. Cassandra # Pre requisites # To backup or export database from Cassandra, following information are required: Cassandra admin password keyspace used by thehive (default = thehive ). This can be checked in the application.conf configuration file, in the database configuration in storage , cql and keyspace attribute. Tip This information can be found in TheHive configuration: [..] db.janusgraph { storage { backend: cql hostname: [\"127.0.0.1\"] cql { cluster-name: thp keyspace: thehive } } [..] Backup # Following actions should be performed to backup the data successfully: Save the database schema Create a snapshot Save the data and the schema Save the database schema # This can be done with the following command: cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <SERVER_IP> -e \"DESCRIBE KEYSPACE <KEYSPACE>\" > schema.cql Create a snapshot and an archive # Considering that your keyspace is thehive and backup_name is the name of the snapshot, run the following commands: Before taking snapshots nodetool cleanup thehive Take a snapshot nodetool snapshot thehive -t backup_name Create and archive with the snapshot data: tar cjf backup.tbz /var/lib/cassandra/data/thehive/*/snapshots/backup_name/ Remove old snapshots (if necessary) nodetool -h localhost -p 7199 clearsnapshot -t <snapshotname> Example # Example of script to generate backups of TheHive keyspace #!/bin/bash ## Create a CQL file with the schema of the KEYSPACE ## and an tbz archive containing the snapshot ## Complete variables before running: ## KEYSPACE: Identify the right keyspace to save in cassandra ## SNAPSHOT: choose a name for the backup IP = 10 .1.1.1 SOURCE_KEYSPACE = thehive SNAPSHOT = thehive_20211124 SNAPSHOT_INDEX = 1 # Backup Cassandra nodetool cleanup ${ SOURCE_KEYSPACE } nodetool snapshot ${ SOURCE_KEYSPACE } -t ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } echo -n \"Cassandra admin password\" : read -s CASSANDRA_PASSWORD ## Save schema cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"DESCRIBE KEYSPACE ${ SOURCE_KEYSPACE } \" > schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Create archive if [[ $? == 0 ]] then tar cjf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } /*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } / fi Restore data # Pre requisites # Following data is required to restore TheHive database successfully: The database schema (example: schema.cql ) A backup of the database (example: backup.tbz ) Keyspace to restore does not exist in the database (or it will be overwritten) Restore # Restore keyspace cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"source 'schema.cql';\" Unarchive backup files: tar jxf /PATH/TO/backup.tbz -C /tmp/cassandra_backup And restore snapshots files: cd /var/lib/cassandra/data/thehive for I in ` ls /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE> ` ; do cp /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE>/ $I /snapshots/<BACKUP_NAME>/* /var/lib/cassandra/data/<KEYSPACE>/ $I / ; done Ensure Cassandra user keep ownership on the files: chown -R cassandra:cassandra /var/lib/cassandra/data/<KEYSPACE> Refresh tables for TABLE in ` ls /var/lib/cassandra/data/<KEYSPACE> ` do sstableloader -d <IP> /var/lib/cassandra/data/<KEYSPACE>/<TABLE> done Ensure no Commitlog file exist before restarting Cassandra service. ( /var/lib/cassandra/commitlog ) Example of script to restore TheHive keyspace in Cassandra #!/bin/bash ## Restore a KEYSPACE and its data from a CQL file with the schema of the ## KEYSPACE and an tbz archive containing the snapshot ## Complete variables before running: ## IP: IP of cassandra server ## TMP: choose a TMP folder !!! this folder will be removed if exists. ## SOURCE_KEYSPACE: KEYSPACE used in the backup ## TARGET_KEYSPACE: new KEYSPACE name ; use same name of SOURCE_KEYSPACE if no changes ## SNAPSHOT: choose a name for the backup ## SNAPSHOT_INDEX: index of the snapshot (1, 20210401 ...) IP = 10 .1.1.1 TMP = /tmp/cassandra_backup SOURCE_KEYSPACE = \"thehive\" TARGET_KEYSPACE = \"\" SNAPSHOT = \"thehive_20211124\" SNAPSHOT_INDEX = \"1\" ## Uncompress data in TMP folder rm -rf ${ TMP } && mkdir ${ TMP } tar jxf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz -C ${ TMP } ## Read Cassandra password echo -n \"Cassandra admin password: \" read -s CASSANDRA_PASSWORD ## Define new KEYSPACE NAME sed -i \"s/ ${ SOURCE_KEYSPACE } / ${ TARGET_KEYSPACE } /g\" schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore keyspace cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } cassandra --file schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore data for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do cp -r ${ TMP } /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } / ${ TABLE } -*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } /* /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / $TABLE -*/ done ## Change ownership chown -R cassandra:cassandra /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ## sstableloader for TABLE in ` ls /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ` do sstableloader -d ${ IP } /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / ${ TABLE } done Files # Backup # Wether you use local or distributed files system storage, copy the content of the folder/bucket. Restore # Restore the saved files into the destination folder/bucket that will be used by TheHive.","title":"Backup & restore"},{"location":"thehive/operations/backup-restore/#backup-and-restore","text":"This guide has only been tested on single node Cassandra server Note https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupRestore.html","title":"Backup and restore"},{"location":"thehive/operations/backup-restore/#overview","text":"To be restored successfully, TheHive requires following data beeing saved: The database Files optionnally, the index.","title":"Overview"},{"location":"thehive/operations/backup-restore/#cassandra","text":"","title":"Cassandra"},{"location":"thehive/operations/backup-restore/#pre-requisites","text":"To backup or export database from Cassandra, following information are required: Cassandra admin password keyspace used by thehive (default = thehive ). This can be checked in the application.conf configuration file, in the database configuration in storage , cql and keyspace attribute. Tip This information can be found in TheHive configuration: [..] db.janusgraph { storage { backend: cql hostname: [\"127.0.0.1\"] cql { cluster-name: thp keyspace: thehive } } [..]","title":"Pre requisites"},{"location":"thehive/operations/backup-restore/#backup","text":"Following actions should be performed to backup the data successfully: Save the database schema Create a snapshot Save the data and the schema","title":"Backup"},{"location":"thehive/operations/backup-restore/#save-the-database-schema","text":"This can be done with the following command: cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <SERVER_IP> -e \"DESCRIBE KEYSPACE <KEYSPACE>\" > schema.cql","title":"Save the database schema"},{"location":"thehive/operations/backup-restore/#create-a-snapshot-and-an-archive","text":"Considering that your keyspace is thehive and backup_name is the name of the snapshot, run the following commands: Before taking snapshots nodetool cleanup thehive Take a snapshot nodetool snapshot thehive -t backup_name Create and archive with the snapshot data: tar cjf backup.tbz /var/lib/cassandra/data/thehive/*/snapshots/backup_name/ Remove old snapshots (if necessary) nodetool -h localhost -p 7199 clearsnapshot -t <snapshotname>","title":"Create a snapshot and an archive"},{"location":"thehive/operations/backup-restore/#example","text":"Example of script to generate backups of TheHive keyspace #!/bin/bash ## Create a CQL file with the schema of the KEYSPACE ## and an tbz archive containing the snapshot ## Complete variables before running: ## KEYSPACE: Identify the right keyspace to save in cassandra ## SNAPSHOT: choose a name for the backup IP = 10 .1.1.1 SOURCE_KEYSPACE = thehive SNAPSHOT = thehive_20211124 SNAPSHOT_INDEX = 1 # Backup Cassandra nodetool cleanup ${ SOURCE_KEYSPACE } nodetool snapshot ${ SOURCE_KEYSPACE } -t ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } echo -n \"Cassandra admin password\" : read -s CASSANDRA_PASSWORD ## Save schema cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"DESCRIBE KEYSPACE ${ SOURCE_KEYSPACE } \" > schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Create archive if [[ $? == 0 ]] then tar cjf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } /*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } / fi","title":"Example"},{"location":"thehive/operations/backup-restore/#restore-data","text":"","title":"Restore data"},{"location":"thehive/operations/backup-restore/#pre-requisites_1","text":"Following data is required to restore TheHive database successfully: The database schema (example: schema.cql ) A backup of the database (example: backup.tbz ) Keyspace to restore does not exist in the database (or it will be overwritten)","title":"Pre requisites"},{"location":"thehive/operations/backup-restore/#restore","text":"Restore keyspace cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"source 'schema.cql';\" Unarchive backup files: tar jxf /PATH/TO/backup.tbz -C /tmp/cassandra_backup And restore snapshots files: cd /var/lib/cassandra/data/thehive for I in ` ls /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE> ` ; do cp /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE>/ $I /snapshots/<BACKUP_NAME>/* /var/lib/cassandra/data/<KEYSPACE>/ $I / ; done Ensure Cassandra user keep ownership on the files: chown -R cassandra:cassandra /var/lib/cassandra/data/<KEYSPACE> Refresh tables for TABLE in ` ls /var/lib/cassandra/data/<KEYSPACE> ` do sstableloader -d <IP> /var/lib/cassandra/data/<KEYSPACE>/<TABLE> done Ensure no Commitlog file exist before restarting Cassandra service. ( /var/lib/cassandra/commitlog ) Example of script to restore TheHive keyspace in Cassandra #!/bin/bash ## Restore a KEYSPACE and its data from a CQL file with the schema of the ## KEYSPACE and an tbz archive containing the snapshot ## Complete variables before running: ## IP: IP of cassandra server ## TMP: choose a TMP folder !!! this folder will be removed if exists. ## SOURCE_KEYSPACE: KEYSPACE used in the backup ## TARGET_KEYSPACE: new KEYSPACE name ; use same name of SOURCE_KEYSPACE if no changes ## SNAPSHOT: choose a name for the backup ## SNAPSHOT_INDEX: index of the snapshot (1, 20210401 ...) IP = 10 .1.1.1 TMP = /tmp/cassandra_backup SOURCE_KEYSPACE = \"thehive\" TARGET_KEYSPACE = \"\" SNAPSHOT = \"thehive_20211124\" SNAPSHOT_INDEX = \"1\" ## Uncompress data in TMP folder rm -rf ${ TMP } && mkdir ${ TMP } tar jxf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz -C ${ TMP } ## Read Cassandra password echo -n \"Cassandra admin password: \" read -s CASSANDRA_PASSWORD ## Define new KEYSPACE NAME sed -i \"s/ ${ SOURCE_KEYSPACE } / ${ TARGET_KEYSPACE } /g\" schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore keyspace cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } cassandra --file schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore data for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do cp -r ${ TMP } /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } / ${ TABLE } -*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } /* /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / $TABLE -*/ done ## Change ownership chown -R cassandra:cassandra /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ## sstableloader for TABLE in ` ls /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ` do sstableloader -d ${ IP } /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / ${ TABLE } done","title":"Restore"},{"location":"thehive/operations/backup-restore/#files","text":"","title":"Files"},{"location":"thehive/operations/backup-restore/#backup_1","text":"Wether you use local or distributed files system storage, copy the content of the folder/bucket.","title":"Backup"},{"location":"thehive/operations/backup-restore/#restore_1","text":"Restore the saved files into the destination folder/bucket that will be used by TheHive.","title":"Restore"},{"location":"thehive/operations/cassandra-security/","text":"Security in Apache Cassandra # References Internal authentication https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureInternalAuthenticationTOC.html Node to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLNodeToNode.html Client to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLClientToNode.html https://docs.janusgraph.org/basics/configuration-reference/#storagecqlssl Authentication with Cassandra # Create an account and grant permissions on keyspace CREATE ROLE thehive WITH PASSWORD = 'thehive1234' AND LOGIN = true ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO thehive ; Configure TheHive with the account Update /etc/thehive/application.conf accordingly: db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"xxx.xxx.xxx.xxx\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" cql { cluster-name: thp keyspace: thehive } } Cassandra node to node encryption # This document addresses communication between Cassandra servers, when a Cassandra cluster contains several nodes. server_encryption_options : internode_encryption : all keystore : /path/to/keystore.jks keystore_password : keystorepassword truststore : /path/to/truststore.jks truststore_password : truststorepassword # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] require_client_auth : false Cassandra dedicated port for SSL (optional) # Optionally, you can setup a dedicated port for SSL communication. Update /etc/cassandra/cassandra.yml configuration file on each node: native_transport_port_ssl : 9142 Note By doing so, all SSL communications will be done using this port. Without this parameter, SSL is setup on native_transport_port . (everything is explained in the cassandra.yaml configuration file). Client to node encryption # This guide explains how to secure connection between Cassandra server and Cassandra clients (TheHive). This document doesn\u2019t address communication between Cassandra servers, when a Cassandra cluster contains several nodes. Requirements # The setup requires a valid X509 certificate for the Cassandra service. It must have standard properties of server certificate: key usage: Digital Signature, Non Repudiation, Key Encipherment, Key Agreement Extended Key Usage: TLS Web Server Authentication Cert Type: SSL Server It also must have a \"Subject Alternative Name\" with the identifier (DNS name or/and IP address) of the Cassandra server seen by the client. The format of the certificate file is PKCS12 (file with extention p12). Then create a truststore containing the certificate authority used to generate the certificate for Cassandra. The truststore must be in Java format (JKS). If you CA file is ca.crt, you can generate the truststore file with the following command: keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks This command ask a password for file integrity checking. The command keytool is available in any JDK distribution. Configuring Cassandra # Locate the section client_encryption_options and set the following options: client_encryption_options : enabled : true # If enabled and optional is set to true encrypted and unencrypted connections are handled. optional : false keystore : /pat/to/keystore.jks keystore_password : keystorepassword require_client_auth : false # Set trustore and truststore_password if require_client_auth is true # truststore: conf/.truststore # truststore_password: cassandra # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] Then the service cassandra must be restarted. Configuring TheHive # db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"ip_node_1\", \"ip_node_2\", \"ip_node_3\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" port: 9142 # if alternative port has been set in Cassandra configuration cql { cluster-name: thp keyspace: thehive ssl { enabled: true truststore { location: \"/path/to/truststore.jks\" password: \"truststorepassword\" } } } } Then the service thehive must be restarted.","title":"Cassandra & security"},{"location":"thehive/operations/cassandra-security/#security-in-apache-cassandra","text":"References Internal authentication https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureInternalAuthenticationTOC.html Node to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLNodeToNode.html Client to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLClientToNode.html https://docs.janusgraph.org/basics/configuration-reference/#storagecqlssl","title":"Security in Apache Cassandra"},{"location":"thehive/operations/cassandra-security/#authentication-with-cassandra","text":"Create an account and grant permissions on keyspace CREATE ROLE thehive WITH PASSWORD = 'thehive1234' AND LOGIN = true ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO thehive ; Configure TheHive with the account Update /etc/thehive/application.conf accordingly: db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"xxx.xxx.xxx.xxx\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" cql { cluster-name: thp keyspace: thehive } }","title":"Authentication with Cassandra"},{"location":"thehive/operations/cassandra-security/#cassandra-node-to-node-encryption","text":"This document addresses communication between Cassandra servers, when a Cassandra cluster contains several nodes. server_encryption_options : internode_encryption : all keystore : /path/to/keystore.jks keystore_password : keystorepassword truststore : /path/to/truststore.jks truststore_password : truststorepassword # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] require_client_auth : false","title":"Cassandra node to node encryption"},{"location":"thehive/operations/cassandra-security/#cassandra-dedicated-port-for-ssl-optional","text":"Optionally, you can setup a dedicated port for SSL communication. Update /etc/cassandra/cassandra.yml configuration file on each node: native_transport_port_ssl : 9142 Note By doing so, all SSL communications will be done using this port. Without this parameter, SSL is setup on native_transport_port . (everything is explained in the cassandra.yaml configuration file).","title":"Cassandra dedicated port for SSL (optional)"},{"location":"thehive/operations/cassandra-security/#client-to-node-encryption","text":"This guide explains how to secure connection between Cassandra server and Cassandra clients (TheHive). This document doesn\u2019t address communication between Cassandra servers, when a Cassandra cluster contains several nodes.","title":"Client to node encryption"},{"location":"thehive/operations/cassandra-security/#requirements","text":"The setup requires a valid X509 certificate for the Cassandra service. It must have standard properties of server certificate: key usage: Digital Signature, Non Repudiation, Key Encipherment, Key Agreement Extended Key Usage: TLS Web Server Authentication Cert Type: SSL Server It also must have a \"Subject Alternative Name\" with the identifier (DNS name or/and IP address) of the Cassandra server seen by the client. The format of the certificate file is PKCS12 (file with extention p12). Then create a truststore containing the certificate authority used to generate the certificate for Cassandra. The truststore must be in Java format (JKS). If you CA file is ca.crt, you can generate the truststore file with the following command: keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks This command ask a password for file integrity checking. The command keytool is available in any JDK distribution.","title":"Requirements"},{"location":"thehive/operations/cassandra-security/#configuring-cassandra","text":"Locate the section client_encryption_options and set the following options: client_encryption_options : enabled : true # If enabled and optional is set to true encrypted and unencrypted connections are handled. optional : false keystore : /pat/to/keystore.jks keystore_password : keystorepassword require_client_auth : false # Set trustore and truststore_password if require_client_auth is true # truststore: conf/.truststore # truststore_password: cassandra # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] Then the service cassandra must be restarted.","title":"Configuring Cassandra"},{"location":"thehive/operations/cassandra-security/#configuring-thehive","text":"db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"ip_node_1\", \"ip_node_2\", \"ip_node_3\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" port: 9142 # if alternative port has been set in Cassandra configuration cql { cluster-name: thp keyspace: thehive ssl { enabled: true truststore { location: \"/path/to/truststore.jks\" password: \"truststorepassword\" } } } } Then the service thehive must be restarted.","title":"Configuring TheHive"},{"location":"thehive/operations/fail2ban/","text":"Fail2ban # Adding TheHive into Fail2Ban # Considering TheHive logs sit in /var/log/thehive/application.log and fail2ban configuration is in /etc/fail2ban : Add a filter file in /etc/fail2ban/filter.d named thehive.conf with the following content: [INCLUDES] before = common.conf [Definition] failregex = ^.*- <HOST> (?:POST \\/api\\/login|GET .*) .*returned 401.*$ ignoreregex = Add a jail file in /etc/fail2ban/jail.d/ named thehive.local with the following content: [thehive] enabled = true port = 80,443 filter = thehive action = iptables-multiport[name=thehive, port=\"80,443\"] logpath = /var/log/thehive/application.log maxretry = 5 bantime = 14400 findtime = 1200 This will ban any IP address for 4 hours after 5 failed authentication are identified during a period of 20 min. Reload the configuration with the command fail2ban-client reload Manage banned IP addresses # Review banned IP addresses: fail2ban-client status thehive Unban an IP address: fail2ban-client set thehive unbanip <IP ADDRESS>","title":"Use fail2ban"},{"location":"thehive/operations/fail2ban/#fail2ban","text":"","title":"Fail2ban"},{"location":"thehive/operations/fail2ban/#adding-thehive-into-fail2ban","text":"Considering TheHive logs sit in /var/log/thehive/application.log and fail2ban configuration is in /etc/fail2ban : Add a filter file in /etc/fail2ban/filter.d named thehive.conf with the following content: [INCLUDES] before = common.conf [Definition] failregex = ^.*- <HOST> (?:POST \\/api\\/login|GET .*) .*returned 401.*$ ignoreregex = Add a jail file in /etc/fail2ban/jail.d/ named thehive.local with the following content: [thehive] enabled = true port = 80,443 filter = thehive action = iptables-multiport[name=thehive, port=\"80,443\"] logpath = /var/log/thehive/application.log maxretry = 5 bantime = 14400 findtime = 1200 This will ban any IP address for 4 hours after 5 failed authentication are identified during a period of 20 min. Reload the configuration with the command fail2ban-client reload","title":"Adding TheHive into Fail2Ban"},{"location":"thehive/operations/fail2ban/#manage-banned-ip-addresses","text":"Review banned IP addresses: fail2ban-client status thehive Unban an IP address: fail2ban-client set thehive unbanip <IP ADDRESS>","title":"Manage banned IP addresses"},{"location":"thehive/operations/https/","text":"","title":"Configure HTTPS"},{"location":"thehive/operations/migration/","text":"Migration to TheHive 4 # TheHive 4.x is delivered with a tool to migrate your data from TheHive 3.x. stored in Elasticsearch. Supported versions # Starting with TheHive 4.1.17, the migration tool supports migrating data from both TheHive 3.4.x and 3.5.x. Migrating from Possible target version TheHive 3.4.x + Elasticsearch 6.x TheHive 4.1.17+ TheHive 3.5.x + Elasticsearch 7.x TheHive 4.1.17+ How it works # All packages of TheHive4 distributed come with the migration program which can be used to import data from TheHive 3.4.0+. By default, it is installed in /opt/thehive/bin/migrate . Pre-requisite # In order to migrate the data: TheHive 4 must be installed on the system running the migration tool; TheHive4 must be configured ; in particular database , index , and file storage ; The service thehive must be stopped ( service thehive stop ) on the target server. This tools must also have access to Elasticsearch database (http://ES:9200) used by TheHive 3, and the configuration file of TheHive 3.x instance. Configuration of TheHive 4 # Warning In TheHive4, users are identified by their email addresses. Thus, a domain will be appended to usernames in order to migrate users from TheHive 3. TheHive 4.x comes with a default domain named thehive.local . Starting the migration without explicitely specifying a domain name will result in migrating all users with a username formatted like user@thehive.local . Change the default domain name used to import existing users in the configuration file of TheHive4 ( /etc/thehive/application.conf ) ; add or update the setting named auth.defaultUserDomain : auth.defaultUserDomain : \"mydomain.com\" This way, the domain mydomain.com will be appended to user accounts imported from TheHive 3.4+ ( user@mydomain.com ). Run the migration # Prepare, install and configure your new instance of TheHive 4.x by following the associated guides . Once TheHive4 configuration file ( /etc/thehive/application.conf ) is correctly filled the migrate command ca be executed. Info This recommended to run this program as the user in charge of running TheHive service ( thehive if you are installing the application with DEB or RPM package) The program comes with a large set of options: # /opt/thehive/bin/migrate --help TheHive migration tool 4.1.17-1 Usage: migrate [options] -v, --version -h, --help -l, --logger-config <file> logback configuration file -c, --config <file> global configuration file -i, --input <file> TheHive3 configuration file -o, --output <file> TheHive4 configuration file -d, --drop-database Drop TheHive4 database before migration -r, --resume Resume migration (or migrate on existing database) -m, --main-organisation <organisation> -u, --es-uri http://ip1:port,ip2:port TheHive3 ElasticSearch URI -e, --es-index <index> TheHive3 ElasticSearch index name -x, --es-index-version <index> TheHive3 ElasticSearch index name version number (default: autodetect) -a, --es-keepalive <duration> TheHive3 ElasticSearch keepalive -p, --es-pagesize <value> TheHive3 ElasticSearch page size -s, --es-single-type <bool> Elasticsearch single type -y, --transaction-pagesize <value> page size for each transaction -t, --thread-count <value> number of threads --max-case-age <duration> migrate only cases whose age is less than <duration> --min-case-age <duration> migrate only cases whose age is greater than <duration> --case-from-date <date> migrate only cases created from <date> --case-until-date <date> migrate only cases created until <date> --case-from-number <number> migrate only cases from this case number --case-until-number <number> migrate only cases until this case number --max-alert-age <duration> migrate only alerts whose age is less than <duration> --min-alert-age <duration> migrate only alerts whose age is greater than <duration> --alert-from-date <date> migrate only alerts created from <date> --alert-until-date <date> migrate only alerts created until <date> --include-alert-types <type>,<type>... migrate only alerts with this types --exclude-alert-types <type>,<type>... don't migrate alerts with this types --include-alert-sources <source>,<source>... migrate only alerts with this sources --exclude-alert-sources <source>,<source>... don't migrate alerts with this sources --max-audit-age <duration> migrate only audits whose age is less than <duration> --min-audit-age <duration> migrate only audits whose age is greater than <duration> --audit-from-date <date> migrate only audits created from <date> --audit-until-date <date> migrate only audits created until <date> --include-audit-actions <value> migration only audits with this action (Update, Creation, Delete) --exclude-audit-actions <value> don't migration audits with this action (Update, Creation, Delete) --include-audit-objectTypes <value> migration only audits with this objectType (case, case_artifact, case_task, ...) --exclude-audit-objectTypes <value> don't migration audits with this objectType (case, case_artifact, case_task, ...) --case-number-shift <value> transpose case number by adding this value Accepted date formats are \"yyyyMMdd[HH[mm[ss]]]\" and \"MMdd\" The Format for duration is: <length> <unit>. Accepted units are: DAY: d, day HOUR: h, hr, hour MINUTE: m, min, minute SECOND: s, sec, second MILLISECOND: ms, milli, millisecond Most of these options are filters you can apply to the program. For example, you could decide to import only some of the Cases/Alerts from your old instance: Import Cases/Alerts not older than X days/hours, Import Cases/Alerts with the ID number, Import part of Audit trail ... Taking the assumption that you are migrating a database hosted in a remote server, with TheHive 3, a basic command line to migrate data to a new instance will be like: /opt/thehive/bin/migrate \\ --output /etc/thehive/application.conf \\ --main-organisation myOrganisation \\ --es-uri http://ELASTICSEARCH_IP_ADDRESS:9200 \\ --es-index the_hive with: Option Description --output specifies the configuration file of TheHive 4.0 (the one that has previously been configured with at least, the database and file storage ) --main-organisation specifies the Organisation named myOrganisation to create during the migration. The tool will then create Users, Cases and Alerts from TheHive3 under that organisation; --es-uri specifies the URL of the Elasticsearch server. If using authentication on Elasticsearch, --input option with a configuration file for TheHive3 is required --es-index specifies the index used in Elasticsearch. Info The migration process can be very long, from several hours to several days, depending on the volume of data to migrate. We highly recommand to not start the application during the migration. Using authentication on Cassandra # if you are using a dedicated account on Cassandra to access TheHive 4 data, this user must have permissions to create keyspaces on the database: GRANT CREATE on ALL KEYSPACES to username ; Migration logs # The migration tool generates some logs during the process. By default, every 10 sec. a log is generated with information regarding the situation of the migration: [info] o.t.t.m.Migrate - [Migrate cases and alerts] CaseTemplate/Task:32 Organisation:1/1 Case/Task:160/201 Case:31/52 Job:103/138 ObservableType:3/17 Alert:25/235 Audit:3207/2986 CaseTemplate:6/6 Alert/Observable:700(52ms) Case/Observable:1325/1665 User:9/9 CustomField:13/13 Case/Task/Log:20/27 Numbers of Observables, Cases and others are estimations and not a definite value as computing these number can be very tedious. Files from MISP imported with TheHive 2.13 and earlier It is important to notice that migrating Cases/Alerts containing MISP event that were imported with TheHive 2.13 ( Sept 2017 ) or older, will cause observable files not being imported in TheHive 4. Indeed, until this version, TheHive referenced the file to the AttributeId in MISP and was not automatically downloaded. It then could generate a log like this: [warn] o.t.t.m.t.Input - Pre 2.13 file observables are ignored in MISP alert ffa3a8503ab0cd4f99fc6937a8e9b827 Starting TheHive 4 # Once the migration process is sucessfully completed, TheHive4 can be started. Warning During the first start data are indexed and service is not available ; this can take some time. Do not stop or restart the service at this time.","title":"Migration from TheHive 3.x"},{"location":"thehive/operations/migration/#migration-to-thehive-4","text":"TheHive 4.x is delivered with a tool to migrate your data from TheHive 3.x. stored in Elasticsearch.","title":"Migration to TheHive 4"},{"location":"thehive/operations/migration/#supported-versions","text":"Starting with TheHive 4.1.17, the migration tool supports migrating data from both TheHive 3.4.x and 3.5.x. Migrating from Possible target version TheHive 3.4.x + Elasticsearch 6.x TheHive 4.1.17+ TheHive 3.5.x + Elasticsearch 7.x TheHive 4.1.17+","title":"Supported versions"},{"location":"thehive/operations/migration/#how-it-works","text":"All packages of TheHive4 distributed come with the migration program which can be used to import data from TheHive 3.4.0+. By default, it is installed in /opt/thehive/bin/migrate .","title":"How it works"},{"location":"thehive/operations/migration/#pre-requisite","text":"In order to migrate the data: TheHive 4 must be installed on the system running the migration tool; TheHive4 must be configured ; in particular database , index , and file storage ; The service thehive must be stopped ( service thehive stop ) on the target server. This tools must also have access to Elasticsearch database (http://ES:9200) used by TheHive 3, and the configuration file of TheHive 3.x instance.","title":"Pre-requisite"},{"location":"thehive/operations/migration/#configuration-of-thehive-4","text":"Warning In TheHive4, users are identified by their email addresses. Thus, a domain will be appended to usernames in order to migrate users from TheHive 3. TheHive 4.x comes with a default domain named thehive.local . Starting the migration without explicitely specifying a domain name will result in migrating all users with a username formatted like user@thehive.local . Change the default domain name used to import existing users in the configuration file of TheHive4 ( /etc/thehive/application.conf ) ; add or update the setting named auth.defaultUserDomain : auth.defaultUserDomain : \"mydomain.com\" This way, the domain mydomain.com will be appended to user accounts imported from TheHive 3.4+ ( user@mydomain.com ).","title":"Configuration of TheHive 4"},{"location":"thehive/operations/migration/#run-the-migration","text":"Prepare, install and configure your new instance of TheHive 4.x by following the associated guides . Once TheHive4 configuration file ( /etc/thehive/application.conf ) is correctly filled the migrate command ca be executed. Info This recommended to run this program as the user in charge of running TheHive service ( thehive if you are installing the application with DEB or RPM package) The program comes with a large set of options: # /opt/thehive/bin/migrate --help TheHive migration tool 4.1.17-1 Usage: migrate [options] -v, --version -h, --help -l, --logger-config <file> logback configuration file -c, --config <file> global configuration file -i, --input <file> TheHive3 configuration file -o, --output <file> TheHive4 configuration file -d, --drop-database Drop TheHive4 database before migration -r, --resume Resume migration (or migrate on existing database) -m, --main-organisation <organisation> -u, --es-uri http://ip1:port,ip2:port TheHive3 ElasticSearch URI -e, --es-index <index> TheHive3 ElasticSearch index name -x, --es-index-version <index> TheHive3 ElasticSearch index name version number (default: autodetect) -a, --es-keepalive <duration> TheHive3 ElasticSearch keepalive -p, --es-pagesize <value> TheHive3 ElasticSearch page size -s, --es-single-type <bool> Elasticsearch single type -y, --transaction-pagesize <value> page size for each transaction -t, --thread-count <value> number of threads --max-case-age <duration> migrate only cases whose age is less than <duration> --min-case-age <duration> migrate only cases whose age is greater than <duration> --case-from-date <date> migrate only cases created from <date> --case-until-date <date> migrate only cases created until <date> --case-from-number <number> migrate only cases from this case number --case-until-number <number> migrate only cases until this case number --max-alert-age <duration> migrate only alerts whose age is less than <duration> --min-alert-age <duration> migrate only alerts whose age is greater than <duration> --alert-from-date <date> migrate only alerts created from <date> --alert-until-date <date> migrate only alerts created until <date> --include-alert-types <type>,<type>... migrate only alerts with this types --exclude-alert-types <type>,<type>... don't migrate alerts with this types --include-alert-sources <source>,<source>... migrate only alerts with this sources --exclude-alert-sources <source>,<source>... don't migrate alerts with this sources --max-audit-age <duration> migrate only audits whose age is less than <duration> --min-audit-age <duration> migrate only audits whose age is greater than <duration> --audit-from-date <date> migrate only audits created from <date> --audit-until-date <date> migrate only audits created until <date> --include-audit-actions <value> migration only audits with this action (Update, Creation, Delete) --exclude-audit-actions <value> don't migration audits with this action (Update, Creation, Delete) --include-audit-objectTypes <value> migration only audits with this objectType (case, case_artifact, case_task, ...) --exclude-audit-objectTypes <value> don't migration audits with this objectType (case, case_artifact, case_task, ...) --case-number-shift <value> transpose case number by adding this value Accepted date formats are \"yyyyMMdd[HH[mm[ss]]]\" and \"MMdd\" The Format for duration is: <length> <unit>. Accepted units are: DAY: d, day HOUR: h, hr, hour MINUTE: m, min, minute SECOND: s, sec, second MILLISECOND: ms, milli, millisecond Most of these options are filters you can apply to the program. For example, you could decide to import only some of the Cases/Alerts from your old instance: Import Cases/Alerts not older than X days/hours, Import Cases/Alerts with the ID number, Import part of Audit trail ... Taking the assumption that you are migrating a database hosted in a remote server, with TheHive 3, a basic command line to migrate data to a new instance will be like: /opt/thehive/bin/migrate \\ --output /etc/thehive/application.conf \\ --main-organisation myOrganisation \\ --es-uri http://ELASTICSEARCH_IP_ADDRESS:9200 \\ --es-index the_hive with: Option Description --output specifies the configuration file of TheHive 4.0 (the one that has previously been configured with at least, the database and file storage ) --main-organisation specifies the Organisation named myOrganisation to create during the migration. The tool will then create Users, Cases and Alerts from TheHive3 under that organisation; --es-uri specifies the URL of the Elasticsearch server. If using authentication on Elasticsearch, --input option with a configuration file for TheHive3 is required --es-index specifies the index used in Elasticsearch. Info The migration process can be very long, from several hours to several days, depending on the volume of data to migrate. We highly recommand to not start the application during the migration.","title":"Run the migration"},{"location":"thehive/operations/migration/#using-authentication-on-cassandra","text":"if you are using a dedicated account on Cassandra to access TheHive 4 data, this user must have permissions to create keyspaces on the database: GRANT CREATE on ALL KEYSPACES to username ;","title":"Using authentication on Cassandra"},{"location":"thehive/operations/migration/#migration-logs","text":"The migration tool generates some logs during the process. By default, every 10 sec. a log is generated with information regarding the situation of the migration: [info] o.t.t.m.Migrate - [Migrate cases and alerts] CaseTemplate/Task:32 Organisation:1/1 Case/Task:160/201 Case:31/52 Job:103/138 ObservableType:3/17 Alert:25/235 Audit:3207/2986 CaseTemplate:6/6 Alert/Observable:700(52ms) Case/Observable:1325/1665 User:9/9 CustomField:13/13 Case/Task/Log:20/27 Numbers of Observables, Cases and others are estimations and not a definite value as computing these number can be very tedious. Files from MISP imported with TheHive 2.13 and earlier It is important to notice that migrating Cases/Alerts containing MISP event that were imported with TheHive 2.13 ( Sept 2017 ) or older, will cause observable files not being imported in TheHive 4. Indeed, until this version, TheHive referenced the file to the AttributeId in MISP and was not automatically downloaded. It then could generate a log like this: [warn] o.t.t.m.t.Input - Pre 2.13 file observables are ignored in MISP alert ffa3a8503ab0cd4f99fc6937a8e9b827","title":"Migration logs"},{"location":"thehive/operations/migration/#starting-thehive-4","text":"Once the migration process is sucessfully completed, TheHive4 can be started. Warning During the first start data are indexed and service is not available ; this can take some time. Do not stop or restart the service at this time.","title":"Starting TheHive 4"},{"location":"thehive/operations/troubleshooting/","text":"Troubleshooting # For some issues, we need extra information in logs to troubleshoot and understand to root causes. To gather and share this, please read carefully and follow these steps. Warning ENABLING TRACE LOGS HAS SIGNIFICANT IMPACT ON PERFORMANCES. DO NOT ENABLE IT ON PRODUCTION SERVERS. Stop TheHive service and ensure it is stopped # service thehive stop Ensure the service is stopped with the following command: service thehive status Renew application.log file # in /var/log/thehive move the file application.log to application.log.bak mv /var/log/thehive/application.log /var/log/thehive/application.log.bak Update log configuration # Edit the file /etc/thehive/logback.xml . Look for the line containing <logger name=\"org.thp\" level=\"INFO\"/> and update it to have following lines: [..] <logger name= \"org.thp\" level= \"TRACE\" /> [..] Save the file. Restart the service # service thehive start A new log file /var/log/thehive/application.log should be created and filed with a huge amount of logs. Wait for the issue to appear and/or the application stop. Save the logs # Copy the log file in a safe place. cp /var/log/thehive/application.log /root Share it with us # Create an issue on Github and please share context and symptoms with the log file. Please add information regarding: Context: instance (single node/cluster, backend type, index engine) System: Operating System, amount of RAM, #CPU for each server/node Symptoms: what you did, how you you come to this situation, what happened The log file with traces Revert # To get back a to normal log configuration, stop thehive, update logback.xml file with the previous configuration, and restart the application.","title":"Troubleshooting"},{"location":"thehive/operations/troubleshooting/#troubleshooting","text":"For some issues, we need extra information in logs to troubleshoot and understand to root causes. To gather and share this, please read carefully and follow these steps. Warning ENABLING TRACE LOGS HAS SIGNIFICANT IMPACT ON PERFORMANCES. DO NOT ENABLE IT ON PRODUCTION SERVERS.","title":"Troubleshooting"},{"location":"thehive/operations/troubleshooting/#stop-thehive-service-and-ensure-it-is-stopped","text":"service thehive stop Ensure the service is stopped with the following command: service thehive status","title":"Stop TheHive service and ensure it is stopped"},{"location":"thehive/operations/troubleshooting/#renew-applicationlog-file","text":"in /var/log/thehive move the file application.log to application.log.bak mv /var/log/thehive/application.log /var/log/thehive/application.log.bak","title":"Renew application.log file"},{"location":"thehive/operations/troubleshooting/#update-log-configuration","text":"Edit the file /etc/thehive/logback.xml . Look for the line containing <logger name=\"org.thp\" level=\"INFO\"/> and update it to have following lines: [..] <logger name= \"org.thp\" level= \"TRACE\" /> [..] Save the file.","title":"Update log configuration"},{"location":"thehive/operations/troubleshooting/#restart-the-service","text":"service thehive start A new log file /var/log/thehive/application.log should be created and filed with a huge amount of logs. Wait for the issue to appear and/or the application stop.","title":"Restart the service"},{"location":"thehive/operations/troubleshooting/#save-the-logs","text":"Copy the log file in a safe place. cp /var/log/thehive/application.log /root","title":"Save the logs"},{"location":"thehive/operations/troubleshooting/#share-it-with-us","text":"Create an issue on Github and please share context and symptoms with the log file. Please add information regarding: Context: instance (single node/cluster, backend type, index engine) System: Operating System, amount of RAM, #CPU for each server/node Symptoms: what you did, how you you come to this situation, what happened The log file with traces","title":"Share it with us"},{"location":"thehive/operations/troubleshooting/#revert","text":"To get back a to normal log configuration, stop thehive, update logback.xml file with the previous configuration, and restart the application.","title":"Revert"},{"location":"thehive/operations/update/","text":"Update guides # Update from TheHive 4.0.x to TheHive 4.1.0 # TheHive 4.1.0 comes with an updated application stack, with new components dedicated to performance improvement. TheHive 4.1.0 requires the usage of a dedicated index engine to manages indexed data. As a result, the minimum configuration required has been updated: If you are a new user of TheHive, follow the installation and configuration guide . If you are an existing user of TheHive 4.0.x, an index engine should be configured alongside the database. And wether you are using a standalone server or a cluster, the solution to implement and the configuration to update are different. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch . Updating a standalone server # In this case, a Lucene can be used. TheHive 4.1.0 comes with its Lucene engine. The configuration of TheHive, can be updated like this: Create a dedicated folder for indexes (for example /opt/thp/thehive/index ). This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Add the index configuration in the db.janusgraph part: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Restart TheHive service thehive restart Once TheHive configuration is updated and restarted, new indexes are created during the start-up phase. Warning The start-up phase of TheHive and the indexes creation can take a certain amount if time. This phase will be quicker once indexes exist. Updating a cluster # In this case, a Elasticsearch should be used, as all nodes should have access to the same index. Once your Elasticsearch instance is up and running, The configuration of TheHive can be updated like this: Here is an example of configuration, use your IP address/hostnames. ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive } } } } In this configuration, all TheHive nodes should have the same configuration. Restart all nodes of the cluster. Info the cluster makes it work out ; one of the nodes manage the indexing process while others are waiting for it to be ready. More information # More information about the configuration of database and indexes can be found in the dedicated configuration guide","title":"Howto update"},{"location":"thehive/operations/update/#update-guides","text":"","title":"Update guides"},{"location":"thehive/operations/update/#update-from-thehive-40x-to-thehive-410","text":"TheHive 4.1.0 comes with an updated application stack, with new components dedicated to performance improvement. TheHive 4.1.0 requires the usage of a dedicated index engine to manages indexed data. As a result, the minimum configuration required has been updated: If you are a new user of TheHive, follow the installation and configuration guide . If you are an existing user of TheHive 4.0.x, an index engine should be configured alongside the database. And wether you are using a standalone server or a cluster, the solution to implement and the configuration to update are different. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch .","title":"Update from TheHive 4.0.x to TheHive 4.1.0"},{"location":"thehive/operations/update/#updating-a-standalone-server","text":"In this case, a Lucene can be used. TheHive 4.1.0 comes with its Lucene engine. The configuration of TheHive, can be updated like this: Create a dedicated folder for indexes (for example /opt/thp/thehive/index ). This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Add the index configuration in the db.janusgraph part: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Restart TheHive service thehive restart Once TheHive configuration is updated and restarted, new indexes are created during the start-up phase. Warning The start-up phase of TheHive and the indexes creation can take a certain amount if time. This phase will be quicker once indexes exist.","title":"Updating a standalone server"},{"location":"thehive/operations/update/#updating-a-cluster","text":"In this case, a Elasticsearch should be used, as all nodes should have access to the same index. Once your Elasticsearch instance is up and running, The configuration of TheHive can be updated like this: Here is an example of configuration, use your IP address/hostnames. ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive } } } } In this configuration, all TheHive nodes should have the same configuration. Restart all nodes of the cluster. Info the cluster makes it work out ; one of the nodes manage the indexing process while others are waiting for it to be ready.","title":"Updating a cluster"},{"location":"thehive/operations/update/#more-information","text":"More information about the configuration of database and indexes can be found in the dedicated configuration guide","title":"More information"},{"location":"thehive/user-guides/","text":"User guides # Get a Quick start with TheHive or follow the guides bellow for more details: For administrators # Administrators as users defined in the admin organisation, created by default in TheHive. Administators have the responsibility of managing the platform by defining organisations and all the platform data available for to all the organisations. Manage organisations Manage profiles, roles and permissions Manage Custom fields Manage Observable types Manage Analyzers templates Manage Tags & taxonomies Manage Tactics, Techniques & Procedures Platform Status For organisation managers # Organisation managers are users belonging to any organisation other than admin and having one of the following permissions to manage users, case template, custom tags and UI configuration. TheHive comes with a default role for organisation managers, called org-admin . Organisations, users and sharing Manage users Manage Case templates Manage custom tags Manage UI configurations For Analysts # Analysts are user belonging to any organisation other than admin without organisation management permissions. Create Alerts Create Cases Create Tasks Create Observables Create TTPs Run Responders Run Analyzers Sharing Cases, Tasks and Observables Close Cases Export Cases to MISP User settings","title":"User guides"},{"location":"thehive/user-guides/#user-guides","text":"Get a Quick start with TheHive or follow the guides bellow for more details:","title":"User guides"},{"location":"thehive/user-guides/#for-administrators","text":"Administrators as users defined in the admin organisation, created by default in TheHive. Administators have the responsibility of managing the platform by defining organisations and all the platform data available for to all the organisations. Manage organisations Manage profiles, roles and permissions Manage Custom fields Manage Observable types Manage Analyzers templates Manage Tags & taxonomies Manage Tactics, Techniques & Procedures Platform Status","title":"For administrators"},{"location":"thehive/user-guides/#for-organisation-managers","text":"Organisation managers are users belonging to any organisation other than admin and having one of the following permissions to manage users, case template, custom tags and UI configuration. TheHive comes with a default role for organisation managers, called org-admin . Organisations, users and sharing Manage users Manage Case templates Manage custom tags Manage UI configurations","title":"For organisation managers"},{"location":"thehive/user-guides/#for-analysts","text":"Analysts are user belonging to any organisation other than admin without organisation management permissions. Create Alerts Create Cases Create Tasks Create Observables Create TTPs Run Responders Run Analyzers Sharing Cases, Tasks and Observables Close Cases Export Cases to MISP User settings","title":"For Analysts"},{"location":"thehive/user-guides/quick-start/","text":"Quick start with TheHive # TL;DR # Default administrator account: admin@thehive.local / secret Login with default account Create an organisaton Create a user account Before starting # Starting from TheHive 4.0-RC1, an email address is requested, and is mandatory to register a new user, and to log in the application. Intialize TheHive 4 # This version of TheHive comes with a big improvement: Multi-tenancy support Fine grained permissions Customized user profiles (a set of permissions) After TheHive installation, a default organisation called \"admin\" is created and contains the initial default super administrator user, having a profile called \"admin\" . We will discuss the user profiles later, but note that the \"admin\" user has all the administration permissions like: create organisation define profiles define observable types define custom fields Members of \" admin \" organisation are dedicated to user accounts in charge of administrating the solution. The initial user has the following credentials: login: admin@thehive.local password: secret This default group cannot create and own Cases or any other related objects like Tasks or Observables . First login # When TheHive starts the first time, you need to login using the credential of the \"admin\" user indicated above ( admin@thehive.local / secret ), and you will be redirected to the administration home page: List of organisations . Note that this organisation cannot be deleted . Possible operations for the \"admin\" users (members of the \"admin\" organisation) are accessible from the \"Admin\" menu located on the header bar: admin Organisation cannot manage Cases. so let's create an organisation and its users. Create an organisation # The initial action that a super admin have to make is to create the organisations (tenants) that will use TheHive to deal with incident response. From the \"List of organisations\" page, hit the \"New Organisation\" button to open the organisation dialog. The organisation name is required and must be unique. Hit \"Save\" to confirm. Create a user # Once you have created an organisation you can open its details page by clicking \"Configure\". This organisation details page, for users with \"admin\" profile allows managing organisation users only. You can see on this page: The details of the organisation: name, description, the user that created it A tab to manage users: Create them Edit their password, api key Edit their profile Reset their 2FA settings Lock and delete them To create a user, just click the \"Create new user\" button, that opens the user creation dialog. Note: The \"Profile\" field will be populated by the profiles that can be assigned to organisation users only. (Administration profiles will not be listed). The first user you must create for each organisation, should be a user with \"org-admin\" profile. That profile allows all the operations within an organisation. A user with \"org-admin\" profile will be able to connect and configure its organisation by at least: Creating other users Creating case templates Once you have created the users, you can set their passwords (they will be able to change them from their own account page). To do that, click on the \"New password\" button on the corresponding user's row and then hit ENTER or click the green check button: Login as org-admin user # Once the user is created, (s)he can connect to TheHive and start using it based on the profile. Note that users with \"org-admin\" profile have an \"Organisation\" menu in the right side corner of the header bar given access to the organisation configuration page with and additionnal tab for case template management. Now that the organisation and users are created, let's define custom fields and then use them to define case templates.","title":"Quick start with TheHive"},{"location":"thehive/user-guides/quick-start/#quick-start-with-thehive","text":"","title":"Quick start with TheHive"},{"location":"thehive/user-guides/quick-start/#tldr","text":"Default administrator account: admin@thehive.local / secret Login with default account Create an organisaton Create a user account","title":"TL;DR"},{"location":"thehive/user-guides/quick-start/#before-starting","text":"Starting from TheHive 4.0-RC1, an email address is requested, and is mandatory to register a new user, and to log in the application.","title":"Before starting"},{"location":"thehive/user-guides/quick-start/#intialize-thehive-4","text":"This version of TheHive comes with a big improvement: Multi-tenancy support Fine grained permissions Customized user profiles (a set of permissions) After TheHive installation, a default organisation called \"admin\" is created and contains the initial default super administrator user, having a profile called \"admin\" . We will discuss the user profiles later, but note that the \"admin\" user has all the administration permissions like: create organisation define profiles define observable types define custom fields Members of \" admin \" organisation are dedicated to user accounts in charge of administrating the solution. The initial user has the following credentials: login: admin@thehive.local password: secret This default group cannot create and own Cases or any other related objects like Tasks or Observables .","title":"Intialize TheHive 4"},{"location":"thehive/user-guides/quick-start/#first-login","text":"When TheHive starts the first time, you need to login using the credential of the \"admin\" user indicated above ( admin@thehive.local / secret ), and you will be redirected to the administration home page: List of organisations . Note that this organisation cannot be deleted . Possible operations for the \"admin\" users (members of the \"admin\" organisation) are accessible from the \"Admin\" menu located on the header bar: admin Organisation cannot manage Cases. so let's create an organisation and its users.","title":"First login"},{"location":"thehive/user-guides/quick-start/#create-an-organisation","text":"The initial action that a super admin have to make is to create the organisations (tenants) that will use TheHive to deal with incident response. From the \"List of organisations\" page, hit the \"New Organisation\" button to open the organisation dialog. The organisation name is required and must be unique. Hit \"Save\" to confirm.","title":"Create an organisation"},{"location":"thehive/user-guides/quick-start/#create-a-user","text":"Once you have created an organisation you can open its details page by clicking \"Configure\". This organisation details page, for users with \"admin\" profile allows managing organisation users only. You can see on this page: The details of the organisation: name, description, the user that created it A tab to manage users: Create them Edit their password, api key Edit their profile Reset their 2FA settings Lock and delete them To create a user, just click the \"Create new user\" button, that opens the user creation dialog. Note: The \"Profile\" field will be populated by the profiles that can be assigned to organisation users only. (Administration profiles will not be listed). The first user you must create for each organisation, should be a user with \"org-admin\" profile. That profile allows all the operations within an organisation. A user with \"org-admin\" profile will be able to connect and configure its organisation by at least: Creating other users Creating case templates Once you have created the users, you can set their passwords (they will be able to change them from their own account page). To do that, click on the \"New password\" button on the corresponding user's row and then hit ENTER or click the green check button:","title":"Create a user"},{"location":"thehive/user-guides/quick-start/#login-as-org-admin-user","text":"Once the user is created, (s)he can connect to TheHive and start using it based on the profile. Note that users with \"org-admin\" profile have an \"Organisation\" menu in the right side corner of the header bar given access to the organisation configuration page with and additionnal tab for case template management. Now that the organisation and users are created, let's define custom fields and then use them to define case templates.","title":"Login as org-admin user"},{"location":"thehive/user-guides/administrators/analyzer-templates/","text":"Manage analyzer template # Before TheHive4, we used to call them Report templates and we allowed two types of templates: Short reports: used to customise the display of analysis report summary Long reports: used to customise the rendering of the raw report of a given analyzer report Starting from TheHive4, short reports have been removed, and TheHive will display the analysis summary the same way for all analyzers: display a tag using taxonomies and level color. List analyzer templates # The management page is accessible from the header menu through the Admin > Analyzer templates menu and required a use with the manageAnalyzerTemplate permission (refer to Profiles and permissions ). Note that analyzer templates are global and common to all the organisations. Analyzer templates are still customisable via the UI and can also be imported. Import analyzer templates # TheHive Project provides a set of analyzer templates (we use the same report-templates.zip archive for backward compatibility reasons). The template archive is available at https://download.thehive-project.org/report-templates.zip . To import the zip file, click on the Import templates , this opens the import dialog. Drop the zip files or click to select it from your storage and finally click Yes, import template archive .","title":"Manage analyzer template"},{"location":"thehive/user-guides/administrators/analyzer-templates/#manage-analyzer-template","text":"Before TheHive4, we used to call them Report templates and we allowed two types of templates: Short reports: used to customise the display of analysis report summary Long reports: used to customise the rendering of the raw report of a given analyzer report Starting from TheHive4, short reports have been removed, and TheHive will display the analysis summary the same way for all analyzers: display a tag using taxonomies and level color.","title":"Manage analyzer template"},{"location":"thehive/user-guides/administrators/analyzer-templates/#list-analyzer-templates","text":"The management page is accessible from the header menu through the Admin > Analyzer templates menu and required a use with the manageAnalyzerTemplate permission (refer to Profiles and permissions ). Note that analyzer templates are global and common to all the organisations. Analyzer templates are still customisable via the UI and can also be imported.","title":"List analyzer templates"},{"location":"thehive/user-guides/administrators/analyzer-templates/#import-analyzer-templates","text":"TheHive Project provides a set of analyzer templates (we use the same report-templates.zip archive for backward compatibility reasons). The template archive is available at https://download.thehive-project.org/report-templates.zip . To import the zip file, click on the Import templates , this opens the import dialog. Drop the zip files or click to select it from your storage and finally click Yes, import template archive .","title":"Import analyzer templates"},{"location":"thehive/user-guides/administrators/custom-fields/","text":"Manage custom fields # In TheHive 4, Metrics have been removed. Why? Because metrics are simply, numeric custom fields. To manage Custom fields you need to login as an \"admin\" user (Member of the \"admin\" organisation) that has a profile including the manageCustomField permission (refer to Profiles and permissions for detailed information). The default \"admin\" user has that permission. \u26a0\ufe0f Note Custom fields are global to all the organisation. When installing TheHive, the list of custom fields is initially empty, administrators have to populate it. To create a custom field, click on the \"Add custom field\" button that opens a dialog: You need to set: a display name a name (automatically pre-filled by the UI based on the display name) a description a type: on of string , intger , booleen , date and float (new type added by TheHive 4) possible values (not available for date and boolean fields) wether the field is mandatory or not (will be prompted when you close a Case without setting its value) Once the custom field is created, you can edit its details or delete it: Only unused custom fields can be removed:","title":"Manage custom fields"},{"location":"thehive/user-guides/administrators/custom-fields/#manage-custom-fields","text":"In TheHive 4, Metrics have been removed. Why? Because metrics are simply, numeric custom fields. To manage Custom fields you need to login as an \"admin\" user (Member of the \"admin\" organisation) that has a profile including the manageCustomField permission (refer to Profiles and permissions for detailed information). The default \"admin\" user has that permission. \u26a0\ufe0f Note Custom fields are global to all the organisation. When installing TheHive, the list of custom fields is initially empty, administrators have to populate it. To create a custom field, click on the \"Add custom field\" button that opens a dialog: You need to set: a display name a name (automatically pre-filled by the UI based on the display name) a description a type: on of string , intger , booleen , date and float (new type added by TheHive 4) possible values (not available for date and boolean fields) wether the field is mandatory or not (will be prompted when you close a Case without setting its value) Once the custom field is created, you can edit its details or delete it: Only unused custom fields can be removed:","title":"Manage custom fields"},{"location":"thehive/user-guides/administrators/observable-types/","text":"Manage observable types # In TheHive4, we have big plans for observable types, since we plan to support observable templates insteand of a simple string value. But this feature is planned for the future. In TheHive 4.0 observable datatype are common to all the organisation, and manageable by administrators (members of the \"admin\" organisation). The management page is accessible from the header menu through the Admin > Observable types menu and required a use with the manageObservableTemplate permission (refer to Profiles and permissions ).","title":"Manage observable types"},{"location":"thehive/user-guides/administrators/observable-types/#manage-observable-types","text":"In TheHive4, we have big plans for observable types, since we plan to support observable templates insteand of a simple string value. But this feature is planned for the future. In TheHive 4.0 observable datatype are common to all the organisation, and manageable by administrators (members of the \"admin\" organisation). The management page is accessible from the header menu through the Admin > Observable types menu and required a use with the manageObservableTemplate permission (refer to Profiles and permissions ).","title":"Manage observable types"},{"location":"thehive/user-guides/administrators/organisations/","text":"Organisations # An organisation can't be deleted To create an organisation , clic on the New Organisation button in Admin > Organisations : Provide an organisation name and a description then clic Save :","title":"Organisations"},{"location":"thehive/user-guides/administrators/organisations/#organisations","text":"An organisation can't be deleted To create an organisation , clic on the New Organisation button in Admin > Organisations : Provide an organisation name and a description then clic Save :","title":"Organisations"},{"location":"thehive/user-guides/administrators/plateform-status/","text":"Plateform status # With the database update and the new indexing engine, a health status page has been introduced. This page not only displays health status for indexes, but also for global data in the database. Indeed, when an element is created, no duplicate should exist, a control is then processed. This is sumarised in the Data health status part. Accessing to this page requires to be admin of the plateform, or at least, have managePlateform permission. Health status page # This plage can be accessed in the Admin organisation view. Open the Admin menu and click on Plateform status Note When opening the page, the indexes status can take a while. with: List of indexes and their health status. Database objects number and Index objects number should be equal for a good health status When the status is Error , proceed to reindex List of data types health status in the database and their duplicate state Process for a duplicate check on a specific data type if status is XXX warning XXX Quand un \u00e9l\u00e9ment est cr\u00e9\u00e9 et qu'il ne doit pas y avoir de doublon (caseNumber, alert type+source+sourceRef, customField, ...), un contr\u00f4le est r\u00e9alis\u00e9 (duplicationCheck). Le r\u00e9sultat de ces contr\u00f4les sont dans la cl\u00e9 duplicateStats avec les champs : - last pour le r\u00e9sultat du dernier check (avec le nombre de doublon et la dur\u00e9e du check en milliseconds), - lastDate pour la date du dernier check - global est l'aggr\u00e9gation de tous les checks depuis le lancement de TH (avec le nombre d'iterations) Les checks sont d\u00e9clench\u00e9s de fa\u00e7on \u00e0 limiter le nombre d'iterations quand on fait plusieurs ajouts dans un courte p\u00e9riode. needCheck indique qu'un check est en attente et duplicateTimer que je check est programm\u00e9. En plus des contr\u00f4les de doublon, il y a une multitude d'autres contr\u00f4les (un share doit \u00eatre attach\u00e9 \u00e0 une seule orga et un seule case, une alerte ne peut \u00eatre import\u00e9 qu'une fois, ...). Ces checks sont r\u00e9alis\u00e9s toutes les 6 heures par d\u00e9faut. On retrouve le r\u00e9sultat de ces checks dans globalStats avec la m\u00eame logique last, lastDate et global. Par contre, les cl\u00e9s ne sont pas duplicate mais un ensemble de valeurs propres \u00e0 chaque type de contr\u00f4le (orphan, extraOrganisation, nonExistentOrganisation, missingOrganisation, ...). Chaque valeur me permettent d'identifier la situation recontr\u00e9e.","title":"Plateform status"},{"location":"thehive/user-guides/administrators/plateform-status/#plateform-status","text":"With the database update and the new indexing engine, a health status page has been introduced. This page not only displays health status for indexes, but also for global data in the database. Indeed, when an element is created, no duplicate should exist, a control is then processed. This is sumarised in the Data health status part. Accessing to this page requires to be admin of the plateform, or at least, have managePlateform permission.","title":"Plateform status"},{"location":"thehive/user-guides/administrators/plateform-status/#health-status-page","text":"This plage can be accessed in the Admin organisation view. Open the Admin menu and click on Plateform status Note When opening the page, the indexes status can take a while. with: List of indexes and their health status. Database objects number and Index objects number should be equal for a good health status When the status is Error , proceed to reindex List of data types health status in the database and their duplicate state Process for a duplicate check on a specific data type if status is XXX warning XXX Quand un \u00e9l\u00e9ment est cr\u00e9\u00e9 et qu'il ne doit pas y avoir de doublon (caseNumber, alert type+source+sourceRef, customField, ...), un contr\u00f4le est r\u00e9alis\u00e9 (duplicationCheck). Le r\u00e9sultat de ces contr\u00f4les sont dans la cl\u00e9 duplicateStats avec les champs : - last pour le r\u00e9sultat du dernier check (avec le nombre de doublon et la dur\u00e9e du check en milliseconds), - lastDate pour la date du dernier check - global est l'aggr\u00e9gation de tous les checks depuis le lancement de TH (avec le nombre d'iterations) Les checks sont d\u00e9clench\u00e9s de fa\u00e7on \u00e0 limiter le nombre d'iterations quand on fait plusieurs ajouts dans un courte p\u00e9riode. needCheck indique qu'un check est en attente et duplicateTimer que je check est programm\u00e9. En plus des contr\u00f4les de doublon, il y a une multitude d'autres contr\u00f4les (un share doit \u00eatre attach\u00e9 \u00e0 une seule orga et un seule case, une alerte ne peut \u00eatre import\u00e9 qu'une fois, ...). Ces checks sont r\u00e9alis\u00e9s toutes les 6 heures par d\u00e9faut. On retrouve le r\u00e9sultat de ces checks dans globalStats avec la m\u00eame logique last, lastDate et global. Par contre, les cl\u00e9s ne sont pas duplicate mais un ensemble de valeurs propres \u00e0 chaque type de contr\u00f4le (orphan, extraOrganisation, nonExistentOrganisation, missingOrganisation, ...). Chaque valeur me permettent d'identifier la situation recontr\u00e9e.","title":"Health status page"},{"location":"thehive/user-guides/administrators/profiles/","text":"User Profiles management # User profiles is a new concept introduced by TheHive4 and coming from the support of role based access control aka RBAC. In TheHive4, users will be assigned a Profile , within an Organisation . A Profile is composed by a set of predefined permissions. Permissions # A Profile is a set of permissions attached to a User and an Organisation . It defines what the user can do on an object hold by the organisation. TheHive has a finite list of permissions: manageOrganisation (1) : the user can create , update an organisation manageConfig (1): the user can update configuration manageProfile (1): the user can create , update and delete profiles manageTag (1): the user can create , update and delete tags manageCustomField (1): the user can create , update and delete custom fields manageCase : the user can create , update and delete cases manageObservable : the user can create , update and delete observables manageAlert : the user can create , update and import alerts manageUser : the user can create , update and delete users manageCaseTemplate : the user can create , update and delete case template manageTask : the user can create , update and delete tasks manageShare : the user can share case, task and observable with other organisation manageAnalyse (2): the user can execute analyse manageAction (2): the user can execute actions manageAnalyzerTemplate (2): the user can create , update and delete analyzer template (previously named report template) (1) Organisations, configuration, profiles and tags are global objects. The related permissions are effective only on \u201cadmin\u201d organisation. (2) Actions, analysis and template is available only if Cortex connector is enabled Note Read information doesn\u2019t require specific permission. By default, users in an organisation can see all data shared with that organisation (cf. shares, discussed in Organisations,Users and sharing ). Profiles # We distinguish two types of profiles: Administration Profiles Organisation Profiles The management page is accessible from the header menu through the Admin > Profiles menu and required a use with the manageProfile permission (refer to the section above). TheHive comes with default profiles but they can be updated and removed (if not used). New profiles can be created. Once the New Profile button is clicked, a dialog is opened asking for the profile type, a name for the profile and a selection of permissions. Multiple selection can be made using CTRL+click. If it is used, a profile can\u2019t be remove but can be updated. Default profiles are: admin : can manage all global objects and users. Can\u2019t create case. analyst : can manage cases and other related objects (observables, tasks, \u2026), including shring them org-admin : all permissions except those related to global objects read-only : no permission","title":"User Profiles management"},{"location":"thehive/user-guides/administrators/profiles/#user-profiles-management","text":"User profiles is a new concept introduced by TheHive4 and coming from the support of role based access control aka RBAC. In TheHive4, users will be assigned a Profile , within an Organisation . A Profile is composed by a set of predefined permissions.","title":"User Profiles management"},{"location":"thehive/user-guides/administrators/profiles/#permissions","text":"A Profile is a set of permissions attached to a User and an Organisation . It defines what the user can do on an object hold by the organisation. TheHive has a finite list of permissions: manageOrganisation (1) : the user can create , update an organisation manageConfig (1): the user can update configuration manageProfile (1): the user can create , update and delete profiles manageTag (1): the user can create , update and delete tags manageCustomField (1): the user can create , update and delete custom fields manageCase : the user can create , update and delete cases manageObservable : the user can create , update and delete observables manageAlert : the user can create , update and import alerts manageUser : the user can create , update and delete users manageCaseTemplate : the user can create , update and delete case template manageTask : the user can create , update and delete tasks manageShare : the user can share case, task and observable with other organisation manageAnalyse (2): the user can execute analyse manageAction (2): the user can execute actions manageAnalyzerTemplate (2): the user can create , update and delete analyzer template (previously named report template) (1) Organisations, configuration, profiles and tags are global objects. The related permissions are effective only on \u201cadmin\u201d organisation. (2) Actions, analysis and template is available only if Cortex connector is enabled Note Read information doesn\u2019t require specific permission. By default, users in an organisation can see all data shared with that organisation (cf. shares, discussed in Organisations,Users and sharing ).","title":"Permissions"},{"location":"thehive/user-guides/administrators/profiles/#profiles","text":"We distinguish two types of profiles: Administration Profiles Organisation Profiles The management page is accessible from the header menu through the Admin > Profiles menu and required a use with the manageProfile permission (refer to the section above). TheHive comes with default profiles but they can be updated and removed (if not used). New profiles can be created. Once the New Profile button is clicked, a dialog is opened asking for the profile type, a name for the profile and a selection of permissions. Multiple selection can be made using CTRL+click. If it is used, a profile can\u2019t be remove but can be updated. Default profiles are: admin : can manage all global objects and users. Can\u2019t create case. analyst : can manage cases and other related objects (observables, tasks, \u2026), including shring them org-admin : all permissions except those related to global objects read-only : no permission","title":"Profiles"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/","text":"Tactics, Techniques & Procedures # TheHive 4.1.0+ is required to use TTPs Starting with version 4.1.0, TheHive allows to bind Cases to TTPs (Tactics, Techniques & Procedures) . The MITRE ATT&CK framework has been chosen to define these TTPs. Import MITRE ATT&CK patterns # To access and import MITRE ATT&CK patterns definition, beeing admin or at least have the role managePattern is required. In the admin organisation, open the ATT&CK Patterns menu Click on Import MITRE ATT&CK Patterns and select the appropriate file Ensure patterns are imported Tip A direct link to the current zip archive of MITRE ATT&CK patterns let you download it quickly from the official github page. Use MITRE ATT&CK # Refer to this page to learn how to add TTPs ( Tactics, Techniques and Procedures ) to a Case.","title":"Tactics, Techniques & Procedures"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/#tactics-techniques-procedures","text":"TheHive 4.1.0+ is required to use TTPs Starting with version 4.1.0, TheHive allows to bind Cases to TTPs (Tactics, Techniques & Procedures) . The MITRE ATT&CK framework has been chosen to define these TTPs.","title":"Tactics, Techniques &amp; Procedures"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/#import-mitre-attck-patterns","text":"To access and import MITRE ATT&CK patterns definition, beeing admin or at least have the role managePattern is required. In the admin organisation, open the ATT&CK Patterns menu Click on Import MITRE ATT&CK Patterns and select the appropriate file Ensure patterns are imported Tip A direct link to the current zip archive of MITRE ATT&CK patterns let you download it quickly from the official github page.","title":"Import MITRE ATT&amp;CK patterns"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/#use-mitre-attck","text":"Refer to this page to learn how to add TTPs ( Tactics, Techniques and Procedures ) to a Case.","title":"Use MITRE ATT&amp;CK"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/","text":"Taxonomies and Tags # TheHive 4.1.0+ is required to use Taxonomies TheHive 4.1.0 introduces the support of Taxonomies as it is defined and published by MISP . These set of classification libraries can be used in THeHive to tag Cases , Observables and Alerts . Tip Not only MISP-Taxonomies are supported by TheHive, but you can also build your own by: Following the IETF draft https://tools.ietf.org/id/draft-dulaunoy-misp-taxonomy-format-07.html Draw inspiration from an existing definition file :-) By default, TheHive does not contain any taxonomy. Import taxonomies # To access and import taxonomies, beeing admin or at least have the role manageTaxonomy is required. In the admin organisation, open the Taxonomies menu Click on Import taxonomies and select the file containing the libraries Tip A direct link to the current zip archive of MISP-Taxonomies let you download it quickly. Enable interesting taxonomies # Select the libraries you would like your user be able to use in Case or Observables , and enable it . Your browser does not support the video tag. Warning Enabling a taxonomy means all users of all Organisations can use one or more included tags in a Case or Observable . Tags from taxonomies versus free text tags # In the UI, users can add free text tags, and also choose to add a tag from a library in a dedicated view. Free text tags are managed at the Organisation level by users with orgadmin profile, or at least manageTag permission. Refer to appropriate pages to learn about how to manage custom tags , and how to use tags in TheHive. Info If a tag is imported with an Alert or created with the API, TheHive tries to dissect it as a machinetag . It tries to identify a namespace, a predicate and an optional value. If successful, and if an associated taxonomy exists and is enabled , the tag is linked to the library ; if not, it is considered as a free text tag.","title":"Taxonomies and Tags"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#taxonomies-and-tags","text":"TheHive 4.1.0+ is required to use Taxonomies TheHive 4.1.0 introduces the support of Taxonomies as it is defined and published by MISP . These set of classification libraries can be used in THeHive to tag Cases , Observables and Alerts . Tip Not only MISP-Taxonomies are supported by TheHive, but you can also build your own by: Following the IETF draft https://tools.ietf.org/id/draft-dulaunoy-misp-taxonomy-format-07.html Draw inspiration from an existing definition file :-) By default, TheHive does not contain any taxonomy.","title":"Taxonomies and Tags"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#import-taxonomies","text":"To access and import taxonomies, beeing admin or at least have the role manageTaxonomy is required. In the admin organisation, open the Taxonomies menu Click on Import taxonomies and select the file containing the libraries Tip A direct link to the current zip archive of MISP-Taxonomies let you download it quickly.","title":"Import taxonomies"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#enable-interesting-taxonomies","text":"Select the libraries you would like your user be able to use in Case or Observables , and enable it . Your browser does not support the video tag. Warning Enabling a taxonomy means all users of all Organisations can use one or more included tags in a Case or Observable .","title":"Enable interesting taxonomies"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#tags-from-taxonomies-versus-free-text-tags","text":"In the UI, users can add free text tags, and also choose to add a tag from a library in a dedicated view. Free text tags are managed at the Organisation level by users with orgadmin profile, or at least manageTag permission. Refer to appropriate pages to learn about how to manage custom tags , and how to use tags in TheHive. Info If a tag is imported with an Alert or created with the API, TheHive tries to dissect it as a machinetag . It tries to identify a namespace, a predicate and an optional value. If successful, and if an associated taxonomy exists and is enabled , the tag is linked to the library ; if not, it is considered as a free text tag.","title":"Tags from taxonomies versus free text tags"},{"location":"thehive/user-guides/analysts/close-case/","text":"Close Cases # Closing a case is one of the basic TheHive functionnalities. It indicates the investigations and responses on this incident are over. To close a case , you must have the manageCase permission (refer to Profiles and permissions ) You can find the Close button on the case banner: Closing a case requires that all tasks contained in the case are closed. If you didn't closed the tasks before, a pop-up will suggest you to close them all. Finally, provide the necessary details to close the case: Status: If the case was a True Positive , a False Positive , if this is still Indeterminate or Other (not an incident) If True positive: Was there an impact (yes/no) Summary: a summary of the incident Once the details provided, clic on Close case.","title":"Close Cases"},{"location":"thehive/user-guides/analysts/close-case/#close-cases","text":"Closing a case is one of the basic TheHive functionnalities. It indicates the investigations and responses on this incident are over. To close a case , you must have the manageCase permission (refer to Profiles and permissions ) You can find the Close button on the case banner: Closing a case requires that all tasks contained in the case are closed. If you didn't closed the tasks before, a pop-up will suggest you to close them all. Finally, provide the necessary details to close the case: Status: If the case was a True Positive , a False Positive , if this is still Indeterminate or Other (not an incident) If True positive: Was there an impact (yes/no) Summary: a summary of the incident Once the details provided, clic on Close case.","title":"Close Cases"},{"location":"thehive/user-guides/analysts/create-alerts/","text":"Create Alerts # In TheHive4, creating an alert is possible only through the API. (refer to Create Alerts ) To create an alert, the account must have manageAlert permission. (refer to Profiles and permissions )","title":"Create Alerts"},{"location":"thehive/user-guides/analysts/create-alerts/#create-alerts","text":"In TheHive4, creating an alert is possible only through the API. (refer to Create Alerts ) To create an alert, the account must have manageAlert permission. (refer to Profiles and permissions )","title":"Create Alerts"},{"location":"thehive/user-guides/analysts/create-case/","text":"Create Cases # Creating a case is one of the basic TheHive functionnalities. To create a case , you must have the manageCase permission (refer to Profiles and permissions ) In TheHive banner, clic the button New case : Then you can either chose to use a Case template , or start it from scratch using Empty case (this option may be unavailable following your organisation configuration): Once you chose your template, fill the case details: Title * Date ( startDate ) * Severity * TLP/PAP * Tags Description * Case tasks Information annoted with a '*' are mandatory information. Once case details filled, finally clic on Create case button.","title":"Create Cases"},{"location":"thehive/user-guides/analysts/create-case/#create-cases","text":"Creating a case is one of the basic TheHive functionnalities. To create a case , you must have the manageCase permission (refer to Profiles and permissions ) In TheHive banner, clic the button New case : Then you can either chose to use a Case template , or start it from scratch using Empty case (this option may be unavailable following your organisation configuration): Once you chose your template, fill the case details: Title * Date ( startDate ) * Severity * TLP/PAP * Tags Description * Case tasks Information annoted with a '*' are mandatory information. Once case details filled, finally clic on Create case button.","title":"Create Cases"},{"location":"thehive/user-guides/analysts/create-observables/","text":"Create Case Observables # In a TheHive case , you can declare observables . To create an observable , open the Observables list ( Case > Observables ). you must have the manageCase permission (refer to Profiles and permissions ) You will find the Add observable button under the Observables tab: In the pop-up, you are invited to fill the observable (s) details: Type *: The observable dataType (eg: ip, hash, domain, ...) Value *: Your observable value (eg: 8.8.8.8) One observable per line: Create one observable per line inserted in value field. One single multiline observable: Create one observable , no matter the number of lines (useful for long URLs for example). TLP *: Define here the way the information should be shared. Is IOC: Check it if this observable is considered as Indicator of Compromission. Has been sighted: Has this observable been sighted on your information system. Ignore for similarity: Do not correlate this observable with other similar observables . Tags **: Tag your observable with insightful information. Description **: Description of the observable . Details annoted with a ' ' are mandatory. Detail annoted with ' *' mean at least. Finally clic on Create Observable(s)","title":"Create Case Observables"},{"location":"thehive/user-guides/analysts/create-observables/#create-case-observables","text":"In a TheHive case , you can declare observables . To create an observable , open the Observables list ( Case > Observables ). you must have the manageCase permission (refer to Profiles and permissions ) You will find the Add observable button under the Observables tab: In the pop-up, you are invited to fill the observable (s) details: Type *: The observable dataType (eg: ip, hash, domain, ...) Value *: Your observable value (eg: 8.8.8.8) One observable per line: Create one observable per line inserted in value field. One single multiline observable: Create one observable , no matter the number of lines (useful for long URLs for example). TLP *: Define here the way the information should be shared. Is IOC: Check it if this observable is considered as Indicator of Compromission. Has been sighted: Has this observable been sighted on your information system. Ignore for similarity: Do not correlate this observable with other similar observables . Tags **: Tag your observable with insightful information. Description **: Description of the observable . Details annoted with a ' ' are mandatory. Detail annoted with ' *' mean at least. Finally clic on Create Observable(s)","title":"Create Case Observables"},{"location":"thehive/user-guides/analysts/create-tasks/","text":"Manage Case Tasks # In a TheHive case , you can find the tab tasks . Tasks List # You can consult cases Task list ( Case > Tasks ). you must have the manageCase permission (refer to Profiles and permissions ) The list contains the following information: Group: The task group membership Task: The task title Date: the startDate of the task Assignee: The user assigned to the task Actions: Delete, start/close or trigger a responder on the task Create a task # You can create a task in a case . you must have the manageCase permission (refer to Profiles and permissions ) Open your Task list and clic Create task button. On the top of the Task list a ribbon will appear, inviting you to fill the Task group and the Task title Task information # Open your task to retrieve it's information ( Case > Tasks > Task ) Task Actions # Open your task to find the possible actions ( Case > Tasks > Task ) Task actions buttons are on the top-right of a Task page : You can trigger the following actions on a task : Sharing: Ability to share with Linked organisations the task Require Action: Declare that an action is required on this task Flag: Put a flag on the task Close: Close the task Responders: Trigger a responder on the task Basic information # Open your task to retrieve these information ( Case > Tasks > Task ) A task Basic information contains the following elements: Title of the task * Group of tasks * Assignee of the task * startDate of the task * Duration of the task Status of the task All information annoted with a '*' can be modified by clicking the pen when hovering the information. Task description # Open your task to retrieve this information ( Case > Tasks > Task ) Under the Basic information , you can find the description field. It's a free text field, markdown formatted. Task logs # Open your task to retrieve the task logs ( Case > Tasks > Task ) The Add new task log allows you to create a Task log . Task logs are markdown formatted text. You can also attach a file to the log. Task logs possible actions are: Create a task log Modify a task log Delete a task log Trigger a responder on the task log","title":"Manage Case Tasks"},{"location":"thehive/user-guides/analysts/create-tasks/#manage-case-tasks","text":"In a TheHive case , you can find the tab tasks .","title":"Manage Case Tasks"},{"location":"thehive/user-guides/analysts/create-tasks/#tasks-list","text":"You can consult cases Task list ( Case > Tasks ). you must have the manageCase permission (refer to Profiles and permissions ) The list contains the following information: Group: The task group membership Task: The task title Date: the startDate of the task Assignee: The user assigned to the task Actions: Delete, start/close or trigger a responder on the task","title":"Tasks List"},{"location":"thehive/user-guides/analysts/create-tasks/#create-a-task","text":"You can create a task in a case . you must have the manageCase permission (refer to Profiles and permissions ) Open your Task list and clic Create task button. On the top of the Task list a ribbon will appear, inviting you to fill the Task group and the Task title","title":"Create a task"},{"location":"thehive/user-guides/analysts/create-tasks/#task-information","text":"Open your task to retrieve it's information ( Case > Tasks > Task )","title":"Task information"},{"location":"thehive/user-guides/analysts/create-tasks/#task-actions","text":"Open your task to find the possible actions ( Case > Tasks > Task ) Task actions buttons are on the top-right of a Task page : You can trigger the following actions on a task : Sharing: Ability to share with Linked organisations the task Require Action: Declare that an action is required on this task Flag: Put a flag on the task Close: Close the task Responders: Trigger a responder on the task","title":"Task Actions"},{"location":"thehive/user-guides/analysts/create-tasks/#basic-information","text":"Open your task to retrieve these information ( Case > Tasks > Task ) A task Basic information contains the following elements: Title of the task * Group of tasks * Assignee of the task * startDate of the task * Duration of the task Status of the task All information annoted with a '*' can be modified by clicking the pen when hovering the information.","title":"Basic information"},{"location":"thehive/user-guides/analysts/create-tasks/#task-description","text":"Open your task to retrieve this information ( Case > Tasks > Task ) Under the Basic information , you can find the description field. It's a free text field, markdown formatted.","title":"Task description"},{"location":"thehive/user-guides/analysts/create-tasks/#task-logs","text":"Open your task to retrieve the task logs ( Case > Tasks > Task ) The Add new task log allows you to create a Task log . Task logs are markdown formatted text. You can also attach a file to the log. Task logs possible actions are: Create a task log Modify a task log Delete a task log Trigger a responder on the task log","title":"Task logs"},{"location":"thehive/user-guides/analysts/export-case/","text":"Export Cases to MISP # TheHive4 has the capability to export a case to a MISP instance. This functionnality allows you to easily share your incident and findings with communities. To export a case , you must have the manageCase permission (refer to Profiles and permissions ) You also must have a MISP instance connected to your TheHive (refer to MISP Connector ) Trigger the Export button on a case action ribbon ( Case > Export ): In the MISP export pop-up, you can chose the MISP instance(s) where you want to export your case . Clic the Export button to send your case to the MISP instance.","title":"Export Cases to MISP"},{"location":"thehive/user-guides/analysts/export-case/#export-cases-to-misp","text":"TheHive4 has the capability to export a case to a MISP instance. This functionnality allows you to easily share your incident and findings with communities. To export a case , you must have the manageCase permission (refer to Profiles and permissions ) You also must have a MISP instance connected to your TheHive (refer to MISP Connector ) Trigger the Export button on a case action ribbon ( Case > Export ): In the MISP export pop-up, you can chose the MISP instance(s) where you want to export your case . Clic the Export button to send your case to the MISP instance.","title":"Export Cases to MISP"},{"location":"thehive/user-guides/analysts/run-analyzers/","text":"Run Analyzers # In TheHive4 you can run analyzers on observables . To run an analyzer , you must have the manageAnalyse permission (refer to Profiles and permissions ) From an observable page # You can trigger an analyzer on a single observable from it's page ( Case > Observables > Observable ). In the Analysis section, you'll find every analyzers available for your organisation and compatible with the observable dataType : On the right side of the Analysis section, you can trigger the analyzers of your choice by clicking on the fire button, or run them all via the button Run all : From the observables list # You can also trigger one or more analyzers on one or more observables from the Observables list ( Case > Observables ) On the left side of the Observables list , you have checkboxes to select which observables to act on. You can even select all of them using the checkbox that is at the very top of the Observables list : Once selected, clic on the Selected observables menu, and chose Run analyzers : Finally select the desired analyzers to trigger and clic Run selected analyzers : Consult analyzers report # Once the analyzer has been triggered and the job terminated, you can consult the Job report directly within TheHive. Short report # In the Observables list ( Case > Observables ), you have access to a short report : Long report # On the Observable page ( Case > Observables > Observable ), in the Analysis table, you can consult a HTML formatted long report by clicking on the analysis link:","title":"Run Analyzers"},{"location":"thehive/user-guides/analysts/run-analyzers/#run-analyzers","text":"In TheHive4 you can run analyzers on observables . To run an analyzer , you must have the manageAnalyse permission (refer to Profiles and permissions )","title":"Run Analyzers"},{"location":"thehive/user-guides/analysts/run-analyzers/#from-an-observable-page","text":"You can trigger an analyzer on a single observable from it's page ( Case > Observables > Observable ). In the Analysis section, you'll find every analyzers available for your organisation and compatible with the observable dataType : On the right side of the Analysis section, you can trigger the analyzers of your choice by clicking on the fire button, or run them all via the button Run all :","title":"From an observable page"},{"location":"thehive/user-guides/analysts/run-analyzers/#from-the-observables-list","text":"You can also trigger one or more analyzers on one or more observables from the Observables list ( Case > Observables ) On the left side of the Observables list , you have checkboxes to select which observables to act on. You can even select all of them using the checkbox that is at the very top of the Observables list : Once selected, clic on the Selected observables menu, and chose Run analyzers : Finally select the desired analyzers to trigger and clic Run selected analyzers :","title":"From the observables list"},{"location":"thehive/user-guides/analysts/run-analyzers/#consult-analyzers-report","text":"Once the analyzer has been triggered and the job terminated, you can consult the Job report directly within TheHive.","title":"Consult analyzers report"},{"location":"thehive/user-guides/analysts/run-analyzers/#short-report","text":"In the Observables list ( Case > Observables ), you have access to a short report :","title":"Short report"},{"location":"thehive/user-guides/analysts/run-analyzers/#long-report","text":"On the Observable page ( Case > Observables > Observable ), in the Analysis table, you can consult a HTML formatted long report by clicking on the analysis link:","title":"Long report"},{"location":"thehive/user-guides/analysts/run-responders/","text":"Run Responders # In TheHive4, you can run responders on 4 type of objects: A case A task A task log An observable To run a responder , you must have the manageAction permission (refer to Profiles and permissions ) A report will be generated and provided to you. From a case # You can trigger a responder from a case . On the case Action ribbon , trigger the Responders button From a task # You can trigger a responder from a task ( Case > Tasks > Task ) On the task Action ribbon , trigger the Responders button. From a task log # You can trigger a responder from a task log ( Case > Tasks > Task > Task log ) On the task log Action ribbon , trigger the Responders button. From an observable # You can trigger a responder from an observable ( Case > Observables > Observable ) On the observable Action ribbon , trigger the Responders button. View responder report # responders provides you a report that can have two status: Success Failure The report is visible in the object where you triggered it ( case , observable , task or task log ) In addition of the status, a text report is provided allowing you to know what happens:","title":"Run Responders"},{"location":"thehive/user-guides/analysts/run-responders/#run-responders","text":"In TheHive4, you can run responders on 4 type of objects: A case A task A task log An observable To run a responder , you must have the manageAction permission (refer to Profiles and permissions ) A report will be generated and provided to you.","title":"Run Responders"},{"location":"thehive/user-guides/analysts/run-responders/#from-a-case","text":"You can trigger a responder from a case . On the case Action ribbon , trigger the Responders button","title":"From a case"},{"location":"thehive/user-guides/analysts/run-responders/#from-a-task","text":"You can trigger a responder from a task ( Case > Tasks > Task ) On the task Action ribbon , trigger the Responders button.","title":"From a task"},{"location":"thehive/user-guides/analysts/run-responders/#from-a-task-log","text":"You can trigger a responder from a task log ( Case > Tasks > Task > Task log ) On the task log Action ribbon , trigger the Responders button.","title":"From a task log"},{"location":"thehive/user-guides/analysts/run-responders/#from-an-observable","text":"You can trigger a responder from an observable ( Case > Observables > Observable ) On the observable Action ribbon , trigger the Responders button.","title":"From an observable"},{"location":"thehive/user-guides/analysts/run-responders/#view-responder-report","text":"responders provides you a report that can have two status: Success Failure The report is visible in the object where you triggered it ( case , observable , task or task log ) In addition of the status, a text report is provided allowing you to know what happens:","title":"View responder report"},{"location":"thehive/user-guides/analysts/sharing/","text":"Sharing Cases, Tasks and Observables # In TheHive4, you can share 3 type of objects: A case A task An observable To share an object, you must have the manageShare permission (refer to Profiles and permissions ) You can share only with organisations that are linked to your organisation (refer to Organisations, Users and sharing ) Share a case # You can share your case by clicking the Sharing button in the case Action ribbon When you share a case , you have to chose: To which organisation(s) To which profile To share tasks or not To share observables or not Share a task # You can share a task (the case have to be shared too for this functionnality to be available) At the very bottom of a Task page ( Case > Observables > Observables ), in the section Task sharing , clic on Add share Then you can select to which organisation you will share the task : Share an observable # You can share an observable (the case have to be shared too for this functionnality to be available) At the very bottom of a Observable page ( Case > Observables > Observable ), in the section Sharing , clic on Add share Then you can select to which organisation you will share the observable : Delete a share # You can cancel the share of an object. For each object type, go to the Share list and trigger the Delete button in the Actions column:","title":"Sharing Cases, Tasks and Observables"},{"location":"thehive/user-guides/analysts/sharing/#sharing-cases-tasks-and-observables","text":"In TheHive4, you can share 3 type of objects: A case A task An observable To share an object, you must have the manageShare permission (refer to Profiles and permissions ) You can share only with organisations that are linked to your organisation (refer to Organisations, Users and sharing )","title":"Sharing Cases, Tasks and Observables"},{"location":"thehive/user-guides/analysts/sharing/#share-a-case","text":"You can share your case by clicking the Sharing button in the case Action ribbon When you share a case , you have to chose: To which organisation(s) To which profile To share tasks or not To share observables or not","title":"Share a case"},{"location":"thehive/user-guides/analysts/sharing/#share-a-task","text":"You can share a task (the case have to be shared too for this functionnality to be available) At the very bottom of a Task page ( Case > Observables > Observables ), in the section Task sharing , clic on Add share Then you can select to which organisation you will share the task :","title":"Share a task"},{"location":"thehive/user-guides/analysts/sharing/#share-an-observable","text":"You can share an observable (the case have to be shared too for this functionnality to be available) At the very bottom of a Observable page ( Case > Observables > Observable ), in the section Sharing , clic on Add share Then you can select to which organisation you will share the observable :","title":"Share an observable"},{"location":"thehive/user-guides/analysts/sharing/#delete-a-share","text":"You can cancel the share of an object. For each object type, go to the Share list and trigger the Delete button in the Actions column:","title":"Delete a share"},{"location":"thehive/user-guides/analysts/ttps/","text":"Tactics, Techniques and Procedures # In TheHive4 you can enrich your cases with TTPs. To manage a case TTPs , you must have the manageCase permission (refer to Profiles and permissions ) Add a TTP to a case # To add a TTP to a case , go to the TTPs list ( Case > TTPs ) then clic the Add TTP button: In the Add Tactic, Technique and Procedure pop-up, you can select: The occur date The Tactic The Technique (you can use filters on techniques) The Procedure (clic to Add procedure to open this free text field) Finally, clic on Add TTP in the bottom of the pop-up: Delete a TTP from a case # You can delete a TTP from a case . Go to the TTPs list ( Case > TTPs ), then clic on the Delete button in the Actions column:","title":"Tactics, Techniques and Procedures"},{"location":"thehive/user-guides/analysts/ttps/#tactics-techniques-and-procedures","text":"In TheHive4 you can enrich your cases with TTPs. To manage a case TTPs , you must have the manageCase permission (refer to Profiles and permissions )","title":"Tactics, Techniques and Procedures"},{"location":"thehive/user-guides/analysts/ttps/#add-a-ttp-to-a-case","text":"To add a TTP to a case , go to the TTPs list ( Case > TTPs ) then clic the Add TTP button: In the Add Tactic, Technique and Procedure pop-up, you can select: The occur date The Tactic The Technique (you can use filters on techniques) The Procedure (clic to Add procedure to open this free text field) Finally, clic on Add TTP in the bottom of the pop-up:","title":"Add a TTP to a case"},{"location":"thehive/user-guides/analysts/ttps/#delete-a-ttp-from-a-case","text":"You can delete a TTP from a case . Go to the TTPs list ( Case > TTPs ), then clic on the Delete button in the Actions column:","title":"Delete a TTP from a case"},{"location":"thehive/user-guides/analysts/user-settings/","text":"User settings configuration # Every TheHive user, has a set of settings that can be updated through the Settings menu located on the right hand side of the navigation bar This page allows the following operations: User settings configuration Update basic Info Update password Configure MFA Update basic Info # This section gives the user the ability to update the his/her name and upload an avatar image Update password # This section is hidden by default, the user needs to enable it, set the current password and the new one twice. Clicking Save button to submit the form Configure MFA # This section allows a user to enable 2FA authentication using a TOTP application (Google Authenticator, Authy, Microsoft Authenticator, 1password etc.) to scan the QR code or the code underneath it. The 2FA will generate A TOTP that the user should supply in the MFA Code area. If it is valid, 2FA will be activated. A Disable button allows the user to deactivate the 2FA settings. A user with 2FA activated, will be prompted to provide a TOTP during login process.","title":"User settings configuration"},{"location":"thehive/user-guides/analysts/user-settings/#user-settings-configuration","text":"Every TheHive user, has a set of settings that can be updated through the Settings menu located on the right hand side of the navigation bar This page allows the following operations: User settings configuration Update basic Info Update password Configure MFA","title":"User settings configuration"},{"location":"thehive/user-guides/analysts/user-settings/#update-basic-info","text":"This section gives the user the ability to update the his/her name and upload an avatar image","title":"Update basic Info"},{"location":"thehive/user-guides/analysts/user-settings/#update-password","text":"This section is hidden by default, the user needs to enable it, set the current password and the new one twice. Clicking Save button to submit the form","title":"Update password"},{"location":"thehive/user-guides/analysts/user-settings/#configure-mfa","text":"This section allows a user to enable 2FA authentication using a TOTP application (Google Authenticator, Authy, Microsoft Authenticator, 1password etc.) to scan the QR code or the code underneath it. The 2FA will generate A TOTP that the user should supply in the MFA Code area. If it is valid, 2FA will be activated. A Disable button allows the user to deactivate the 2FA settings. A user with 2FA activated, will be prompted to provide a TOTP during login process.","title":"Configure MFA"},{"location":"thehive/user-guides/organisation-managers/case-templates/","text":"Case Templates # Some cases may share the same structure ( customfields , tags , tasks , description , ...). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template. List case templates # The management of the case templates is accessible through the menu Organisation > Case Templates . To manage them your profile must have the permission 'manageCaseTemplate' (refer to Profiles and permissions ). Create or upload template # Create a case template # In the case templates management page, clic the New template button ( Organisation > Case Templates > New Template ). In the case template you can set: Title prefix Severity TLP/PAP Tags Description Tasks Customfields Two fields are mandatory: Template name (should be unique) Description Import a case template # You can also import your case template using a file in JSON format by clicking on the Import template button ( Organisation > Case templates > Import template ) Edit a case template # To edit a case template, open the case template list and clic the edit button on the actions column ( Organisation > Case Templates > Edit ). Export a case template # To export a case template, open the case template list and clic the export button on the actions column ( Organisation > Case Templates > Export ). Delete a case template # To delete a case template, open the case template list and clic the export button on the actions column ( Organisation > Case Templates > Export ).","title":"Case Templates"},{"location":"thehive/user-guides/organisation-managers/case-templates/#case-templates","text":"Some cases may share the same structure ( customfields , tags , tasks , description , ...). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template.","title":"Case Templates"},{"location":"thehive/user-guides/organisation-managers/case-templates/#list-case-templates","text":"The management of the case templates is accessible through the menu Organisation > Case Templates . To manage them your profile must have the permission 'manageCaseTemplate' (refer to Profiles and permissions ).","title":"List case templates"},{"location":"thehive/user-guides/organisation-managers/case-templates/#create-or-upload-template","text":"","title":"Create or upload template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#create-a-case-template","text":"In the case templates management page, clic the New template button ( Organisation > Case Templates > New Template ). In the case template you can set: Title prefix Severity TLP/PAP Tags Description Tasks Customfields Two fields are mandatory: Template name (should be unique) Description","title":"Create a case template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#import-a-case-template","text":"You can also import your case template using a file in JSON format by clicking on the Import template button ( Organisation > Case templates > Import template )","title":"Import a case template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#edit-a-case-template","text":"To edit a case template, open the case template list and clic the edit button on the actions column ( Organisation > Case Templates > Edit ).","title":"Edit a case template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#export-a-case-template","text":"To export a case template, open the case template list and clic the export button on the actions column ( Organisation > Case Templates > Export ).","title":"Export a case template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#delete-a-case-template","text":"To delete a case template, open the case template list and clic the export button on the actions column ( Organisation > Case Templates > Export ).","title":"Delete a case template"},{"location":"thehive/user-guides/organisation-managers/custom-tags/","text":"Custom Tags # custom tags are tags manually created (out of libraries). You must have the permission manageTag on your profile to manage custom tags. (refer to Profiles and permissions ) List custom tags # You can find the list of your custom tags in Organization > Custom tags . The list contains the following information, for each tag : Number of cases tagged Number of alerts tagged Number of observables tagged Number of case templates containing the tag Modify a custom-tag border colour # You can modify your custom tags border colours. In the custom tags list ( Organization > Custom tags ), in the Colour column, clic on the square or colour code value to modify it. This will apply to all cases , alerts and observables that contains the tag . Delete a custom tag # You can also delete a custom tag. In the custom tags list ( Organization > Custom tags ), in the Actions column, clic on the delete button \u26a0\ufe0f Note Deleting a custom tag will delete the tag on each object containing it.","title":"Custom tags"},{"location":"thehive/user-guides/organisation-managers/custom-tags/#custom-tags","text":"custom tags are tags manually created (out of libraries). You must have the permission manageTag on your profile to manage custom tags. (refer to Profiles and permissions )","title":"Custom Tags"},{"location":"thehive/user-guides/organisation-managers/custom-tags/#list-custom-tags","text":"You can find the list of your custom tags in Organization > Custom tags . The list contains the following information, for each tag : Number of cases tagged Number of alerts tagged Number of observables tagged Number of case templates containing the tag","title":"List custom tags"},{"location":"thehive/user-guides/organisation-managers/custom-tags/#modify-a-custom-tag-border-colour","text":"You can modify your custom tags border colours. In the custom tags list ( Organization > Custom tags ), in the Colour column, clic on the square or colour code value to modify it. This will apply to all cases , alerts and observables that contains the tag .","title":"Modify a custom-tag border colour"},{"location":"thehive/user-guides/organisation-managers/custom-tags/#delete-a-custom-tag","text":"You can also delete a custom tag. In the custom tags list ( Organization > Custom tags ), in the Actions column, clic on the delete button \u26a0\ufe0f Note Deleting a custom tag will delete the tag on each object containing it.","title":"Delete a custom tag"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/","text":"Organisations, Users and sharing # User role, profile and permission # User # In TheHive, a user is a member of one or more organisations. One user has a profile for each organisation and can have different profiles for different organisations. For example: \u201c analyst \u201d in \u201c organisationA \u201d; and \u201c admin \u201d in \u201c organisationB \u201d; and \u201c read-only \u201d in \u201c organisationC \u201d. Organisations and sharing # TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other, and can't share with any. To do so, an organisation must be \"linked\" with another one. Only super administrators or users with manageOrganisation permissions can give the ability of a organisation to see an other one. This ability named \u201c link \u201d is unidirectional. Link with other organisations # To share a case with another organisation, a user must be able to see it: its organisation must be \"linked\" with the targeted organisation. Share and effective permissions # When a user creates a case, the case is linked to the user\u2019s organisation with the profile \u201corg-admin\u201d. It means that there is no restriction, the effective permissions are the permissions the user has in his organisation. If he decides to share that case with another organisation, he must choose the profile applied on that share. To exerce a action on a case, the related permission must be present in the user profile and in the case share. When you share a case, you can share its tasks or observables but it is not mandatory. Tasks (and observables) can be unitary shared. They can be shared only with organisations for which case is already shared. A case can be shared only once for a given organisation. Thus a case an its tasks/observables are shared with the same permissions for the same organisation.","title":"Organisations, Users and sharing"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#organisations-users-and-sharing","text":"","title":"Organisations, Users and sharing"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#user-role-profile-and-permission","text":"","title":"User role, profile and permission"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#user","text":"In TheHive, a user is a member of one or more organisations. One user has a profile for each organisation and can have different profiles for different organisations. For example: \u201c analyst \u201d in \u201c organisationA \u201d; and \u201c admin \u201d in \u201c organisationB \u201d; and \u201c read-only \u201d in \u201c organisationC \u201d.","title":"User"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#organisations-and-sharing","text":"TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other, and can't share with any. To do so, an organisation must be \"linked\" with another one. Only super administrators or users with manageOrganisation permissions can give the ability of a organisation to see an other one. This ability named \u201c link \u201d is unidirectional.","title":"Organisations and sharing"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#link-with-other-organisations","text":"To share a case with another organisation, a user must be able to see it: its organisation must be \"linked\" with the targeted organisation.","title":"Link with other organisations"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#share-and-effective-permissions","text":"When a user creates a case, the case is linked to the user\u2019s organisation with the profile \u201corg-admin\u201d. It means that there is no restriction, the effective permissions are the permissions the user has in his organisation. If he decides to share that case with another organisation, he must choose the profile applied on that share. To exerce a action on a case, the related permission must be present in the user profile and in the case share. When you share a case, you can share its tasks or observables but it is not mandatory. Tasks (and observables) can be unitary shared. They can be shared only with organisations for which case is already shared. A case can be shared only once for a given organisation. Thus a case an its tasks/observables are shared with the same permissions for the same organisation.","title":"Share and effective permissions"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/","text":"UI configuration # You can change some user interface settings in the page UI Configuration ( Organisation > UI Configuration ) You must have the permission manageConfig on your profile to manage UI Configuration. (refer to Profiles and permissions ) Hide Empty Case button # Check this checkbox to prevent your analyst to create a case without using a case template . Merge alerts into closed cases # Check this checkbox to disallow merging alerts into closed cases Select the default filter of alert case similarity panel # In this dropdown list, you can chose from various filter the default one used in alerts or cases similarity panel Define the default date format used to display dates # Define the time format used in your organisation .","title":"UI configuration"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#ui-configuration","text":"You can change some user interface settings in the page UI Configuration ( Organisation > UI Configuration ) You must have the permission manageConfig on your profile to manage UI Configuration. (refer to Profiles and permissions )","title":"UI configuration"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#hide-empty-case-button","text":"Check this checkbox to prevent your analyst to create a case without using a case template .","title":"Hide Empty Case button"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#merge-alerts-into-closed-cases","text":"Check this checkbox to disallow merging alerts into closed cases","title":"Merge alerts into closed cases"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#select-the-default-filter-of-alert-case-similarity-panel","text":"In this dropdown list, you can chose from various filter the default one used in alerts or cases similarity panel","title":"Select the default filter of alert case similarity panel"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#define-the-default-date-format-used-to-display-dates","text":"Define the time format used in your organisation .","title":"Define the default date format used to display dates"},{"location":"thehive/user-guides/organisation-managers/users-management/","text":"Users management # In TheHive4 you can manage users that belongs to your organisation in the Users page ( Organisation > Users ) You must have the permission manageUser on your profile to manage users of your organisation . (refer to Profiles and permissions ) Create new user # You can create a new user in your organisation . clic the Create new user button in the Users page ( Organisation > Users ) You must provide the following information (they are all mandatory): Organisation (autmatically filled, non modifiable) Login (This will be used by the user to authenticate) Full Name (This will be used as display name) Profile (drop-down list to set a profile, that will define user permissions ) List users # You can list the users that belongs to your organisation in the Users page ( Organisation > Users ) In this list you can find the following information: Status Login Full Name Profile API Key MFA activation Creation and last update dates Set or modify a user password # To set or modify a user password, clic the button New password (if the user never had a password) or Edit password in the column Password of the User list ( Organisation > Users ) Create, renew, revoke or reveal an user API Key # In the column API Key of the user list ( Organisation > Users ), you can: Create an API Key if the user never had one before Renew the user API Key Revoke the user API Key Reveal the user API Key in your user interface. Edit user information # You can edit the following user information from the user list ( Organisation > Users ): Full name Profile To edit an user information, clic the Edit user button on the Actions column: Lock an user # Locking an user make the account unusable for any action. You can lock an user from the User list ( Organisation > Users ). To lock an user , clic on the lock button: Delete an user # You can delete an user from your organisation via the User list ( Organisation > Users ) \u26a0\ufe0f Note Deleting an user is irrevocable. Recreating an user with the same information will not reattribute the cases the previous account was assigned to. To delete an user, clic on the trash icon:","title":"Users management"},{"location":"thehive/user-guides/organisation-managers/users-management/#users-management","text":"In TheHive4 you can manage users that belongs to your organisation in the Users page ( Organisation > Users ) You must have the permission manageUser on your profile to manage users of your organisation . (refer to Profiles and permissions )","title":"Users management"},{"location":"thehive/user-guides/organisation-managers/users-management/#create-new-user","text":"You can create a new user in your organisation . clic the Create new user button in the Users page ( Organisation > Users ) You must provide the following information (they are all mandatory): Organisation (autmatically filled, non modifiable) Login (This will be used by the user to authenticate) Full Name (This will be used as display name) Profile (drop-down list to set a profile, that will define user permissions )","title":"Create new user"},{"location":"thehive/user-guides/organisation-managers/users-management/#list-users","text":"You can list the users that belongs to your organisation in the Users page ( Organisation > Users ) In this list you can find the following information: Status Login Full Name Profile API Key MFA activation Creation and last update dates","title":"List users"},{"location":"thehive/user-guides/organisation-managers/users-management/#set-or-modify-a-user-password","text":"To set or modify a user password, clic the button New password (if the user never had a password) or Edit password in the column Password of the User list ( Organisation > Users )","title":"Set or modify a user password"},{"location":"thehive/user-guides/organisation-managers/users-management/#create-renew-revoke-or-reveal-an-user-api-key","text":"In the column API Key of the user list ( Organisation > Users ), you can: Create an API Key if the user never had one before Renew the user API Key Revoke the user API Key Reveal the user API Key in your user interface.","title":"Create, renew, revoke or reveal an user API Key"},{"location":"thehive/user-guides/organisation-managers/users-management/#edit-user-information","text":"You can edit the following user information from the user list ( Organisation > Users ): Full name Profile To edit an user information, clic the Edit user button on the Actions column:","title":"Edit user information"},{"location":"thehive/user-guides/organisation-managers/users-management/#lock-an-user","text":"Locking an user make the account unusable for any action. You can lock an user from the User list ( Organisation > Users ). To lock an user , clic on the lock button:","title":"Lock an user"},{"location":"thehive/user-guides/organisation-managers/users-management/#delete-an-user","text":"You can delete an user from your organisation via the User list ( Organisation > Users ) \u26a0\ufe0f Note Deleting an user is irrevocable. Recreating an user with the same information will not reattribute the cases the previous account was assigned to. To delete an user, clic on the trash icon:","title":"Delete an user"}]}